[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Practitioner’s Guide to Machine Learning",
    "section": "",
    "text": "Vorwort",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Practitioner’s Guide to Machine Learning",
    "section": "",
    "text": "Buchcover mit einer Zeichnung eines Teils einer Siphonophorae (einer Quallenart) von Ernst Haeckel aus seinem Buch “Kunstformen der Natur” (1900, Tafel 37; Quelle: www.BioLib.de).↩︎",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "01a_intro.html",
    "href": "01a_intro.html",
    "title": "Einleitung",
    "section": "",
    "text": "ML ist überall!\nDieses Kapitel illustriert mit verschiedenen motivierenden Beispiele den Aufstieg von Machine Learning (ML).\nMaschinelles Lernen wird bereits überall um uns herum verwendet, um unser Leben bequemer zu machen:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#sec-ml-everywhere",
    "href": "01a_intro.html#sec-ml-everywhere",
    "title": "Einleitung",
    "section": "",
    "text": "Gesichtserkennung\nEine der ersten Erfolgsgeschichten aus dem Bereich maschinelles Lernen und Computer Vision ist die Gesichtserkennungstechnologie, welche heutzutage in jeder Digitalkamera und jedem Smartphone verbaut ist.\nWährend die in einer Kameraanwendung implementierten Algorithmen ziemlich einfach sind und nur Gesichter im Allgemeinen erkennen, um sicherzustellen, dass man bei der Aufnahme gut zu sehen ist, werden in immer mehr Ländern auch ausgefeiltere Algorithmen von Regierungen und Strafverfolgungsbehörden verwendet, um ein erkanntes Gesicht einer bekannten Person in ihren biometrischen Datenbanken zuzuordnen, um beispielsweise um Kriminelle zu identifizieren. Also …bitte lächeln!?\n\n\n\nQuelle: https://thesocietypages.org/socimages/2008/12/15/nikon-s60-auto-focuses-on-voyeurs-savages-ghosts/ (15.12.2008)\n\n\n\n\nObjekterkennung (z.B. für autonomes Fahren)\nEin weiteres Beispiel aus dem Bereich Computer Vision ist die Objekterkennung oder die Bildsegmentierung im Allgemeinen. Dies wird beispielsweise in selbstfahrenden Autos verwendet, um sicherzustellen, dass Straßenschilder und Fußgänger erkannt werden.\n\n\n\nQuelle: https://medium.com/intro-to-artificial-intelligence/c01eb6eaf9d (16.06.2018)\n\n\n\n\nAnalyse von medizinischen Bildern\nEin abschließendes Beispiel für die Auswertung von Bilddaten stammt aus dem Anwendungsbereich Medizin: Unten sind zwei Aufnahmen von der Netzhaut abgebildet, anhand derer eine häufige Diabetes-Komplikation diagnostiziert werden kann. Unbehandelt kann diese zu Blindheit führen.\nDer Diagnosealgorithmus zur Erkennung von Krankheitsmarkern in solchen Bildern wurde von Forschern bei Google entwickelt und hat die gleiche Genauigkeit wie menschliche Experten auf diesem Gebiet. Google hatte sogar ein Team von Top-Spezialisten zusammengestellt, um die schwierigsten Fälle noch einmal zu besprechen und einheitliche Labels für alle Bilder zu generieren, wodurch sie ihr Modell noch weiter verbessern konnten.\nDa die Geräte zur Aufnahme dieser Bilder relativ günstig sind, können mit diesem ML-Modell Experten-Diagnoseentscheidungen auch denjenigen zugänglich gemacht werden, die sonst eventuell nicht die Möglichkeit haben, einen Top-Spezialisten zu konsultieren.\n\n\n\nQuelle: https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html (29.11.2016)\n\n\n\n\nSprachassistenten (oder genau genommen: Spracherkennung…)\nGenug zu Computer Vision; nun ein Beispiel aus dem Bereich Natural Language Processing (NLP; “Verarbeitung natürlicher Sprache”): Sprachassistenten, wie Siri oder Alexa, warten bei vielen Menschen zu Hause auf Befehle. Während einige der Antworten, die sie geben, noch von Menschen geschrieben wurden (wie im Screenshot unten), besteht die eigentliche Herausforderung darin, zu verstehen, was die Person tatsächlich sagt. Die Spracherkennung, also das automatische Transkribieren gesprochener Sprache in Text, ist ein ziemlich schwieriges Problem, da Menschen z.B. in verschiedenen Dialekten sprechen und zusätzliche Hintergrundgeräusche auftreten können.\n\n\n\nScreenshot: Siri von macOS (13.12.2018)\n\n\n\n\nMaschinelle Übersetzung\nNochmal aus dem Bereich NLP: Maschinelle Übersetzung, also das automatische Übersetzen von Texten in eine andere Sprache.\nFalls du Google Translate (als Beispiel im Screenshot unten gezeigt) kurz nach seiner Erscheinung 2006 verwendet hast, warst du wahrscheinlich meistens ziemlich enttäuscht von den Ergebnissen. Die Übersetzungen klangen so als hätte jemand die Wörter nur nacheinander in einem Wörterbuch nachgeschlagen (= statistische maschinelle Übersetzung). Dies änderte sich 10 Jahre später im Jahr 2016 als Google anfing, die Übersetzungen mit einem neuronalen Netzmodell zu generieren: Jetzt sind die übersetzten Texte tatsächlich lesbar und erfordern in der Regel nur noch geringfügige manuelle Korrekturen, wenn überhaupt.\n\n\n\nScreenshot: https://translate.google.com/ (13.12.2018)\n\n\n\n\nEmpfehlungssysteme (Recommender Systems)\nEin weiteres ML-Anwendungsgebiet sind Empfehlungssysteme, z.B. auf E-Commerce-Plattformen wie Amazon (siehe Screenshot unten), die dem Nutzer (idealerweise) hilfreiche Suchergebnisse und Vorschläge liefern, wodurch die jeweiligen Unternehmen wiederum Umsätze generieren. Auch Social Media Plattformen, Netflix, YouTube & Co fesseln ihre Nutzer damit länger an den Bildschirm.\nManchmal helfen die generierten Vorschläge dem Nutzer genau das zu finden, wonach er gesucht hat. Aber insbesondere Plattformen mit nicht-kuratierten Inhalten wie YouTube wurden in der Vergangenheit kritisiert, da sie durch personalisierte Empfehlungen unter anderem die Verbreitung von Verschwörungstheorien förderten. Da diese Art von Inhalten ein besonderes Suchtpotential haben, wurden sie häufiger empfohlen und trieben die Nutzer dadurch weiter in den postfaktischen Sumpf, anstatt auch Perspektiven außerhalb der eigenen Informationsblase anzubieten.\nAuf der anderen Seite hat die Erforschung von Empfehlungssystemen aber auch Entwicklungen in anderen Wissenschaftsbereichen beflügelt. Zum Beispiel kann die Suche nach Heilmitteln für Krankheiten beschleunigt werden, indem Wirkstoffmoleküle empfohlen werden, die zu den Proteinen passen, die eine Schlüsselrolle in der Krankheit spielen.\n\n\n\n\n\n\n\n\nScreenshot: https://www.amazon.com/ (12.12.2018)\n\n\n\n\nBesser als der Mensch: AlphaGo\nIm Jahr 2016 präsentierte DeepMind, ein später von Google übernommenes Startup, AlphaGo, das erste Computerprogramm, das einen menschlichen Go-Meister besiegte.\nDies war ein großer Meilenstein für die KI-Forschungsgemeinschaft. Go ist mit einem Spielfeld von 19 x 19 Feldern viel komplexer als Schach (8 x 8 Felder und restriktivere Bewegungsmuster) und selbst die optimistischsten KI-Forscher hatten nicht erwartet, dass ein Computer vor 2020 gegen einen Go-Meister gewinnen könnte.\nDie in AlphaGo verwendeten Algorithmen stammen aus dem Teilgebiet des Reinforcement Learning, auf das wir später noch genauer eingehen.\n\n\n\nQuelle: https://www.nature.com/nature/volumes/529/issues/7587 (28.01.2016)\n\n\n\n\nProteinfaltung – ein 50 Jahre altes Problem ist gelöst\nIm Jahr 2020 konnte DeepMind eine weitere Erfolgsgeschichte erzählen: Ihr AlphaFold-Modell kann die 3D-Struktur von Proteinen aus ihrer ursprünglichen Aminosäuresequenz bestimmen – und zwar genauso akkurat wie traditionelle Simulationsmodelle.\nProteine spielen oft eine Schlüsselrolle in Krankheiten. Kennt man die 3D-Struktur eines Proteins, kann man bestimmen, welche Wirkstoffmoleküle an dieses Protein binden können. Dadurch können Zielstrukturen identifiziert werden, die weiter untersucht werden sollten, um ein Heilmittel für die entsprechende Krankheit zu finden.\nZwar gab es die exakten Simulationsmodelle zur Berechnung der 3D-Struktur eines Proteins schon länger, diese waren jedoch sehr langsam und es dauerte oft mehrere Tage, um die Faltung eines einzelnen Proteins zu berechnen. Mit dem neuen neuronalen Netzmodell kann dieselbe Berechnung jetzt in Minuten oder sogar Sekunden durchgeführt werden, wodurch die Medikamentenentwicklung enorm beschleunigt wurde.\n\n\n\nQuelle: https://deepmind.google/discover/blog/alphafold-using-ai-for-scientific-discovery-2020/ (15.01.2020)\n\n\n\n\nNeuronale Netze werden kreativ\nViele unterhaltsame Anwendungen verwenden neuronale Netze, um neue Inhalte zu generieren, d.h. kreative Tätigkeiten auszuführen, die bisher ausschließlich den Menschen vorbehalten schienen.\nZum Beispiel hat eine KI ein etwas verwirrendes, aber urkomisches Skript für einen Film geschrieben, der dann sogar produziert wurde.\nNeuronale Netze auch verwendet, um Musik zu visualisieren. Dabei werden passende Bilder kombiniert und fließend transformiert wie in diesem Video:\n\nUnd du hast wahrscheinlich auch schon einige Beispiele für “Neural Style Transfer” gesehen, eine Technik mit der man z.B. ein Social-Media-Profilbild wie ein Van-Gogh-Gemälde aussehen lassen kann:\n\n\n\nQuelle: https://pytorch.org (28.05.2022)\n\n\nAuch Stock-Fotos sind nun im Grunde obsolet, da man mit Hilfe neuronaler Netze Bilder aus einer Textbeschreibung generieren kann:\n\n\n\nScreenshot: https://imagen.research.google/ (28.05.2022)\n\n\nUnd Chatbots können Gespräche führen und Informationen in verschiedenen Formen wiedergeben:\n\n\n\nScreenshot: https://chat.openai.com/chat (04.01.2023)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#sec-ml-history",
    "href": "01a_intro.html#sec-ml-history",
    "title": "Einleitung",
    "section": "ML Geschichte: Warum jetzt?",
    "text": "ML Geschichte: Warum jetzt?\nWarum gibt es einen solchen Anstieg von ML Anwendungen? Allgegenwärtig ist ML nicht nur in unserem Alltag, auch die Zahl der jährlich veröffentlichten Forschungsarbeiten zu dem Thema ist exponentiell gestiegen:\n\n\n\nDatenquelle: https://www.webofknowledge.com/\n\n\nInteressanterweise liegt das aber nicht etwa an einer Fülle bahnbrechender theoretischer Errungenschaften in den letzten Jahren (in der Grafik als violette Rauten gekennzeichnet). Im Gegenteil: Viele der heute verwendeten Algorithmen wurden bereits Ende der 50er / Anfang der 60er Jahre entwickelt. So ist beispielsweise das Perzeptron der Vorläufer von neuronalen Netzen, die hinter allen im letzten Abschnitt gezeigten Beispielen stecken. Einige der wichtigsten neuronalen Netzarchitekturen, Recurrent Neural Networks (RNN, “rekurrente neuronale Netze”) und Convolutional Neural Networks (CNN, “faltende neuronale Netze”), welche die Grundlage für moderne Sprach- bzw. Bildverarbeitung bilden, wurden in den frühen 80er und 90er Jahren entwickelt. Aber zu dieser Zeit hatten wir noch nicht die Rechenressourcen, um diese Modelle für mehr als kleine Experimente zu verwenden.\nAufgrund dessen korreliert der Anstieg der ML-Publikationen stärker mit der Anzahl der Transistoren auf CPUs (also den regulären Prozessoren in normalen Computern) und GPUs (Grafikkarten, die die Arten von Berechnungen parallelisieren, die zum effizienten Trainieren von neuronalen Netzwerkmodellen erforderlich sind):\n\n\n\nDatenquelle: https://en.wikipedia.org/wiki/Transistor_count\n\n\nDarüber hinaus hat die Veröffentlichung vieler Open-Source-Bibliotheken wie scikit-learn (für traditionelle ML-Modelle) und theano, tensorflow und (py)torch (für die Implementierung neuronaler Netze) die Verwendung von ML-Algorithmen in anderen Fachbereichen deutlich erleichtert.\n\n\n\n\n\n\nHinweis\n\n\n\nEinerseits demokratisieren solche Bibliotheken die Verwendung von ML, andererseits resultiert eine Nutzung ohne Wissen über die theoretischen Grundlagen auch in Fehlanwendungen. Die Modelle zeigen dann oft nicht die erwartete Performance, was zu (deplatzierter) Enttäuschung führt. Im ungünstigsten Fall kann es passieren, dass die Modelle bestimmte Teile der Bevölkerung diskriminieren, z.B. Kreditbewertungsalgorithmen, die von Banken verwendet werden und die aufgrund von Verzerrungen in den historischen Daten Frauen systematisch Kredite zu höheren Zinssätzen anbieten als Männern. Wir werden solche Probleme im Kapitel zur Vermeidung häufiger Fehler besprechen.\n\n\nEin weiterer Faktor, der zur Verbreitung von ML beiträgt, ist die Verfügbarkeit von (digitalen) Daten. Unternehmen wie Google, Amazon und Meta hatten hier einen Vorsprung, da ihr Geschäftsmodell von Anfang an auf Daten aufgebaute. Andere Unternehmen holen inzwischen langsam auf. Während traditionelle ML-Modelle nur minimal von diesen verfügbaren Daten profitieren, können große neuronale Netzmodelle mit vielen Freiheitsgraden jetzt ihr volles Potenzial entfalten, indem sie aus all den Texten und Bildern lernen, die täglich im Internet veröffentlicht werden:\n\n\n\n\n\n\nAber wir sind nach wie vor noch weit von Artificial General Intelligence (AGI, “künstliche allgemeine Intelligenz” oder “starke KI”) entfernt!\n\n\n\n\n\nEine AGI ist ein hypothetisches Computersystem mit menschenähnlichen kognitiven Fähigkeiten, das in der Lage wäre, ein breites Spektrum von Aufgaben in verschiedenen Bereichen zu verstehen, zu lernen und auszuführen. Speziell würde eine AGI nicht nur bestimmte Aufgaben ausführen, sondern auch ihre Umgebung verstehen und daraus lernen, autonom Entscheidungen treffen und ihr Wissen auf vollkommen neue Situationen verallgemeinern.\nIn der Praxis wird stattdessen Artificial Narrow Intelligence (ANI, auch “schwache KI”) verwendet: Modelle, die explizit programmiert wurden, um eine bestimmte Aufgabe zu lösen, z.B. Texte von einer Sprache in eine andere übersetzen. Diese Modelle können nicht (eigenständig) verallgemeinern und neue Aufgaben lernen, sprich das maschinelle Übersetzungsmodell wird nicht morgen auf die Idee kommen, dass es nun auch Gesichter in Bildern erkennen will. Natürlich kann man mehrere einzelne ANIs in einem großen Programm kombinieren, um so mehrere verschiedene Aufgaben zu lösen, aber auch diese Sammlung von ANIs ist nicht in der Lage, selbstständig neue Fähigkeiten darüber hinaus zu erlernen.\nViele KI-Forscher sind derzeit überzeugt, dass wir zumindest mit den aktuell verwendeten Methoden (z.B. den Large Language Models (LLMs) wie ChatGPT von OpenAI) wahrscheinlich nie eine echte menschenähnliche AGI erschaffen werden. Speziell mangelt es diesen KI-Systemen noch immer an einem allgemeinen Verständnis von Kausalität und physikalischen Gesetzen wie der Objektpermanenz – etwas, das sogar viele Haustiere verstehen.\nWenn du mehr über die Mängel aktueller KI-Systeme erfahren möchtest, sind die Blogartikel von Gary Marcus sehr zu empfehlen!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "01b_basics.html",
    "href": "01b_basics.html",
    "title": "Grundlagen",
    "section": "",
    "text": "Daten sind das neue Öl!?\nDieses Kapitel gibt eine allgemeine Einführung in das Thema Machine Learning (ML) und zeigt auf in welchen Bereichen der Einsatz von ML sinnvoll ist und welche Probleme man lieber mit einfacheren Mitteln lösen sollte.\nLass uns von vorne anfangen. Alles beginnt mit Daten. Wahrscheinlich hast du diese Behauptung schon einmal gehört: “Daten sind das neue Öl!”. Dies legt nahe, dass Daten wertvoll sind. Aber sind sie das?\nDer Grund, warum Öl als wertvoll angesehen wird, liegt darin, dass wir wichtige Anwendungsfälle dafür haben: den Antrieb unserer Autos, die Beheizung unserer Häuser und die Herstellung von Kunststoffen oder Düngemitteln. Genauso verhält es sich auch mit Daten: sie sind nur so wertvoll wie das was wir aus ihnen machen. Wofür können wir also Daten verwenden?\nDie wichtigsten Anwendungsfälle fallen in eine von zwei Kategorien:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "01b_basics.html#sec-data-oil",
    "href": "01b_basics.html#sec-data-oil",
    "title": "Grundlagen",
    "section": "",
    "text": "Insights\nErkenntnisse können wir entweder durch kontinuierliches Monitoring (“Sind wir auf Kurs?”) oder eine tiefere Analyse (“Was läuft falsch?”) generieren.\nIn dem wir wichtige Variablen oder Key Performance Indicators (KPIs) in Berichten oder Dashboards visualisieren, machen wir den Status Quo transparenter und quantifizieren wie nah wir einem bestimmten Ziel schon gekommen sind. Wenn ein KPI weit ab von seinem Zielwert liegt, können wir mit einer explorativen Datenanalyse tiefer in die Daten eintauchen, um die Ursache des Problems zu identifizieren und Fragen wie diese zu beantworten:\n\nWarum erreichen wir unser Ziel nicht?\nWas sollen wir als nächstes tun?\n\nZufriedenstellende Antworten zu finden ist allerdings oft mehr Kunst als Wissenschaft 😉 – mehr dazu im Kapitel Datenanalyse.\n\n\nAutomatisierung\nWie in den folgenden Unterkapiteln beschrieben, können Machine Learning Modelle dazu verwendet werden, um eine ‘Input → Output’ Aufgabe zu automatisieren, welche sonst ein (speziell geschulter) Mensch erledigen müsste. Diese Aufgaben fallen einem (entsprechend geschulten) Menschen in der Regel leicht:\n\nÜbersetzen von Texten von einer Sprache in eine andere\nProdukte mit Kratzern aussortieren, wenn sie einen Kontrollpunkt am Fließband passieren\nEinem Freund Filme empfehlen\n\nDie ML-Modelle müssen dafür mit einer großen Menge historischer Daten trainiert werden (z.B. Texte in beiden Sprachen, Bilder von Produkten mit und ohne Kratzer, Informationen über verschiedene Nutzer und welche Filme sie gesehen haben).\nDie resultierende Software kann dann entweder verwendet werden, um die Aufgabe vollständig zu automatisieren, oder wir können einen Menschen dazwischen schalten, der eingreifen und die Vorschläge des Modells korrigieren kann.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "01b_basics.html#sec-what-is-ml",
    "href": "01b_basics.html#sec-what-is-ml",
    "title": "Grundlagen",
    "section": "Was ist ML?",
    "text": "Was ist ML?\nWas genau ist nun dieses maschinelle Lernen, das bereits unser aller Leben verändert?\nML ist zunächst ein Forschungsgebiet im Bereich der theoretischen Informatik, an der Schnittstelle von Mathematik und Informatik:\n\n\n\n\n\nGenauer gesagt ist maschinelles Lernen ein Überbegriff für Algorithmen, die Muster erkennen und Regeln aus Daten lernen.\n\n\n\n\n\n\nHinweis\n\n\n\nVereinfacht kann man sich einen Algorithmus als Strategie oder Rezept zur Lösung eines speziellen Problems vorstellen. Es gibt zum Beispiel effektive Algorithmen, um den kürzesten Weg zwischen zwei Städten zu finden (z.B. genutzt in den Navigationssystemen von Google Maps) oder um Planungsprobleme zu lösen wie z.B.: “Welche Aufgabe sollte zuerst erledigt werden und welche Aufgabe danach um alle Aufgaben vor ihrer Deadline zu schaffen unter Berücksichtigung eventueller Abhängigkeiten zwischen den Aufgaben.” Maschinelles Lernen befasst sich mit der Teilmenge von Algorithmen, die statistische Regelmäßigkeiten in einem Datensatz erkennen und nutzen, um bestimmte Ergebnisse zu erzielen.\n\n\n\nAnalog zu den Werkzeugen, mit denen man etwas in einem traditionellen Produktionsprozess herstellt, kann man sich ML-Algorithmen als Werkzeuge vorstellen, um Wert aus Daten zu generieren:\n\n\n\n\n\nUm ML erfolgreich anzuwenden, sollte man sich einige wichtige Fragen stellen:\n\nWas könnte wertvoll sein? Dies kann beispielsweise ein neues Feature für ein existierendes Produkt sein, z.B. Face ID als neue Möglichkeit um ein Smartphone zu entsperren.\nWelche Daten werden benötigt? Einen Holzstuhl kann man nicht aus Stoff und Metall oder ein paar im Wald gefundenen Zweigen herstellen. Genauso benötigt man je nachdem, was man mit ML erreichen möchte, auch die richtigen Daten in ausreichender Qualität & Quantität um die Algorithmen überhaupt anwenden zu können. In vielen Anwendungsfällen ist dieser Teil besonders schwierig, da man die benötigten Daten oft nicht einfach kaufen kann, wie Holz im Baumarkt, sondern diese selbst sammeln – also quasi einen eigenen Wald anpflanzen muss, was einige Zeit dauern kann.\nWelcher ML-Algorithmus ist das richtige Werkzeug für diese Aufgabe? (Welche Kategorie von ML-Algorithmen generiert die Art von Output, die wir benötigen?)\nVerfüge ich bzw. meine Mitarbeiter über die notwendigen Fähigkeiten und genügend Rechenleistung, um das Vorhaben erfolgreich umzusetzen?\n\n\nDie unterschiedlichen Algorithmen bilden unseren ML Werkzeugkasten:\n\n\n\nML selbst ist ein Teilbereich der Künstlichen Intelligenz (KI/AI), das derzeit zwar häufiger verwendete Buzzword, jedoch basieren alle wirklich interessanten Anwendungen, z.B. die initial gezeigten Beispiele, tatsächlich auf ML. Zu KI gehören ansonsten zum Beispiel noch einige Suchalgorithmen, die beim Bau der ersten Schachcomputer verwendet wurden. ML kann wiederum in drei Hauptbereiche unterteilt werden, Unsupervised (“Unüberwachtes”), Supervised (“Überwachtes”) und Reinforcement (“Bestärkendes”) Learning. Das Buzzword “Deep Learning” ist außerdem ein Überbegriff für neuronale Netzwerkmodelle und beinhaltet auch Generative AI (GenAI) Modelle wie ChatGPT. Einige der einfachsten im ML verwendeten Algorithmen, wie die lineare Regression oder PCA (sehr ähnlich der Faktorenanalyse), werden auch von Statistikern verwendet, die jedoch auch andere Werkzeuge wie Hypothesentests verwenden, welche keine Regeln oder Muster aus Daten lernen. Die meisten Data Scientists verwenden viele Methoden aus ML und Statistik, aber auch zusätzliche Werkzeuge wie A/B-Tests, z.B. um zu überprüfen, ob ein roter oder grüner “Kaufen”-Button auf einer Website mehr Umsatz generiert.\n\n\n\nML Algorithmen lösen “Input → Output” Probleme\nAll diesen ML-Algorithmen aus unserem Werkzeugkasten ist gemein, dass sie “Input → Output” Probleme wie diese lösen:\n\n\n\nBeispiel “Input → Output” ML Probleme: Erkennen von Objekten in Bildern; Übersetzen von Text von einer Sprache in eine andere; Bestimmen eines guten nächsten Zuges angesichts des aktuellen Zustands eines Go-Boards; Gruppieren ähnlicher Nutzer/Kunden anhand verschiedener Informationen über sie, wie z.B. Umfrageantworten (im Marketing als Kundensegmentierung bezeichnet dient dies dazu z.B. unterschiedliche Kundengruppen in sozialen Medien mit spezifischen Werbekampagnen anzusprechen).\n\n\nWährend in den obigen Beispielen ein (entsprechend geschulter) Mensch zur jeweiligen Eingabe in der Regel leicht die richtige Ausgabe erzeugt (z.B. kann sogar ein kleines Kind die Katze im ersten Bild erkennen), fällt es Menschen oft schwer zu beschreiben, wie sie zur richtigen Antwort gekommen sind (woran erkennt man, dass dies eine Katze und kein kleiner Hund ist? an den spitzen Ohren? den Schnurrhaaren?). Im Gegensatz dazu lernen ML-Algorithmen solche Regeln aus den gegebenen Beispieldaten.\n\n\nML vs. herkömmliche Software\nWährend man herkömmliche Software-Lösungen dazu verwenden kann, um Aufgaben zu automatisieren, bei denen eine vordefinierte Abfolge von Aktionen nach einigen festgeschriebenen Regeln ausgeführt wird (z.B. “Eine Tür soll sich öffnen, wenn ein Objekt eine Lichtschranke durchquert und sich 20 Sekunden später wieder schließen”), kann man mit maschinellem Lernen “Input → Output”-Aufgaben automatisieren, für die es sonst schwierig wäre, solche Regeln aufzustellen.\nDie Qualitätskontrolle in einer Keks-Fabrik ist ein Beispiel für eine solche “Input (Keks) → Output (ok/fehlerhaft)”-Aufgabe: Während z.B. zerbrochene Kekse automatisch aussortiert werden könnten, indem man überprüft, dass jeder Keks etwa 15g wiegt, ist es schwierig, Regeln zu formulieren, die alle möglichen Produktionsfehler zuverlässig abfangen. Also müsste entweder ein Mensch die Produktionslinie beobachten, um z.B. zusätzlich verbrannte Kekse zu erkennen, oder man könnte Bilder von den Keksen machen und sie als Input für ein Machine Learning Modell verwenden, um die fehlerhaften Kekse zu erkennen:\n\n\n\n\n\nUm dieses Problem mit ML zu lösen, muss zunächst ein großer Datensatz mit Fotos vieler guter, aber auch allerlei fehlerhafter Kekse zusammengestellt werden, inklusive der entsprechenden Annotationen, also einem Label für jedes Bild, ob es einen guten oder fehlerhaften Keks zeigt (aber nicht unbedingt die Art des Fehlers). Aus diesen Beispielen kann dann ein ML-Algorithmus lernen, zwischen guten und fehlerhaften Keksen zu unterscheiden.\n\n\nWann man ML (nicht) verwenden sollte\nML ist übertrieben, wenn:\n\nein manuell definiertes Regelwerk oder ein mechanistisches (white box) Modell das Problem lösen kann. Wenn beispielsweise in unserer Keks-Fabrik das einzige Qualitätsproblem immer nur zerbrochene Kekse wären, dann würde die Regel “Keks-Gewicht muss zwischen 14-16g liegen” ausreichen, um alle fehlerhaften Kekse zuverlässig zu erkennen. Eine solche Regel ist außerdem einfacher zu implementieren, da nicht erst ein großer Datensatz gesammelt werden muss.\n\nML ist deine beste Chance, wenn:\n\nMenschen von sehr komplexen, hochdimensionalen Daten überfordert sind. Z.B. fällt es einem Menschen schwer, bei einer Excel-Tabelle mit hunderten von Spalten den Durchblick zu behalten. Wir können in so einem Meer von Zahlen oft keine Muster erkennen. Im ungünstigsten Fall gibt es tatsächlich keine Zusammenhänge in den Daten, da vielleicht nicht alle relevanten Faktoren gemessen wurden, aber wenn doch, wird ML die Zusammenhänge höchstwahrscheinlich finden.\n\nML hat großes Potenzial, wenn:\n\neine exakte Simulation mit einem mechanistischen Modell zu lange dauert (dieses aber verwendet werden kann, um einen qualitativ hochwertigen Datensatz zu generieren). Z.B. wurde das in der Einleitung erwähnte AlphaFold-Modell, mit dem die 3D-Struktur eines Proteins anhand seiner Aminosäuresequenz vorhergesagt werden kann, auf den Daten trainiert, die das ursprüngliche Simulationsmodell zur Lösung dieser Aufgabe zuvor generiert hat, welches allerdings zu langsam ist, um auf eine große Anzahl von Proteinen angewendet zu werden.\neine “einfache”, aber schwer zu erklärenden Aufgabe gelöst werden soll, die ein (geschulter) Mensch in ~1 Sekunde lösen kann, z.B. etwas auf einem Bild erkennen\n⇒ Dadurch können repetitive Aufgaben automatisiert und Expertenwissen für alle zugänglich gemacht werden, wie z.B. beim eingangs gezeigten Diagnosemodell für diabetische Retinopathie von Google.\nAber: Erfolg hängt von der Datenqualität & -quantität ab!\n→ Menschen können aus wenigen Beispielen viel besser verallgemeinern. Ein Arzt kann beispielsweise eine Krankheit auch dann noch leicht erkennen, wenn die Bilder z.B. mit einer älteren Maschine aufgenommen wurden. Ein ML-Modell muss für dieses geänderte Setup u.U. neu trainiert werden, wofür wieder viele zusätzliche Daten gesammelt werden müssen.\n\n\n\n\n\n\n\nVorsicht\n\n\n\nVerwende ML nur dann, wenn gelegentliche Fehler akzeptabel sind. ML-Modelle werden normalerweise mit von Menschen erstellten Daten trainiert, die anfällig für Rauschen sind, da selbst Experten bei bestimmten Fällen unterschiedlicher Meinung sein können. Außerdem müssen ML-Modelle möglicherweise extrapolieren und Vorhersagen für neue Datenpunkte machen, die sich von den Trainingsdaten unterscheiden, was zu falschen Ergebnissen führen kann. Um Fehler zu minimieren, kann es hilfreich sein, die Vorhersagen des ML-Modells gelegentlich von einem Menschen überprüfen zu lassen (“human in the loop”).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "01b_basics.html#sec-how-machines-learn",
    "href": "01b_basics.html#sec-how-machines-learn",
    "title": "Grundlagen",
    "section": "Wie “lernen” Maschinen?",
    "text": "Wie “lernen” Maschinen?\nWie lösen ML-Algorithmen diese “Input → Output” Probleme, also wie erkennen sie Muster und lernen Regeln aus Daten?\nML-Algorithmen lassen sich nach ihrer Funktionsweise, d.h. wie sie lernen, weiter unterteilen. Diese Unterteilung ist inspiriert von der Art und Weise wie Menschen lernen:\n\n\n\nUnsupervised Learning: Menschen sind sehr gut darin, statistische Gesetzmäßigkeiten in der Welt zu erkennen, ohne dass sie ausdrücklich dazu aufgefordert werden. Ist dir zum Beispiel schon einmal aufgefallen, dass wir beim Sprechen keine … Pausen … zwischen … Wörtern machen? Dennoch lernen Kinder intuitiv, welche Silben zu einem Wort gehören und wo dieses Wort endet und das nächste beginnt. Dies funktioniert, weil die Silben eines Wortes immer in dieser spezifischen Kombination vorkommen, diesem Wort dann aber unterschiedliche Wörter, beginnend mit vielen verschiedenen Silben, folgen. Das bedeutet, dass wir durch Hören von gesprochenem Text die bedingten Wahrscheinlichkeitsverteilungen von Silben erfassen. Supervised Learning: Diese Art des Lernens benötigt eine Lehrerin, die uns die richtigen Antworten sagt und uns korrigiert, wenn wir etwas falsch machen. Um z.B. einem Kind ein Wort beizubringen veranschaulicht man ihm die Bedeutung anhand von Beispielen, und wenn das Kind etwas falsch benennt, z.B. einen kleinen Hund als Katze bezeichnet, korrigiert man es. Reinforcement Learning: Auch diese Art von “Learning by Doing” passiert ganz natürlich wenn Menschen aus den Konsequenzen ihrer Handlungen lernen. Durch Experimentieren und Üben können wir zum Beispiel eine komplexe Abfolge von Handbewegungen einstudieren, mit der wir einer Geige schöne Klänge entlocken, anstatt schmerzhaftes Kreischen zu erzeugen. Zwar ist keine einzelne Handbewegung an sich gut oder schlecht, jedoch nur die richtige Kombination bringt Musik in unsere Ohren.\n\n\nAnalog dazu können auch Maschinen nach diesen drei Strategien lernen:\n\n\n\nUnsupervised Learning: Diese Algorithmen erkennen statistische Regelmäßigkeiten in den Daten, z.B. können sie Gruppen ähnlicher Dinge identifizieren (Beispiel Kundensegmentierung) oder einzelne Punkte finden, die aus der Reihe tanzen (Anomalieerkennung), z.B. ungewöhnliches Verhalten einer Maschine aufgrund eines defekten Teils oder verdächtige Kreditkartentransaktionen im Onlinehandel. Supervised Learning: Diese Algorithmen lernen aus vielen Input-Output-Beispielen, z.B. Bilder und darauf sichtbare Objekte oder Produktionsbedingungen zusammen mit der resultierenden Produktqualität. Das gelernte Modell kann dann für einen neuen Input den dazugehörigen Output vorhersagen. Reinforcement Learning: Diese Art des Lernens ist etwas komplizierter: Der Lernalgorithmus wird hier als Agent bezeichnet, welcher sich in einer Umgebung bewegt, z.B. ein Roboter in der realen Welt oder ein virtueller Agent in einer Simulationsumgebung wie einem Videospiel (was normalerweise viel billiger ist ;-)). Die Umgebung teilt dem Agenten mit, in welchem Zustand oder in welcher Situation er sich gerade befindet, dann kann der Agent auswählen, wie er in diesem Zustand reagieren will, d.h. welche (vordefinierte) Aktion er ausführt. Die Konsequenzen dieser Aktion werden anschließend durch die Umgebung bestimmt (z.B. ein Monster in einem Videospiel besiegen oder von einer Klippe fallen) und je nach Ergebnis wird eine Belohnung zurückgeben (z.B. zusätzliche Punkte für eingesammelte Münzen). Dann wiederholt sich der Zyklus, da sich der Agent im nächsten Zustand befindet. Basierend auf der erhaltenen Belohnung lernt der Agent im Laufe der Zeit, welche Aktionen in welchen Situationen von Vorteil sind und wie er sich in der Umgebung zurechtfindet. Das Schwierige daran ist, dass der Agent die Belohnungssignale teilweise erst lange nachdem die Aktion ausgeführt wurde erhält. In einem Videospiel zum Beispiel findet der Agent zu Beginn eines Levels einen Schlüssel, aber die Tür, die damit geöffnet werden kann, kommt erst viele Frames später. Eine verzögerte Belohnung macht es dem Agent schwerer diese mit der entsprechenden Aktion zu verknüpfen. Da Menschen viel Hintergrundwissen haben, ist es für uns viel einfacher herauszufinden, was in einem Spiel funktioniert und was nicht.\n\n\n\nDaten-Voraussetzungen für das Lernen mit diesen Strategien:\n\nUnsupervised Learning: ein Datensatz mit Beispielen\n\n\n\n\n\nSupervised Learning: ein Datensatz mit gelabelten Beispielen\n\n\n\n\n\nReinforcement Learning: eine (Simulations-) Umgebung, die, basierend auf den Aktionen des Agenten, Daten (Belohnung + neue Zustände) generiert\n\n\n\n\n\nAufgrund der Abhängigkeit von einer datenerzeugenden Umgebung ist Reinforcement Learning ein Sonderfall. Außerdem ist es derzeit noch sehr schwer, Reinforcement Learning Algorithmen zum Laufen zu bringen, weshalb sie hauptsächlich in der Forschung und weniger für praktische Anwendungen verwendet werden.\n\n\n\nSupervised Learning\nSupervised Learning ist die verbreitetste Art des maschinellen Lernens in heutigen Anwendungen.\nBeim Supervised Learning möchten wir ein Modell (= eine mathematische Funktion) \\(f(x)\\) lernen, um den Zusammenhang zwischen einem oder mehreren Input(s) \\(x\\) (z.B. Produktionsbedingungen wie Temperatur, Material, etc.) und einem Output \\(y\\) (z.B. resultierende Produktqualität) zu beschreiben.\nDieses Modell kann dann verwendet werden, um Vorhersagen für neue Datenpunkte zu machen, also \\(f(x') = y'\\) für ein neues \\(x'\\) zu berechnen (z.B. für einen neuen Satz von Produktionsbedingungen vorhersagen, ob das Produkt von hoher Qualität sein wird, oder ob man den Vorgang lieber gleich abbrechen sollte).\nSupervised Learning – kurz und knapp:\n\n\n\nZuerst muss man sich darüber im Klaren sein, was vorhergesagt werden soll und inwiefern einem die Vorhersage hilft, das eigentliche Ziel zu erreichen und Mehrwert generiert. Außerdem muss man sich überlegen, wie man den Erfolg misst, d.h. den Key Performance Indicator (KPI) des Prozesses bestimmen. Anschließend muss man Trainingsdaten sammeln. Da wir hier im Bereich des Supervised Learnings sind, müssen dies gelabelte Daten sein, also inkl. der Zielvariable, die man vorhersagen möchte. Dann “lernt” (oder “trainiert” oder “fittet”) man ein Modell auf diesen Daten um schließlich Vorhersagen für neue Datenpunkte zu generieren.\n\n\n\nFeatures & Labels\nEin typisches Supervised Learning Problem wäre beispielsweise ein Produktionsprozess, bei dem wir vorhersagen möchten, ob ein unter bestimmten Produktionsbedingungen produziertes Teil Ausschuss ist. Hier beinhalten die gesammelten Daten für jedes produzierte Teil die Prozessbedingungen, unter denen es hergestellt wurde, und ob das resultierende Produkt in Ordnung oder Ausschuss war:\n\n\n\nBei den für diesen Anwendungsfall erhobenen Daten handelt es sich um strukturierte Daten in tabellarischer Form (z.B. in einer Excel-Tabelle). Ein Datenpunkt / eine Probe / eine Beobachtung befindet sich immer in einer Zeile dieser Tabelle.\n\n\n\n\n\nDie Spalten der Tabelle enthalten die unterschiedlichen Messwerte/Variablen, welche für jeden Datenpunkt erhoben wurden. Dabei unterscheiden wir zwischen Features (in diesem Fall die Produktionsbedingungen) und Labels (ob das jeweilige Produkt in Ordnung oder Ausschuss war). Features, auch als Matrix \\(X\\) bezeichnet, sind typischerweise diejenigen Messwerte, die wir ohne Aufwand erhalten, da sie während des Prozesses bereits für andere Zwecke gesammelt werden. Wenn beispielsweise der Bediener der Maschine die Temperatur für die Produktion auf einen bestimmten Wert einstellt, wird dies aufgezeichnet während das Signal an das Heizgerät weitergeleitet wird. Das Sammeln der dazugehörigen Labels, bezeichnet als Vektor \\(\\mathbf{y}\\), ist oft kostspieliger. Um beispielsweise im Produktionsprozess einen Datenpunkt mit dem Label “Ausschuss” zu sammeln, muss (vorsätzlich) ein fehlerhaftes Produkt produziert werden, was uns wertvolle Ressourcen kostet. Ein anderes Beispiel für die mit Labels verbunden Kosten ist die Tatsache, dass Google ein Team von Fachärzten bezahlte, um mehrere Bilder von diabetischer Retinopathie, zu denen es widersprüchliche Meinungen gab, zu diskutieren und neu zu bewerten.\n\n\nIm Supervised Learning werden die Features als Input (Eingabe) für das Modell verwendet, während die Labels die Zielvariable, d.h. der vorhergesagte Output (Ausgabe) darstellen. Im Allgemeinen sollten Features unabhängige Variablen sein (z.B. Einstellungen, die der Bediener nach Belieben wählen kann), während der Zielwert von diesen Eingaben abhängig sein sollte, damit wir ihn vorhersagen können.\n\n\nEin Modell aus Daten “lernen”\nZiel: Beschreibe den Zusammenhang zwischen Input(s) \\(x\\) und Output \\(y\\) mit einem Modell, d.h. einer mathematischen Funktion \\(f(x)\\)\n\n\n\n\n\n\nWähle eine Modell Klasse (= Struktur der Funktion): Annahme: Zusammenhang ist linear\n→ lineares Regressionsmodell: \\(y = f(x) = b + w\\cdot x\\)\nDefiniere die Zielfunktion: Minimiere die Abweichung zwischen echtem & vorhergesagtem \\(y\\):\n→ \\(\\min_{b,w} \\sum_i (y_i - f(x_i))^2\\)\nFinde die besten Modellparameter für die gegebenen Daten: d.h. löse das in Schritt 2 definierte Optimierungsproblem\n⇒ \\(f(x) = -2.7 + 5.2x\\)\n\n\n\n\n\n\n\n\nTippVideo Empfehlung\n\n\n\nWenn dir lineare Regression nichts sagt, findest du hier eine tolle Erklärung von der Google-Mitarbeiterin Cassie Kozyrkov zur Funktionsweise der linearen Regression, des einfachsten Supervised Learning Algorithmus: [Teil 1] [Teil 2] [Teil 3]\n\n\nDie existierenden Supervised Learning Algorithmen unterscheiden sich in der Art des \\(x \\to y\\) Zusammenhangs, den sie beschreiben können (z.B. linear oder nichtlinear) und welche Art von Zielfunktion (auch Fehlerfunktion genannt) sie minimieren. Die Aufgabe eines Data Scientists besteht darin, einen für den Datensatz passenden Modelltyp auszuwählen. Den Rest erledigt dann eine Optimierungsmethode, die Parameter für das Modells findet, welche die Zielfunktion des Modells minimieren, d.h. sodass der Vorhersagefehler des Modells auf den gegebenen Daten so klein wie möglich ist.\n\n\n\n\n\n\nHinweis\n\n\n\nIn diesem Buch werden die Begriffe “ML-Algorithmus” und “ML-Modell” meist synonym verwendet. Wenn man es aber genau nimmt, verarbeitet der Algorithmus die gegebenen Daten und lernt die Parameterwerte. Die gefundenen Parameter bestimmen das eigentliche Modell. Ein lineares Regressionsmodell wird beispielsweise durch seine Koeffizienten (d.h. die Parameter des Modells) definiert. Diese werden gefunden, in dem die Schritte des linearen Regressionsalgorithmus befolgt werden, also das Optimierungsproblem auf den gegebenen Daten gelöst wird.\n\n\n\nDa geht noch mehr!\n\n\n\nIn vielen Anwendungsfällen reicht es nicht aus, einfach “nur” die Zielvariable für einen neuen Datenpunkt vorherzusagen; ob beispielsweise ein unter bestimmten Bedingungen hergestelltes Produkt von hoher Qualität sein wird oder nicht. Stattdessen ist es oft notwendig, zusätzlich erklären zu können, warum diese Vorhersage gemacht wurde und welche Features für die Vorhersage entscheidend waren, sowohl um mögliche Ursachen eines Problems besser zu verstehen, aber auch um sicherzustellen, dass die Vorhersagen des Modells auf vernünftigen Annahmen basieren. Darüber hinaus kann ein erlerntes Modell auch innerhalb einer äußeren Optimierungsschleife verwendet werden, um optimale Inputs zu bestimmen. Im einfachsten Fall könnte man z.B. systematisch überprüfen, welche Produktqualität das Modell für verschiedene Prozessbedingungen vorhersagt, um für neue Produkte die Einstellungen mit der am höchsten vorhergesagten Qualität zu wählen. Aber Vorsicht: ML-Modelle sind nur zum Interpolieren, nicht zum Extrapolieren konzipiert, d.h. es muss sichergestellt werden, dass die getesteten Eingaben im gleichen Wertebereich liegen wie in den Daten, die zum Trainieren des Modells verwendet wurden.\n\n\n\nPrädiktive Analyse\nMit Supervised Learning Algorithmen können wir basierend auf historischen Daten ein Vorhersagemodell generieren, das Prognosen über zukünftige Szenarien anstellt um uns bei Planungen zu unterstützen.\nBeispiel: Verkaufsprognosen verwenden, um Lagerbestände besser zu planen.\nInterpretation Prädiktiver Modelle\nInterpretiere das Vorhersagemodell und seine Prognosen, um die zugrundeliegenden Ursache-Wirkung Beziehungen des Prozesses besser zu verstehen.\nBeispiel: Gegeben ein Modell, welches die Qualität eines Produkts aus den Produktionsbedingungen vorhersagt, verstehe welche Faktoren dazu führen dass Produkte von geringerer Qualität sind.\nWas-wäre-wenn Analyse & Optimierung\nVerwende ein Vorhersagemodell für eine Was-wäre-wenn-Vorhersage, um zu untersuchen, wie ein System auf unterschiedliche Bedingungen reagieren könnte (aber Vorsicht!).\nBeispiel: Gegeben ein Modell, welches die verbleibende Lebenszeit für eine Maschinenkomponente aus den Prozessbedingungen vorhersagt, wie schnell würde diese Komponente unter anderen Prozessbedingungen verschleißen?\nWenn wir noch einen Schritt weiter gehen wollen, können wir auch automatisch verschiedene Inputs mit dem Vorhersagemodell in einer Optimierungsschleife bewerten, um systematisch optimale Einstellungen zu finden.\nBeispiel: Gegeben ein Modell, welches die Qualität eines Produkts aus den Produktionsbedingungen vorhersagt, bestimme automatisch die besten Produktionseinstellungen für einen neuen Rohstofftyp.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "01b_basics.html#sec-ml-use-cases",
    "href": "01b_basics.html#sec-ml-use-cases",
    "title": "Grundlagen",
    "section": "ML Anwendungsfälle",
    "text": "ML Anwendungsfälle\nML-Algorithmen können auf Input Daten in verschiedensten Formaten angewendet werden…\n\nStrukturierte vs. Unstrukturierte Daten\nDaten können in verschiedenen Formaten vorliegen und während einige Datentypen evtl. zusätzliche Verarbeitungsschritte erfordern, können ML-Algorithmen im Prinzip mit jeder Art von Daten arbeiten.\n\n\n\nDie wichtigste Unterscheidung bei der Charakterisierung von Daten liegt zwischen strukturierten Daten, das ist jeder Datensatz, der einzelne Messgrößen/Variablen/Attribute/Features enthält, die eindeutige Größen darstellen, und unstrukturierten Daten, die nicht in sinnvolle Variablen unterteilt werden können. Z.B. in Bildern “erster Pixel von links” oder in Texten “10. Wort im 2. Absatz” würden wir nicht Variablen nennen, im Gegensatz zu “Fläche in Quadratmetern” oder “Anzahl Schlafzimmern”, womit man sinnvoll eine Wohnung beschreiben könnte. Strukturierte Daten sind oft heterogen, da die verschiedenen Variablen in einem Datensatz typischerweise für sehr unterschiedliche Dinge stehen. Wenn man beispielsweise mit Sensordaten arbeitet enthält ein Datensatz nicht nur Temperaturmessungen, sondern zusätzlich z.B. Druck und Durchflusswerte, die unterschiedliche Einheiten haben und in sehr unterschiedlichen Skalen gemessen werden können. Unstrukturierte Daten hingegen sind homogen, z.B. gibt es keinen qualitativen Unterschied zwischen dem 10. und dem 100. Pixel in einem Bild.\n\n\n\n…aber unser Ziel, also der gewünschte Output, bestimmt welche Art von Algorithmus wir für das Problem verwenden sollten:\n\n\n\n\nWenn unser Ziel darin besteht, Muster in einem Datensatz zu entdecken (discover), sind Unsupervised Learning Algorithmen ideal: mit Dimensionsreduktion bekommen wir durch eine 2D Visualisierung eine Übersicht über die Daten, Anomalieerkennung identifiziert Ausreißer (z.B. eine defekte Maschine oder eine betrügerische Kreditkartentransaktion), und Clustering gruppiert ähnliche Datenpunkte (z.B. zur Kundensegmentierung).\nSupervised Learning Modelle werden verwendet, um unbekannte Werte zu schätzen (estimate) (z.B. vorhersagen, ob ein Produkt fehlerhaft ist, wenn es unter bestimmten Bedingungen produziert wird): Regression sagt kontinuierliche Werte voraus (z.B. Anzahl der Nutzer, Preis etc.), während Klassifikation diskrete Labels basierend auf den Inputdaten zuweist (z.B. kann ein Tier in einem Bild entweder eine Katze oder ein Hund sein, aber nichts dazwischen).\nRecommender Systems und Information Retrieval-Algorithmen können Artikel von Interesse empfehlen (recommend), wie Dokumente, Songs oder Filme, basierend auf den Präferenzen eines Nutzers oder den Artikeln, mit denen sie interagiert haben.\nDie vielseitigsten Modelle sind Generative AI und Deep Learning, die hauptsächlich unstrukturierte Daten nutzen. Sie können vielfältige Outputs erzeugen (generate) – wie Bilder, Texte (z.B. für maschinelle Übersetzung) oder Musik – basierend auf einem gegebenen Prompt.\nSchließlich werden Reinforcement Learning-Algorithmen verwendet, um Prozesse zu planen und zu steuern (plan and control), indem optimale Aktionssequenzen unter spezifischen Umweltbedingungen bestimmt werden.\n\n\n\nEinige Beispiele für ‘Input → Output’ Aufgaben und welche Art von ML-Algorithmus verwendet werden kann, um sie zu lösen:\n\n\n\nInput \\(X\\)\nOutput \\(Y\\)\nML-Algorithmus Kategorie\n\n\n\n\nFragebogenantworten\nKundensegmentierung\nClustering\n\n\nSensormessungen\nalles normal?\nAnomalieerkennung\n\n\nVergangene Nutzung einer Maschine\nRestlebensdauer\nRegression\n\n\nE-Mail\nSpam (ja/nein)\nKlassifikation (binär)\n\n\nBild\nwelches Tier?\nKlassifikation (mehrere Klassen)\n\n\nBisherige Einkäufe des Nutzers\nProduktvorschläge\nEmpfehlungssysteme\n\n\nSuchanfrage\nrelevante Dokumente\nInformation Retrieval\n\n\nAudio\nText\nSpracherkennung\n\n\nText auf Englisch\nText auf Französisch\nMaschinelle Übersetzung\n\n\n\nZusammengefasst (siehe auch: Übersichtstabelle als PDF):\nExistierende ML-Lösungen & entsprechende Outputs (für einen Datenpunkt):\n\nDimensionsreduktion: (normalerweise) 2D-Koordinaten (um den Datensatz zu visualisieren)\nAusreißer-/Anomalieerkennung: Anomalie-Score (normalerweise ein Wert zwischen 0 und 1, der angibt, inwiefern dieser Punkt von der Norm abweicht)\nClustering: Cluster-Index (eine Zahl zwischen 0 und k-1, die angibt, zu welchem der k Cluster ein Datenpunkt gehört (oder -1 für Ausreißer))\nRegression: ein kontinuierlicher Wert (eine numerischer Größe, die vorhergesagt werden soll)\nKlassifikation: ein diskreter Wert (eine von mehreren sich gegenseitig ausschließenden Kategorien)\nGenerative AI: unstrukturierte Outputs wie Text oder Bild (z.B. Spracherkennung, maschinelle Übersetzung, Bild Generierung oder Neural Style Transfer)\nEmpfehlungssysteme & Information Retrieval: Rangliste einer Menge von Elementen (Empfehlungssysteme ordnen beispielsweise die Produkte nach Relevanz für den jeweiligen Nutzer; Information Retrieval Systeme sortieren Elemente nach ihrer Ähnlichkeit zu einer gegebenen Suchanfrage)\nReinforcement Learning: eine Sequenz von Aktionen (abhängig vom Zustand in dem sich der Agent befindet)\n\nBeginnen wir mit einem detaillierteren Blick auf die verschiedenen Unsupervised und Supervised Learning Algorithmen und wie sie uns helfen können:\n\n\n\nFür die Anwendung von Unsupervised Learning Algorithmen braucht man nur eine Feature Matrix \\(X\\), während man zum Erlernen eines Vorhersagemodells mit Supervised Learning Algorithmen auch die dazugehörigen Labels \\(\\mathbf{y}\\) benötigt.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nAuch wenn unser eigentliches Ziel darin besteht, etwas vorherzusagen (also Supervised Learning zu verwenden), kann es dennoch sehr hilfreich sein, zunächst Unsupervised Learning Algorithmen anzuwenden, um den Datensatz besser zu verstehen. Beispielsweise kann man die Daten im Vorfeld mit Dimensionsreduktionsmethoden visualisieren, um alle Datenpunkte und ihre Vielfalt auf einen Blick zu sehen. Anschließend kann man den Datensatz bereinigen, in dem man Ausreißer identifiziert. Bei einem Klassifikationsproblem ist es häufig sinnvoll, die Datenpunkte zuerst zu clustern, um zu überprüfen, in wie weit die angegebenen Klassenlabels mit den natürlich vorkommenden Gruppen in den Daten übereinstimmen. Beispielsweise sieht man dann vielleicht, dass man das Problem vereinfachen kann, in dem man zwei sehr ähnliche Klassen kombiniert.\n\n\n\nGleicher Datensatz, unterschiedliche Anwendungsfälle\nWir veranschaulichen den Nutzen der fünf verschiedenen Arten von Unsupervised und Supervised Learning Algorithmen in dem wir sie auf diesen Beispieldatensatz anwenden:\n\nDies ist ein kleiner Musterdatensatz mit strukturierten Daten über verschiedene Wohnungen, wie sie jemand über eine Immobilienwebsite gesammelt haben könnte, z.B. die Größe der Wohnung in Quadratmetern, die Anzahl der Schlafzimmer, die Anzahl der Badezimmer, das Jahr der letzten Renovierung und schließlich den Preis der Wohnung und ob sie zu diesem Preis verkauft wurde (1) oder nicht (0).\n\n\nm2\n# Schlafz.\n# Bad\nRenoviert\n…\nPreis\nVerkauft\n\n\n\n\n125\n4\n2\n2000\n…\n500k\n1\n\n\n75\n2\n1\n1990\n…\n350k\n1\n\n\n150\n6\n2\n2010\n…\n750k\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n35\n5\n2\n1999\n…\n620k\n0\n\n\n65\n3\n1\n2015\n…\n220k\n1\n\n\n100\n3\n1\n2003\n…\n450k\n0\n\n\n\n\n\nDimensionsreduktion\nAnwendungsfälle:\n\nErstellen einer 2D-Visualisierung, um den Datensatz im Ganzen zu überblicken, wobei wir oft bereits beim Draufschauen Muster identifizieren können, wie Datenpunkte, die zusammen gruppiert werden können (Cluster) oder die nicht ins Bild passen (Ausreißer)\nRauschunterdrückung und/oder Feature-Engineering als Datenvorverarbeitungsschritt zur Verbesserung der Ergebnisse eines Vorhersagemodells\n\n\nBeispiel Unsupervised Learning: Dimensionsreduktion\nZiel: Datensatz visualisieren\n\n\n\nDer erste Schritt bei der Analyse eines neuen Datensatzes besteht normalerweise darin, diesen zu visualisieren, um einen besseren Überblick über alle Datenpunkte und ihre Vielfalt zu erhalten. Dafür eignet sich ein Dimensionsreduktionsalgorithmus, der die ursprünglichen hochdimensionalen Daten als Input nimmt, wobei jede Spalte (= Feature) in der Tabelle eine Dimension darstellt, und eine niedrigerdimensionale Darstellung der Datenpunkte ausgibt, d.h. eine neue Matrix mit weniger Spalten (normalerweise zwei für eine Visualisierung). Diese beiden neuen Features, in unserem Fall \\(z_1\\) und \\(z_2\\) genannt, können jetzt verwendet werden, um einen Scatterplot des Datensatzes zu erstellen. Dabei wird jeder Datenpunkt / Zeile (also in diesem Fall jede Wohnung) als Punkt in diesem neuen 2D-Koordinatensystems dargestellt. Wir können uns diese Darstellung als eine Landkarte unseres Datensatzes vorstellen, die es uns ermöglicht, alle Datenpunkte auf einen Blick zu sehen und evtl. schon interessante Muster zu erkennen, z.B. Gruppen ähnlicher Datenpunkte, die auf dieser 2D-Karte eng beieinander liegen. Aber bitte beachte: was sich hinter diesen neuen Koordinaten verbirgt lässt sich bei den meisten Dimensionsreduktionsmethoden nicht genau nachvollziehen. Insbesondere sind dies nicht einfach die beiden informativsten Originalfeatures, sondern völlig neue Variablen, die die Informationen der ursprünglichen Inputs zusammenfassen. Um den Scatterplot besser interpretieren zu können, ist es oft hilfreich, die Punkte nachträglich mit den Werten einer Variablen einzufärben, wodurch möglicherweise die treibenden Faktoren hinter den auffälligsten Mustern im Datensatz aufgedeckt werden können. In diesem Beispiel hätten wir die Punkte nach dem Preis der entsprechenden Wohnung einfärben können und dadurch gesehen, ob ähnlich bepreiste Wohnungen nah beieinander liegen.\n\n\n\nMögliche Herausforderungen:\n\ndie Transformation der Daten mit Dimensionsreduktionsmethoden konstruiert neue Features als (nicht)lineare Kombination der ursprünglichen Features, was die Interpretation der nachfolgenden Analyseergebnisse erschwert\n\n\n\nAnomalieerkennung\nAnwendungsfälle:\n\nBereinigung der Daten, z.B. durch Entfernen von Datenpunkten mit falsch eingegebenen Werten, als Datenvorverarbeitungsschritt zur Verbesserung der Ergebnisse eines Vorhersagemodells\nWarnung für Anomalien einrichten, zum Beispiel:\n\nBetrugserkennung: Identifizierung betrügerischer Kreditkartentransaktionen im E-Commerce\nÜberwachen einer Maschine, um zu erkennen, wenn etwas Außergewöhnliches passiert oder die Maschine möglicherweise gewartet werden muss\n\n\n\nBeispiel Unsupervised Learning: Anomalieerkennung\nZiel: Ausreißer im Datensatz finden\n\n\n\nAls nächstes prüfen wir unseren Datensatz auf Ausreißer/Anomalien (Outliers/Anomalies), um diese Datenpunkte dann gegebenenfalls nachträglich zu korrigieren oder zu entfernen. Ein Algorithmus zur Anomalieerkennung gibt für jeden Datenpunkt einen Anomalie-Score aus, der angibt, inwieweit dieser Datenpunkt von der Norm abweicht. Diese Scores können wir auch verwenden, um die 2D-Landkarte unseres Datensatzes aus dem vorherigen Schritt einzufärben. Dadurch werden die Anomalien im Kontext sichtbar. Leider sagt uns ein Anomalieerkennungsalgorithmus nicht, warum ein bestimmter Punkt als Ausreißer erkannt wurde. Ein Data Scientist muss deshalb die gefundenen Anomalien untersuchen und entscheiden, ob diese z.B. aufgrund fehlerhafter Messungen entfernt werden sollten oder ob sie interessante Sonderfälle darstellen. In unserem Beispiel handelt es sich bei dem als Anomalie identifizierten Datenpunkt um eine Wohnung, die angeblich nur eine Größe von 35\\(m^2\\), aber gleichzeitig 5 Schlafzimmer hat. Da liegt die Vermutung nahe, dass hier beim Eintragen der Daten ein Fehler passiert ist und die Größe der Wohnung stattdessen 135\\(m^2\\) sein sollte.\n\n\n\nMögliche Herausforderungen:\n\ndu solltest immer einen guten Grund haben, Datenpunkte weg zu lassen – Ausreißer sind selten zufällig, manchmal sind dies interessante Randfälle, die nicht ignoriert werden sollten\n\n\n\nClustering\nAnwendungsfälle:\n\nIdentifizieren von Gruppen verwandter Datenpunkte, zum Beispiel:\n\nKundensegmentierung für gezielte Marketingkampagnen\n\n\n\nBeispiel Unsupervised Learning: Clustering\nZiel: Natürlich vorkommende Gruppen im Datensatz finden\n\n\n\nAls letzte explorative Analyse können wir überprüfen, ob der Datensatz natürlich vorkommende Gruppen enthält. Das funktioniert mit einem Clustering-Algorithmus, der einen Cluster-Index für jeden Datenpunkt zurückgibt, wobei Punkte mit demselben Index in der selben Gruppe sind. Bitte beachte, dass diese Cluster-Indizes nicht geordnet sind und beim erneuten Ausführen des Algorithmus den Datenpunkten möglicherweise andere Nummern zugewiesen werden. Die Datenpunkte, denen vorher die gleiche Nummer zugewiesen wurde, sollten allerdings immer noch im gleichen Cluster sein, nur kann es eben sein, dass dieser Cluster nun ‘5’ statt ‘3’ heißt. Auch diese Cluster-Indizes können wir zur Einfärbung unserer Daten-Landkarte verwenden, um die Cluster im Kontext zu sehen. Allerdings sagt uns auch ein Clustering-Algorithmus wieder nur, welche Punkte sich ähnlich genug sind, um zusammen gruppiert zu werden, aber nicht, warum die Punkte dem Cluster zugeordnet wurden und was die Cluster bedeuten. Die Data Scientistin muss auch hier wieder die Ergebnisse untersuchen und versuchen zu interpretieren, worin sich die Cluster unterscheiden. In unserem Beispiel könnten die Cluster “billige Studiowohnungen”, “große Familienwohnungen” und “luxuriöse Penthouse Wohnungen” sein. Beim Unsupervised Learning gibt es keine richtige Lösung und ein anderer Clustering-Algorithmus könnte andere Ergebnisse liefern. Verwende einfach die Lösung, die für deinen Anwendungsfall am hilfreichsten ist.\n\n\n\nMögliche Herausforderungen:\n\nkeine Ground Truth: Modell- und Parameterselektion nicht trivial → die Algorithmen werden immer etwas finden, aber ob dies sinnvoll ist (d.h. was die identifizierten Muster bedeuten), kann ein Menschen in einem Nachbearbeitungsschritt bestimmen\nviele der Algorithmen beruhen auf Ähnlichkeiten oder Distanzen zwischen Datenpunkten, und es kann schwierig sein, dafür ein geeignetes Maß zu definieren oder im Voraus zu wissen, welche Merkmale verglichen werden sollten (z.B. was macht zwei Kunden ähnlich?)\n\n\nUnsupervised Learning hat keine zugrundeliegende Wahrheit\nBei Unsupervised Learning Problemen sollte uns bewusst sein, dass es keine richtigen oder falschen Antworten gibt. Unsupervised Learning Algorithmen erkennen lediglich Muster in den Daten. Das Ergebnis kann für uns Menschen sinnvoll sein oder auch nicht.\nEin Beispiel: Im Bereich Unsupervised Learning gibt es eine Reihe verschiedener Algorithmen, die Datenpunkte in Cluster gruppieren. Dabei arbeitet jeder Algorithmus nach einer etwas anderen Strategie und bewertet unterschiedlich, ab wann zwei Punkte ähnlich genug sind, dass sie in denselben Cluster eingeordnet werden können.\n\n\n\nDie offensichtlichste Reaktion eines Menschen ist, diese Bilder nach den darauf gezeigten Früchten zu sortieren. Theoretisch ist es jedoch genauso richtig, die Bilder anhand eines anderen Merkmals zu gruppieren, z.B. der Hintergrundfarbe, ob an der Frucht ein Blatt hängt, in welche Richtung der Stiel zeigt usw.\n\n\nEs ist unsere Aufgabe, die Ergebnisse eines Unsupervised Learning Algorithmus zu untersuchen und zu verstehen. Entsprechen die Resultate nicht unseren Erwartungen, spricht nichts dagegen, einen anderen Algorithmus auszuprobieren.\n\n\n\nRegression & Klassifikation\nAnwendungsfälle:\n\nLerne ein Modell, um eine Input-Output-Beziehung zu beschreiben und Vorhersagen für neue Datenpunkte zu treffen, zum Beispiel:\n\nvor der Produktion vorhersagen, ob ein unter den vorgeschlagenen Prozessbedingungen hergestelltes Produkt von hoher Qualität oder Ressourcenverschwendung sein wird\nChurn Prediction: Erkenne Kunden, die kurz davor stehen, ihren Vertrag zu kündigen (oder Mitarbeiter, die kurz davor stehen zu kündigen), damit du sie kontaktieren und überzeugen kannst zu bleiben\nPreisoptimierung: Bestimme den optimalen Preis für ein Produkt (oft für dynamische Preisgestaltung verwendet, z.B. um Preise basierend auf dem Gerät anzupassen, das ein Kunde verwendet, wenn er auf eine Website zugreift, wie z.B. ein neues iPhone gegenüber einem alten Android-Handy)\nPredictive Maintenance: Sage voraus, wie lange ein Maschinenbauteil halten wird\nUmsatzprognosen: Sag den Umsatz in den kommenden Wochen und den erforderlichen Lagerbestand voraus, um die Nachfrage zu befriedigen\n\n\n\n\nBeispiel Supervised Learning: Klassifikation\nZiel: Vorhersage eines diskreten Werts für jeden Datenpunkt\n\n\n\nNun könnten wir vorhersagen, ob eine Wohnung zum angegebenen Preis verkauft wird. Da die Variable “Verkauft” nur die diskreten Werte ‘ja’ (1) oder ‘nein’ (0) annehmen kann, ist dies ein binäres Klassifikationsproblem. Ein Klassifikationsmodell nimmt die Attribute einer Wohnung zusammen mit dem Angebotspreis als Input und berechnet dann, ob die Wohnung zu diesem Preis verkauft wird oder nicht. Da wir hier nun die wahren Labels kennen (zumindest für den ursprünglich gesammelten Datensatz), können wir die Genauigkeit des Modells bewerten, indem wir berechnen, wie viele falsche Vorhersagen es generiert. Das ist ein Vorteil beim Supervised Learning: Wir können objektiv bestimmen, wie gut eine Lösung ist, und somit verschiedene Modelle systematisch miteinander vergleichen, während die Data Scientistin beim Unsupervised Learning die Ergebnisse manuell im Detail untersuchen muss, um sie zu bewerten.\n\n\n\n\nBeispiel Supervised Learning: Regression\nZiel: Vorhersage eines kontinuierlichen Werts für jeden Datenpunkt\n\n\n\nSchließlich möchten wir vielleicht für eine neue Wohnung einen angemessenen Preis vorhersagen. Da Preise kontinuierliche Werte sind, ist dies ein Regressionsproblem. Das Modell verwendet die Attribute der Wohnungen als Input und schlägt dann einen geeigneten Preis vor. Da uns für die erhobenen Daten die tatsächlichen (bzw. vom Immobilienmakler bestimmten) Preise vorliegen, können wir erneut berechnen, inwieweit das Regressionsmodell mit seinen Schätzungen vom Originalpreis abweicht.\n\n\n\nMögliche Herausforderungen:\n\nErfolg ungewiss: die Anwendung der Algorithmen ist zwar relativ einfach, aber es ist schwierig, im Voraus festzustellen, ob überhaupt ein Zusammenhang zwischen den gemessenen Inputs und Outputs besteht (→ Achtung: Garbage in, Garbage out!)\nangemessene Definition des Ergebnisses/Ziels/KPI, das modelliert werden soll, d.h. was bedeutet es eigentlich, dass ein Prozess gut läuft, und wie könnten externe Faktoren diese Definition beeinflussen (können wir z.B. die gleiche Leistung in einem außergewöhnlich heißen Sommertag erwarten?)\nwichtige Inputs fehlen, z.B. wenn es andere Einflussfaktoren gibt, die wir nicht berücksichtigt haben oder nicht messen konnten, wodurch nicht die gesamte Varianz der Zielgröße erklärt werden kann\nviele möglicherweise irrelevante Inputs, die eine sorgfältige Feature Selektion erfordern, um Scheinkorrelationen zu vermeiden, die zu falschen “Was-wäre-wenn”-Prognosen führen würden, da die wahre kausale Beziehung zwischen den Inputs und Outputs nicht erfasst wird\noft sehr zeitintensive Datenvorverarbeitung notwendig, z.B. bei der Zusammenführung von Daten aus unterschiedlichen Quellen und manuellem Feature Engineering\n\n\n\nDeep Learning & Generative AI\nAnwendungsfälle:\n\nAutomatisierung langwieriger, repetitiver Aufgaben, die sonst ein Menschen erledigt würde, z.B. (siehe auch ML ist überall!):\n\nTextklassifizierung (z.B. Spam / Hate Speech / Fake News erkennen; Kundensupportanfragen an die passende Abteilung weiterleiten)\nSentimentanalyse (Teilaufgabe der Textklassifikation: Positive oder negative Texte erkennen, z.B. um Produktbewertungen oder das, was Social-Media-Nutzer über ein Unternehmen sagen, zu überwachen)\nSpracherkennung (z.B. diktierte Notizen transkribieren oder Videos mit Untertiteln versehen)\nmaschinelle Übersetzung (Texte von einer Sprache in eine andere übersetzen)\nBildklassifizierung / Objekterkennung (z.B. Identifizierung problematischer Inhalte (wie Kinderpornografie) oder Erkennung von Straßenschildern und Fußgängern beim autonomen Fahren)\nBildbeschreibungen generieren (z.B. um das Online-Erlebnis für Menschen mit Sehbehinderung zu verbessern)\nPredictive Typing (z.B. bei der Texteingabe auf dem Smartphone mögliche nächste Wörter vorschlagen)\nDatengenerierung (z.B. neue Fotos/Bilder von bestimmten Objekten oder Szenen generieren)\nStyle Transfer (ein Bild in einen anderen Stil zeigen, z.B. Fotos wie van Gogh-Gemälde aussehen lassen)\neinzelne Quellen eines Audiosignals trennen (z.B. einen Song entmischen, d.h. Gesang und Instrumente in einzelne Spuren trennen)\n\nklassische Simulationsmodelle durch ML Modelle ersetzen: da exakte Simulationsmodelle oft langsam sind, kann die Berechnung für neue Datenpunkte beschleunigt werden, indem die Ergebnisse stattdessen mit einem ML-Modell vorhergesagt werden, z.B.:\n\nAlphaFold: 3D-Proteinstruktur aus Aminosäuresequenz generieren (zur Erleichterung der Arzneimittelentwicklung)\nSchNet: Energie und andere Eigenschaften von Molekülen anhand ihrer Atomkonfiguration vorhersagen (um die Materialforschung zu beschleunigen)\n\n\nMögliche Herausforderungen:\n\nAuswählen einer geeigneten neuronalen Netzarchitektur und dafür sorgen, dass das Modell gute Vorhersagen generiert; insbesondere beim Ersetzen traditioneller Simulationsmodelle ist es häufig erforderlich, eine völlig neue Art von neuronaler Netzarchitektur zu entwickeln, die speziell für diese Aufgabe und Inputs/Outputs ausgelegt ist, was viel ML- und Domänenwissen, Intuition und Kreativität erfordert\nRechenressourcen (trainiere kein neuronales Netz ohne GPU!)\nDatenqualität und -quantität: es werden viele konsistent (von Menschen) gelabelte Daten benötigt\n\n\n\nInformation Retrieval\nAnwendungsfälle:\n\nVerbesserte Suchergebnisse durch Identifizierung ähnlicher Artikel, zum Beispiel:\n\nbei einer Suchanfrage passende Dokumente / Websites zurückgeben\ngegeben einem Film, den der Nutzer gerade anschaut, ähnliche Filme anzeigen (z.B. gleiches Genre, gleicher Regisseur usw.)\n\n\nMögliche Herausforderungen:\n\nQualität der Ergebnisse hängt stark von der gewählten Ähnlichkeitsmetrik ab; die Identifizierung semantisch verwandter Elemente ist derzeit für einige Datentypen (z.B. Bilder) schwieriger als für andere (z.B. Text)\n\n\n\nEmpfehlungssysteme\nAnwendungsfälle:\n\npersonalisierte Vorschläge: gegeben einer Instanz (z.B. Benutzer, Proteinstruktur) die relevantesten Elemente identifizieren (z.B. Film, Arzneimittelzusammensetzung), zum Beispiel:\n\neinem Nutzer Filme vorschlagen, die anderen Nutzern mit ähnlichem Geschmack ebenfalls gefallen haben\nMolekülstrukturen empfehlen, die in eine, für eine bestimmte Krankheit relevante, Proteinstruktur passen könnte\n\n\nMögliche Herausforderungen:\n\nwenig / unvollständige Daten, z.B. mögen verschiedene Nutzer denselben Artikel aus unterschiedlichen Gründen und es ist unklar, ob z.B. ein Nutzer einen Film nicht angesehen hat, weil er sich nicht dafür interessiert oder weil er ihn einfach noch nicht gefunden hat\n\n\n\nReinforcement Learning\nAnwendungsfälle:\n\nErmittlung einer optimalen Handlungsabfolge bei wechselnden Umgebungsbedingungen, z.B.:\n\nVirtueller Agent, der ein (Video-)Spiel spielt\nRoboter mit komplexen Bewegungsmustern, z.B. Aufnehmen unterschiedlich geformter Gegenstände aus einer Kiste\n\n\n⇒ Anders als bei der regulären Optimierung, wo eine optimale Eingabe für einen spezifischen externen Zustand bestimmt wird, versucht hier ein “Agent” (= der RL-Algorithmus) eine optimale Reihenfolge von Eingaben zu finden, um die kumulative Belohnung über mehrere Schritte zu maximieren. Dabei kann zwischen einer Handlung und der dazugehörigen Belohnung eine erhebliche Zeitverzögerung liegen (z.B. wenn in einem Videospiel zu Beginn eines Levels ein Schlüssel gefunden werden muss, aber die Tür, die damit geöffnet werden kann, erst einige Frames später kommt).\nMögliche Herausforderungen:\n\nerfordert normalerweise eine Simulationsumgebung, in der der Agent “angelernt” wird, bevor er anfängt, in der realen Welt zu handeln. Die Entwicklung eines exakten Simulationsmodells ist allerdings nicht einfach und der Agent wird alle Bugs ausnutzen, wenn dies zu höheren Belohnungen führt\nes kann schwierig sein, eine klare Belohnungsfunktion zu definieren, die optimiert werden soll (“Imitation Learning” ist dabei oft einfacher, wobei der Agent stattdessen versucht, die Entscheidungen eines Menschen in einer bestimmten Situation nachzuahmen)\nlange Verzögerungen zwischen kritischen Aktionen und der dazugehörigen Belohnung erschweren das lernen korrekter Assoziationen\nder Agent generiert seine eigenen Daten: Wenn er mit einer schlechten Policy startet, wird es schwierig, dieser zu entkommen (z.B. wenn der Agent in einem Videospiel immer in eine Lücke fällt, anstatt darüber zu springen, sieht er nie die Belohnung, die auf der anderen Seite wartet und lernt daher nicht, dass es von Vorteil wäre, über die Lücke zu springen)\n\n\n\nAndere\n\n\n\n\n\n\nWichtig\n\n\n\nML-Algorithmen werden anhand des Outputs kategorisiert, den sie für eine Eingabe generieren. Wenn man ein ‘Input → Output’-Problem mit einem anderen als den oben aufgeführten Outputs lösen möchte, wird das wahrscheinlich auf ein mehrjähriges Forschungsprojekt hinauslaufen – wenn das Problem überhaupt mit ML gelöst werden kann!\n\n\n\nUm komplexe Probleme zu lösen, benötigt man möglicherweise mehrere Algorithmen\nBeispiel: virtueller Assistent (z.B. Siri oder Alexa): “Hey &lt;Sprachassistent&gt;, erzähl mir einen Witz!” → ein zufälliger Witz\n\nDas sieht zwar zunächst wie ein Input-Output-Problem aus, es direkt zu lösen wäre allerdings sehr schwierig und ineffizient. Stattdessen zerlegen wir das Problem in kleinere Teilaufgaben, die mit bestehenden Algorithmen gelöst werden können:\n\n\nTriggerwort Erkennung:\nAudio → “Hey &lt;Sprachassistent&gt;” (ja/nein)?\nSpracherkennung:\nAudio → Text\nKlassifizierung der Absicht:\nText → (Witz/Timer/Wettervorhersage/…)?\nFühre spezifisches Programm aus (z.B. wähle zufälligen Witz aus)\nSprachgenerierung:\nText → Audio\n\n\nZunächst muss der Smart Speaker wissen, ob er mit einem bestimmten Triggerwort (z.B. “Hey Siri”) angesprochen wurde. Dies ist eine einfache binäre Klassifikation (ja/nein), die normalerweise auf dem Gerät selbst ausgeführt wird, da wir nicht möchten, dass alles was wir sagen permanent in die Cloud gestreamt wird. Als nächstes werden die nach dem Triggerwort gesprochenen Wörter in Text übersetzt. Text ist einfacher zu handhaben, da beispielsweise Variationen aufgrund unterschiedlicher Akzente entfernt werden. Anhand dieses Textes wird die Absicht des Nutzers erkannt, also welche der verschiedenen Funktionalitäten des virtuellen Assistenten genutzt werden soll (z.B. Witz erzählen, Musik abspielen, Wecker stellen etc.). Dies ist ein Multiclass-Klassifikationsproblem. Zur Ausführung des Befehls wird kein ML benötigt, sondern ein normales, aufgabenspezifisches Programm, das als App auf dem Gerät installiert ist, z.B. wird ein Witz aus einer Datenbank ausgewählt oder ein Timer gesetzt usw.. Anschließend muss der Output des ausgeführten Programms wieder in ein Audiosignal umgewandelt werden. Ein ML-Modell kann dabei helfen, flüssig gesprochenen Text zu erzeugen – und in naher Zukunft vielleicht auch mit der Stimme von Morgan Freeman oder einer anderen berühmten Person, wie bei “Deep Fake”-Anwendungen.\n\n⇒ Generell sollte man zunächst darüber nachzudenken, ob man ein Problem in leichter zu lösende Teilprobleme zerlegen kann, da es dafür oft schon große Datensätze oder sogar fertige Modelle gibt. Ein Modell für Spracherkennung kann beispielsweise zusätzlich auf Hörbüchern und transkribierten politische Reden trainiert werden, nicht nur auf den Daten, die von den Sprachassistent-Nutzern gesammelt wurden.\n\n\n\n\n\n\nVorsicht\n\n\n\nWenn ein ML-Modell als Input den Output eines anderen ML-Modells erhält, bedeutet dies, dass wir, sobald wir eine neue Version des ersten ML-Modells verwenden, auch die folgenden Modelle neu trainieren sollten, da diese Modelle dann evtl. leicht andere Inputs erhalten, d.h. wir es mit einem Daten Drift zu tun haben.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "01b_basics.html#sec-solving-problems",
    "href": "01b_basics.html#sec-solving-problems",
    "title": "Grundlagen",
    "section": "Mit ML Probleme lösen",
    "text": "Mit ML Probleme lösen\nDas Lösen von “Input → Output”-Problemen mit ML erfordert drei Hauptschritte:\n\n\n\n\n\n\n1. Identifiziere ein Problem\nDer erste (und wohl wichtigste) Schritt besteht darin, zu identifizieren, wo maschinelles Lernen überhaupt eingesetzt werden kann (und sollte).\n\nSchritte zur Identifizierung eines potenziellen ML-Projekts\n\nErstelle ein Prozessdiagramm: Welche Schritte werden in einem Geschäftsprozess ausgeführt und welche Daten werden wo gesammelt (Material- & Informationsfluss). Zum Beispiel in einem Produktionsprozess, bei dem einige der produzierten Teile fehlerhaft sind:\n\n\n\n\n\nIdentifiziere Teile des Prozesses, die entweder mit ML automatisiert werden könnten (z.B. einfache, sich wiederholende Aufgaben, die sonst von Menschen erledigt werden) oder die auf andere Weise durch eine Analyse von Daten verbessert werden könnten (z.B. um die Ursachen eines Problems zu verstehen, die Planung mit Was-wäre-wenn-Simulationen zu verbessern, oder die Ressourcennutzung zu optimieren):\n\n\n\nDie erste Idee besteht darin, die bisher von einem Menschen durchgeführte Qualitätsprüfung zu automatisieren: Da der Mensch die Defekte in den aufgenommenen Produktbildern leicht erkennen kann, sollte ein ML-Modell dies auch schaffen. Die nächste Idee besteht darin, anhand der Zusammensetzung der Rohstoffe und der gegebenen Prozessbedingungen vor der Herstellung vorherzusagen, ob ein Produkt fehlerhaft sein wird oder nicht: Der Erfolg ist hier unklar, da eine menschliche Expertin nicht von vorne herein abschätzen kann, ob alle relevanten Informationen dafür in diesen Daten enthalten sind. Aber dennoch wäre es einen Versuch wert, da man dadurch viele Ressourcen sparen könnte. Während das endgültige ML-Modell, welches das Input-Output-Problem löst, als Software im laufenden Betrieb eingesetzt werden kann, kann ein Data Scientist zusätzlich die Ergebnisse analysieren und das Modell interpretieren und somit Erkenntnisse gewinnen und Handlungsempfehlungen aussprechen.\n\n\nPriorisieren: Welches Projekt hätte eine große Wirkung und gleichzeitig gute Erfolgsaussichten, hätte also einen hohen Return on Investment (ROI)? Als Beispiel: Die Verwendung von ML zur Automatisierung einer einfachen Aufgabe ist aus technischer Sicht eine vergleichsweise risikoarme Investition, kann jedoch dazu führen, dass einige Fließbandarbeiter ihre Jobs verlieren. Andererseits könnten durch die Ermittlung der Ursachen, warum in einem Produktionsprozess 10% Ausschuss produziert werden, Millionen eingespart werden. Allerdings ist nicht vorhersehbar, ob eine solche Analyse tatsächlich nützliche Ergebnisse liefert, da die gesammelten Daten zu den Prozessbedingungen möglicherweise nicht alle erforderlichen Informationen enthalten.\n\n\n\nML Projekt Checkliste\nMotivation\n\nWelches Problem möchtest du lösen?\nMachine Learning kann dir in verschiedenen Situationen helfen, z.B. indem es Erkenntnisse aus großen Mengen (möglicherweise unstrukturierter) Daten generiert, Entscheidungsfindungs- und Planungsprozesse durch Vorhersagen über zukünftige Ereignisse verbessert, oder mühsame Aufgaben automatisiert, die sonst menschliche Experten erfordern.\nWo läuft etwas ineffizient, was durch eine effektivere Nutzung von Daten verbessert werden könnte? Du könntest zum Beispiel nach Möglichkeiten suchen, verschwendete Ressourcen/Zeit/Kosten zu reduzieren oder den Umsatz/die Kundenzufriedenheit/usw. zu steigern.\nUm systematisch Probleme oder Verbesserungsmöglichkeiten zu erkennen, kann es auch helfen, ein Prozessdiagramm oder eine Customer Journey Map zu erstellen.\nAuf welche Art und Weise würde dies Wert für eure Organisation generieren?\nWie könnte eure Organisation damit Geld verdienen oder Kosten senken?\n\nKönnte dies einen internen Prozess verbessern (z.B. könnte ein Prozess mit den Erkenntnissen aus einer Analyse effizienter gestaltet werden oder eine lästige Aufgabe, die sonst einen menschlichen Arbeiter erfordern würde, kann mithilfe eines ML-Modells automatisiert werden)?\nKönnte das ML-Modell als neues Feature in ein bestehendes Produkt integriert werden und dadurch z.B. dieses Produkt für Kunden attraktiver machen?\nKönnte die ML-Lösung als völlig neues Produkt oder Service verkauft werden, z.B. als Software-as-a-Service (SaaS)-Lösung angeboten werden?\n\n\n\nBitte beachte, dass die letztendliche Verwendung der ML-Lösung auch eine strategische Entscheidung sein kann, die für jede Organisation unterschiedlich sein kann. Beispielsweise könnte eine ML-Lösung, die Kratzer auf produzierten Produkten erkennt, von einem Unternehmen verwendet werden, um ihren internen Produktionsprozess zu verbessern, während ein anderes Unternehmen, das die Maschinen herstellt, die die Produkte herstellen, dies als neues Feature in seine Maschinen integrieren könnte, und ein drittes Unternehmen bietet dies möglicherweise als SaaS-Lösung an, die mit verschiedenen Produktionslinien kompatibel ist.\n\n\nIn welcher Größenordnung könnte dieses Projekt Mehrwerte generieren?\nÜberlege dir den Impact im Hinblick auf\n\nGrößenordnung: Kleine Verbesserung oder Revolution? Würde die Lösung einen strategischen Vorteil schaffen?\nSkalierung: Wie oft wird es verwendet? Wie viele Benutzer/Kunden/Mitarbeiter werden davon profitieren?\nZum Beispiel:\n\nKleine Prozessoptimierung, aber da dieser Prozess täglich in der gesamten Organisation verwendet wird, spart dies unzählige Stunden\nNeue Funktion, die ein Produkt revolutioniert und euch von der Konkurrenz abhebt, aber der Markt dafür ist winzig\n\nHätte dies irgendwelche wertvollen Nebenwirkungen? Was wird anders sein? Gibt es zusätzliche Möglichkeiten, die sich daraus ergeben könnten? Können Synergien geschaffen werden zwischen Abteilungen, die mit ähnlichen Daten arbeiten?\n\nWann hast du dein Ziel erreicht?\nWie würde Erfolg aussehen, d.h. was ist deine Definition von ‘fertig’?\n\nKannst du den Fortschritt mit einem KPI quantifizieren?\nWie ist der Status quo, d.h. wie weit bist du derzeit von deinem Ziel entfernt?\nWelche Metriken sollten sich nicht verändern (verschlechtern) durch das Projekt?\n\n\nLösungsansatz\n\nWie sieht deine Vision für die Zukunft mit ML aus?\n\nWie sieht dein bestehender Prozess / dein bestehendes System aus und was wird nach der Integration der ML-Lösung anders sein?\nWer sind die Nutzer und wie werden sie von dieser Veränderung betroffen sein, brauchen sie z.B. zusätzliche Schulungen, um das neue System zu nutzen?\n\nWas sind die Deliverables?\nBesteht die Lösung aus einer Software, die irgendwo eingesetzt wird, um kontinuierlich Vorhersagen für neue Datenpunkte zu generieren, oder interessierst du dich für die Erkenntnisse, die aus einer einmaligen Analyse historischer Daten gewonnen werden?\nWie wird im Falle einer Softwarelösung das ML-Modell in das bestehende Setup integriert?\n\nWie sieht eine Interaktion mit dem System aus (= 1 Datenpunkt / Beobachtung), z.B. ein Nutzer, der eine Anfrage stellt oder ein produziertes Produkt, das eine Qualitätskontrolle passiert?\nVon welchem System stammen die Inputs für das ML-Modell? Was passiert mit den Outputs des ML-Modells?\nWird eine zusätzliche Benutzeroberfläche (UI) oder API benötigt, um mit dem ML-Modell zu interagieren?\nMuss das ML-Modell Vorhersagen sofort treffen, sobald neue Daten eintreffen, oder kann es Daten asynchron in Batches verarbeiten? Wie hoch ist der erwartete Traffic (d.h. die Anzahl der Datenpunkte, die pro Sekunde verarbeitet werden müssen)?\nWie sollte das ML-Modell deployed werden (z.B. Cloud, On-Premise oder Edge-Gerät)? Erfordert dies zusätzliche Infrastruktur oder spezielle Hardware (z.B. GPUs)?\nModellwartung: Was sind die Pläne im Hinblick auf Pipelines für zukünftige Datenerfassung, Modellüberwachung und automatisiertes Nachtrainieren?\n\nWie sehen die Input Daten aus? Wie sollen die Outputs aussehen?\n\nWelche Art von Inputs bekommt das ML Modell (z.B. Bild / Text / Sensormessungen / etc.)?\nWelche Art von Outputs soll das ML-Modell produzieren, d.h. welche Kategorie von ML-Algorithmen löst dieses Problem?\nHast du bereits Zugriff auf einen initialen Datensatz, um das Modell zu trainieren?\n\nWie soll die Performance des ML-Modells evaluiert werden?\n\nWelche Evaluierungsmetriken sind für den gegebenen ML-Anwendungsfall geeignet (z.B. Accuracy)?\nWie stehen diese Metriken in Zusammenhang mit den Geschäfts-KPIs, die durch diese Lösung verbessert werden sollen?\nWie kann die Performance des Modells im laufenden Betrieb überwacht werden? Werden dafür kontinuierlich neue gelabelte Daten gesammelt?\n\nGibt es eine einfachere Lösung ohne ML?\nVerwende ML, um unbekannte, komplexe Regeln aus Daten zu lernen.\n\nAuch wenn ML hier die richtige Wahl ist, könntest du ein minimal funktionsfähiges Produkt ohne ML entwickeln, um die Lösung als Ganzes zu validieren, bevor du in ML investierst?\n\n\nHerausforderungen & Risiken\n\nGibt es genügend hochwertige Daten um das Modell zu trainieren und zu evaluieren?\n\nQualität: Hast du die richtigen Inputs und eindeutige Labels?\n→ Frage eine Fachexpertin, ob sie denkt, dass alle relevanten Inputs vorhanden sind, um das gewünschte Ergebnis zu berechnen. Dies ist bei unstrukturierten Daten wie Bildern in der Regel leicht zu bestimmen - wenn ein Mensch das Objekt im Bild sehen kann, sollte es ML auch können. Aber bei strukturierten Daten, wie z.B. einer Tabelle mit Hunderten von Spalten mit Sensormessungen, kann dies unmöglich zu bestimmen sein, bevor man eine Analyse der Daten durchführt.\nQuantität: Wie viele Daten wurden bereits gesammelt (einschließlich seltener Ereignisse und Labels)? Wie lange würde es dauern, mehr Daten zu sammeln? Könnten zusätzliche Daten von einem Anbieter gekauft werden und wenn ja, wie viel würde das kosten?\nWie schwierig ist es, auf alle Daten zuzugreifen und sie ordentlich an einem Ort zu kombinieren? Mit wem würdest du sprechen, um die Dateninfrastruktur einzurichten/zu verbessern?\nWie viel Vorverarbeitung ist notwendig (z.B. Entfernung von Ausreißern, Korrigieren fehlender Werte, Feature Engineering, d.h. Berechnung neuer Variablen aus den bestehenden Messungen, etc.)? Was sollten die nächsten Schritte sein, um die Datenqualität und -quantität systematisch zu verbessern und die Vorverarbeitungsanforderungen in Zukunft zu verringern?\n\nKann das Problem mit einem existierenden ML-Algorithmus gelöst werden?\nFrage eine ML-Expertin, ob ein ähnliches Problem bereits mit einem bewährten Algorithmus gelöst wurde.\n\nFür bekannte Lösungen: Wie komplex ist es, das Modell zum Laufen zu bringen (z.B. lineare Regression vs. tiefes neuronales Netz)?\nFür unbekannte Lösungen: Anstatt Jahre in die Forschung zu investieren, um einen neuartigen Algorithmus zu entwickeln, wäre es möglich, das Input-Output-Problem in einfachere Teilprobleme mit bekannten Lösungen zu zerlegen?\n\nWas würde im schlimmsten Fall passieren, wenn das Modell falsch liegt?\nDas ML-System wird (wie Menschen auch) Fehler machen. Benutze kein ML wenn du immer 100% korrekte Ergebnisse brauchst!\n\nWelches Performance-Level wird mindestens benötigt, damit die ML-Lösung Mehrwert liefert? Z.B. welche Falsch Positiv oder Falsch Negativ Rate wäre noch akzeptabel? Was wäre das Worst-Case-Szenario und wie viel Risiko bist du bereit einzugehen?\nWie wahrscheinlich ist es, dass sich die Inputs im Laufe der Zeit ändern, beispielsweise auf Grund sich ändernder Demographie der Nutzer oder durch unerwartete Ereignisse (Black Swan Events) wie eine Pandemie (z.B. COVID-19)? Wie oft müsste man das Modell nachtrainieren, um diese Veränderungen zu kompensieren und werden dafür schnell genug neue (gelabelte) Daten gesammelt?\nBesteht für die Nutzer ein Anreiz, das System absichtlich zu täuschen (z.B. entwickeln Spammer raffiniertere Nachrichten, wenn ihre ursprünglichen Nachrichten vom Spamfilter abgefangen werden)?\nGibt es eine Möglichkeit, das System erst zu überwachen und trotzdem einen Mehrwert zu generieren (z.B. mit einer “Human in the Loop” Lösung), statt vom ersten Tag an voll auf ML zu setzen?\n\nGibt es rechtliche oder ethische Bedenken beim Einsatz der Lösung?\n\nVerbietet irgendeine Verordnung, zum Beispiel das EU-Gesetz über künstliche Intelligenz (EU AI Act), den Einsatz von ML für diese Anwendung?\nGibt es datenschutzrechtliche Bedenken, zum Beispiel weil personenbezogene Daten verwendet werden?\nMüssen die Entscheidungen des ML-Modells transparent und nachvollziehbar sein, z.B. wenn jemandem aufgrund eines algorithmisch generierten Kreditscores ein Kredit verweigert wird?\nBesteht die Gefahr, dass das Modell Nutzer diskriminieren könnte, beispielsweise weil es mit verzerrten Daten trainiert wurde?\n\nWas könnte sonst noch schief gehen?\n\nWarum könnten die Nutzer von der Lösung frustriert sein? An welcher Stelle würden sie z.B. lieber mit einem echten Menschen statt einem Chatbot interagieren?\n\n\n\n\n\nGlücklicherweise sind lebensbedrohliche Situationen bei den meisten Machine Learning Anwendungsfällen kein Thema. Es ist jedoch wichtig, das Worst-Case-Szenario in Betracht zu ziehen, sollte das Modell falsche Ergebnisse liefern. Besonders bei komplexen Modellen, wie den Large Language Models (LLMs), die generative KI möglich machen, ist es oft schwer, gute Schutzmaßnahmen gegen Missbrauch zu entwickeln.\n\n\nSelbst bauen oder einkaufen?\n\nKernbereich vs. generische Anwendung: Schafft diese Lösung einen strategischen Vorteil?\nWird die Lösung ein wichtiger Bestandteil eures Geschäfts sein, z.B. ein neues Feature, das euer Produkt attraktiver macht und/oder erfordert die Lösung spezifisches Domänenwissen, das nur in eurer Organisation verfügbar ist, z.B. weil du Daten analysierst, die von euren eigenen speziellen Prozessen/Maschinen generiert werden? Oder ist dies ein allgemeines (aber komplexes) Problem, für das es bereits eine Lösung gibt (z.B. als Software-as-a-Service (SaaS)-Produkt), die ihr von der Stange kaufen könntet?\nBeispielsweise ist das Extrahieren der relevanten Informationen aus gescannten Rechnungen zur Automatisierung von Buchhaltungsprozessen eine relativ komplexe Aufgabe, für die es bereits viele gute Lösungen gibt. Wenn du nicht gerade in einem Unternehmen arbeitest, das Buchhaltungssoftware entwickelt, und ihr plant, eine bessere Alternative zu diesen vorhandenen Lösungen zu verkaufen, ist es wahrscheinlich nicht sinnvoll, dies selbst zu implementieren.\nBesitzt ihr die nötige technische Expertise und Wissen im Anwendungsbereich, um die Lösung selbst zu implementieren?\n\nWie schwierig wäre es, die ML-Lösung selbst zu implementieren? Welche Open-Source-Bibliotheken gibt es, die eine solche Aufgabe lösen?\nVerfügt eure Organisation über das nötige ML-Talent? Falls nicht könnte auch eine hybride Entwicklung zusammen mit einer Universität oder Forschungsinstitution oder externen Beratern möglich sein.\n\nWas wäre der Return on Investment (ROI) einer eingekauften Lösung?\n\nWie zuverlässig ist die eingekaufte ML-Lösung? Gibt es Benchmarks und/oder könnt ihr sie mit einigen gängigen Beispielen und Randfällen selbst testen?\nWie aufwändig wäre die Vorverarbeitung eurer Daten um die eingekaufte ML-Lösung zu verwenden?\nWie kompliziert wäre es, die Outputs der eingekauften ML-Lösung in euer System zu integrieren? Macht diese Lösung genau das, was ihr braucht, oder wären zusätzliche Nachbearbeitungsschritte erforderlich?\nKann die eingekaufte ML-Lösung intern deployed werden oder läuft sie auf einem externen Server und würde dies Datenschutzprobleme mit sich bringen?\nWie hoch sind die laufenden Lizenzgebühren und was ist in der Wartung enthalten (z.B. wie oft werden die Modelle nachtrainiert)?\n\n\nSofern die ML-Lösung kein integraler Bestandteil eures Geschäftsmodells sein wird, wird es am Ende wahrscheinlich darauf hinauslaufen, die internen Kosten für die Entwicklung, Implementierung, den Betrieb und die Wartung des Systems mit den Kosten für die Integration der Standardlösung in euren bestehenden Arbeitsablauf (einschließlich der erforderlichen Datenvorverarbeitung) und den laufenden Lizenzgebühren zu vergleichen.\nAuch wenn ihr euch dafür entscheidet, eine eigene ML-Lösung zu bauen, fängt man dabei selten komplett bei null an – in der Regel nutzt man generische Bausteine wie Open-Source-Bibliotheken oder vortrainierte Modelle.\n\n\n\nDie Entscheidung “Build or Buy” liegt oft auf einem Kontinuum: Man kann ein fertiges Tool kaufen, das direkt funktioniert; die gekaufte Software lässt sich eventuell noch verbessern, wenn man sie mit eigenen Daten finetuned; oder man entwickelt eine eigene Lösung – meist basierend auf bestehenden Komponenten wie Cloud-Infrastruktur, Open-Source-Bibliotheken oder vortrainierten Modellen.\n\n\nIn dem Zusammenhang solltet ihr euch auch überlegen, welche Teile eures ML-Produkts für Wettbewerber am schwersten zu kopieren sind: Meistens ist das die firmeneigene Datenbasis, auf der eure Modelle trainiert wurden. Zwar lassen sich viele Datensätze auch aus dem Netz ziehen (was gerade bei urheberrechtlich geschützten Inhalten rechtlich sehr umstritten ist), aber mit euren eigenen Daten hebt ihr euch wirklich ab – weil sie den speziellen Kontext eures Unternehmens widerspiegeln, nicht einfach kopierbar sind und oft auch zu besseren Modellen führen.\n\nEine kompakte Zusammenfassung dieser Punkte findest du auch im Data Product Canvas.\nFür weitere Informationen lies diesen Blog Artikel.\n\n\n2. Entwickle eine Lösung\nSobald ein geeignetes “Input → Output”-Problem identifiziert wurde, müssen historische Daten gesammelt und der richtige ML-Algorithmus ausgewählt und angewendet werden, um eine funktionierende Lösung zu erhalten. Darum geht es in den nächsten Kapiteln.\nUm ein konkretes Problem mit ML zu lösen, gehen wir in der Regel wie folgt vor:\n\n\n\nWir beginnen immer mit einer Fragestellung oder einem Problem, das mit ML gelöst werden soll. Und um es zu lösen, benötigen wir Daten, die in der Regel bereinigt werden müssen, bevor man mit ihnen arbeiten kann (z.B. verschiedene Excel-Dateien zusammenführen, fehlende Werte korrigieren usw.). Dann ist es Zeit für eine explorative Analyse, um besser zu verstehen, womit wir es zu tun haben. Abhängig von der Art der Daten ist es evtl. erforderlich, geeignete “Features” (Charakteristiken oder Kennzahlen) zu extrahieren oder zusätzlich zu berechnen, wobei Domänenwissen sehr hilfreich ist. Alle diese Schritte sind unter “Preprocessing” (Vorverarbeitung) gruppiert (roter Kasten) und die Arbeitsschritte sind nicht linear angeordnet, da wir oft zwischen diesen Schritten hin- und herspringen. Beispielsweise stellen wir bei der Visualisierung des Datensatzes fest, dass es Ausreißer in den Daten gibt, die wir entfernen müssen, oder nachdem neue Features berechnet wurden, gehen wir zurück und visualisieren den Datensatz erneut. Als nächstes kommt der ML-Teil (grüner Kasten): Normalerweise fängt man mit einem einfachen Modell an, evaluiert es, probiert ein komplexeres Modell aus, experimentiert mit verschiedenen Hyperparametern, … und stellt dann fest, dass man den ML-Werkzeugkasten durch hat, aber kein Modell eine zufriedenstellende Performance zeigt. An diesem Punkt muss man einen Schritt zurück gehen und entweder aussagekräftigere Features berechnen oder, wenn das auch nicht hilft, mehr und/oder bessere Daten sammeln (z.B. mehr Proben, Daten von zusätzlichen Sensoren, eindeutigere Labels usw.). Wenn wir uns schließlich auf die Vorhersagen des Modells verlassen können, gibt es zwei Wege, die man gehen kann: Entweder die Data Science Route, bei der die gewonnen Erkenntnisse an Stakeholder übermittelt werden (was oft zu weiteren Fragen führt). Oder die ML-Software Route, in welcher das endgültige Modell in den Prozess eingebunden wird. Aber Achtung: Die Performance des Modells muss kontinuierlich überwacht werden und es müssen auch in Zukunft weiterhin neue Daten gesammelt werden, damit das Modell immer wieder nachtrainiert werden kann, vor allem falls es Änderungen im Prozess gibt. Insgesamt ist die Arbeit an einem Machine Learning Projekt ein sehr iterativer Prozess.\n\n\nDa viele Unternehmen keine standardisierte Dateninfrastruktur besitzen, ist die traurige Wahrheit leider, dass eine Data Scientistin normalerweise (mindestens) etwa 90% ihrer Zeit damit verbringt, die Daten zu sammeln, zu bereinigen und anderweitig vorzuverarbeiten, um sie in ein Format zu bringen worauf die ML-Algorithmen angewendet werden können:\n\n\n\n\n\nAuch wenn es manchmal frustrierend ist, ist die Zeit, die man mit der Bereinigung und Vorverarbeitung der Daten verbringt, nie verschwendet, da die ML-Algorithmen nur mit einer soliden Datengrundlage brauchbare Ergebnisse erzielen können.\n\n\n3. Setze die Lösung ein\nWenn die prototypische Lösung implementiert ist und das geforderte Performance-Level erfüllt, muss diese Lösung dann “deployed” werden, d.h. produktiv in den allgemeinen Workflow und die Infrastruktur integriert werden, damit sie in der Praxis tatsächlich zur Verbesserung des jeweiligen Prozesses eingesetzt werden kann (als Software, die kontinuierlich Vorhersagen für neue Datenpunkte macht). Das könnte auch den Bau zusätzlicher Software rund um das ML-Modell erfordern, wie etwa eine API, um das Modell programmatisch abzufragen, oder eine dedizierte Benutzeroberfläche, um mit dem System zu interagieren. Schließlich gibt es im Allgemeinen zwei Strategien, wie die fertige Lösung betrieben werden kann:\n\nDas ML-Modell läuft auf einem “Edge-Gerät”, d.h. auf jedem einzelnen Gerät (z.B. Smartphone), das Inputdaten erzeugt und die Ergebnisse des Modells im nachfolgenden Prozessschritt verwendet. Dies ist oft die beste Strategie, wenn Ergebnisse in Echtzeit berechnet werden müssen und/oder eine durchgehende Internetverbindung nicht gewährleistet ist, wie z.B. bei selbstfahrenden Autos. Der Nachteil dieser Strategie ist jedoch, dass je nach Art des ML-Modells vergleichsweise teure Rechenressourcen in jedes Gerät eingebaut werden müssen, z.B. GPUs für neuronale Netze.\nDas ML-Modell läuft in der “Cloud”, d.h. auf einem zentralen Server, z.B. in Form einer Webanwendung, die Daten einzelner Nutzer entgegennimmt, verarbeitet und die Ergebnisse zurücksendet. Dies ist oft die effizientere Lösung, wenn für den Anwendungsfall eine Antwort innerhalb weniger Sekunden ausreicht. Die Verarbeitung personenbezogener Daten in der “Cloud” kann jedoch Datenschutzbedenken mit sich bringen. Einer der Hauptvorteile dieser Lösung besteht darin, dass man das ML-Modell einfacher aktualisieren kann, sobald mehr historische Daten verfügbar werden oder wenn sich der Prozess ändert und das Modell nun mit leicht anderen Eingaben umgehen muss (worauf wir in späteren Kapiteln noch ausführlicher eingehen).\n\n→ Da diese Entscheidungen stark vom spezifischen Anwendungsfall abhängen, sprengen sie den Rahmen dieses Buches. Suche online nach “MLOps” oder lies das Buch Designing Machine Learning Systems, um mehr über diese Themen zu erfahren und beauftrage eine:n Machine Learning oder Data Engineer, um die erforderliche Infrastruktur im Unternehmen einzurichten.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen</span>"
    ]
  },
  {
    "objectID": "02_data.html",
    "href": "02_data.html",
    "title": "Datenanalyse & Preprocessing",
    "section": "",
    "text": "Datenanalyse\nWie wir gesehen haben, lösen ML-Algorithmen Input-Output-Aufgaben. Und um ein ML-Problem zu lösen, müssen wir zunächst Daten sammeln, diese verstehen und dann so transformieren, dass ML-Algorithmen angewendet werden können (= Daten Vorverarbeitung / “Preprocessing”):\nDas Analysieren von Daten ist nicht nur ein wichtiger Schritt, bevor diese Daten für ein Machine Learning Projekt verwendet werden, sondern kann auch wertvolle Erkenntnisse generieren, die zu besseren (datengestützten) Entscheidungen führen. Normalerweise analysieren wir Daten aus einem von zwei Gründen:\nWas alle Arten der Datenanalyse gemein haben, ist, dass wir nach “(umsetzbaren) Erkenntnissen” suchen.\nIm besten Fall werden wichtige Kennzahlen kontinuierlich in Dashboards oder Berichten überwacht, um Abweichungen von der Norm so schnell wie möglich zu erkennen, während die Identifizierung der Ursache oft eine maßgeschneiderte Analyse erfordert.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Datenanalyse & Preprocessing</span>"
    ]
  },
  {
    "objectID": "02_data.html#sec-data-analysis",
    "href": "02_data.html#sec-data-analysis",
    "title": "Datenanalyse & Preprocessing",
    "section": "",
    "text": "Wir benötigen bestimmte Informationen, um eine (bessere) Entscheidung zu treffen (reaktive Analyse, zum Beispiel wenn etwas schief gelaufen ist und wir nicht wissen, warum).\nWir sind neugierig auf die Daten und wissen noch nicht, was die Analyse bringen wird (proaktive Analyse, zum Beispiel um die Daten zu Beginn eines ML-Projekts besser zu verstehen).\n\n\nErgebnisse einer Datenanalyse können auf verschiedene Arten generiert und kommuniziert werden\n\nEine maßgeschneiderte Analyse, deren Ergebnissen zum Beispiel in einer PowerPoint-Präsentation präsentiert werden\nEin standardisierter Bericht, zum Beispiel in Form eines PDF-Dokuments, der statische Visualisierungen historischer Daten zeigt\nEin Dashboard, also eine Webanwendung, die (fast) Echtzeitdaten zeigt, normalerweise mit interaktiven Elementen (z.B. Optionen zum Filtern der Daten)\n\n\n\n\n\n\nWährend die Datenstory in einer Präsentation in der Regel vorgegeben ist, haben Nutzer in einem interaktiven Dashboard mehr Möglichkeiten, die Daten zu interpretieren und selbst zu analysieren.\n\n\n\nWas ist sind Erkenntnisse?\nDer Psychologe Gary Klein definiert eine Erkenntnis als “eine unerwartete Veränderung in der Art, wie wir Dinge verstehen”.\n\n\n\nSpannend wird es, wenn wir in den Daten etwas unerwartetes finden. [Adaptiert von: Effective Data Storytelling von Brent Dykes]\n\n\nZu einer Erkenntnis kommen wir somit in zwei Schritten:\n\nEtwas Unerwartetes entdecken, zum Beispiel einen plötzlichen Rückgang oder Anstieg in einer Metrik.\nVerstehen, warum dies passiert ist, also tiefer in die Daten eintauchen, um die Ursache (Root Cause) zu identifizieren.\n\nWenn wir verstehen, warum etwas passiert ist, können wir oft auch eine potenzielle Maßnahme identifizieren, die uns wieder auf Kurs bringen könnte, wodurch dies zu einer umsetzbaren Erkenntnis (actionable insight) wird.\n\n\n\n\n\n\nTipp\n\n\n\nDomänenwissen ist oft hilfreich, um zu wissen, welche Werte unerwartet sind und wo es sich lohnen könnte, tiefer in die Daten einzusteigen. Daher kann es sinnvoll sein, die Ergebnisse zusammen mit einem Fachexperten durchzugehen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nAls Datenanalyst wird man manchmal mit spezifischeren Fragen konfrontiert, wie zum Beispiel “Wir überlegen, wo wir eine neue Marketingkampagne starten sollen. Kannst du mir die Anzahl der Nutzer für alle europäischen Länder zeigen?”. In solchen Fällen kann es hilfreich sein, nach dem Warum zu fragen, um zu verstehen, wo die Person etwas Unerwartetes bemerkt hat, das diese Analyseanfrage ausgelöst hat. Wenn die Antwort lautet “Oh, wir haben noch etwas Marketingbudget übrig und müssen das Geld irgendwo ausgeben”, dann gib ihnen einfach die Ergebnisse. Wenn die Antwort jedoch lautet “Unser Umsatz für dieses Quartal war niedriger als erwartet”, könnte es sich lohnen, andere mögliche Ursachen zu untersuchen, denn vielleicht liegt das Problem nicht in der Anzahl der Nutzer, die die Website besuchen, sondern darin, dass viele Nutzer vor Erreichen der Checkout-Seite aussteigen. Das Geld könnte möglicherweise besser in eine Usability-Studie investiert werden, um zu verstehen, warum Nutzer den Verkaufsprozess nicht abschließen.\n\n\n\nDatengestützte Entscheidungen\nSo spannend es auch sein kann, etwas über die Daten und ihren Kontext zu lernen – wir generieren damit noch keinen Wert. Erkenntnisse werden erst wertvoll, wenn sie eine Entscheidung beeinflussen und dazu führen, dass jemand anders handelt, als er oder sie es ohne die Analyse getan hätte.\n\n\n\n\n\nDazu müssen wir zunächst klarstellen, welche Entscheidung unsere Erkenntnisse beeinflussen sollen.\n\n\n\n\n\n\nHinweis\n\n\n\nNicht alle Entscheidungen müssen datengestützt getroffen werden. Entscheidungsträger sollten aber ehrlich sein, ob eine Entscheidung von den Ergebnissen der Analyse beeinflusst werden kann und welche Daten sie dazu bringen würden, ihre Meinung zu ändern und einen anderen Handlungsweg zu wählen. Wenn Daten nur angefordert werden, um eine Entscheidung zu untermauern, die in Wirklichkeit bereits getroffen wurde, erspare den Analysten den Aufwand!\n\n\nBevor wir mit einer Datenanalyse beginnen müssen wir uns im Klaren darüber sein:\n\nWer sind die relevanten Stakeholder, also wer wird die Ergebnisse unserer Analyse sehen (= die Zielgruppe / Dashboardnutzer)?\nWas ist ihr Ziel? \n\nIm geschäftlichen Kontext hängen die Ziele der Nutzer in der Regel irgendwie mit der Gewinnerzielung für das Unternehmen zusammen, zum Beispiel durch Umsatzsteigerung (z.B. durch effektivere Lösung von Kundenproblemen im Vergleich zur Konkurrenz) oder Kostenreduzierung.\nUm das Erreichen dieser Ziele zu tracken verwenden wir sogenannte Key Performance Indicators (KPIs), d.h. benutzerdefinierte Metriken, die uns anzeigen, wie gut die Dinge laufen. Wenn wir beispielsweise an einer Webanwendung arbeiten, könnte ein interessanter KPI die “Nutzerzufriedenheit” sein. Leider kann man die tatsächliche Nutzerzufriedenheit nur schwer messen, aber wir können stattdessen die Anzahl der wiederkehrenden Nutzer und wie lange sie auf unserer Seite bleiben tracken, und diese und andere Messungen dann geschickt zu einer Proxy-Variablen kombinieren, die wir dann “Nutzerzufriedenheit” nennen.\n\n\n\n\n\n\nVorsicht\n\n\n\nEin KPI ist nur dann eine zuverlässige Metrik, wenn er nicht gleichzeitig dazu verwendet wird, das Verhalten von Personen zu steuern, da diese sonst versuchen, das System auszutricksen (Goodharts Gesetz). Wenn unser Ziel beispielsweise eine qualitativ hochwertige Software ist, ist die Anzahl von Bugs in unserer Software kein zuverlässiges Qualitätsmaß, wenn wir gleichzeitig Programmierer für jeden gefundenen und behobenen Bug belohnen.\n\n\n\nLagging vs. Leading KPIs\nLeider können die Dinge, die uns wirklich interessieren, oft nur nachträglich gemessen werden, also wenn es schon zu spät ist, um korrigierend einzugreifen. Zum Beispiel interessieren uns im Vertrieb die “realisierten Umsätze”, also das auf der Bank eingegangene Geld. Aber wenn die Einnahmen am Ende des Quartals geringer sind als erhofft, können wir nur versuchen, im nächsten Quartal besser zu sein. Diese Kennzahlen nennt man lagging (nachlaufende) KPIs.\nLeading (führende) KPIs hingegen sagen uns, wann wir handeln sollten, bevor es zu spät ist. Im Vertrieb könnte das zum Beispiel das Volumen der Deals in der Vertriebspipeline sein. Auch wenn nicht alle diese Deals letztendlich zu realisierten Umsätzen führen, wissen wir sicher, dass wir unsere Umsatzziele nicht erreichen werden, wenn die Pipeline leer ist.\nWenn die Ursache-Wirkung-Beziehungen zwischen Variablen zu komplex sind, um direkt leading KPIs zu identifizieren, können wir stattdessen versuchen, lagging KPIs mit einem Machine Learning Modell vorherzusagen. Zum Beispiel könnte in einem Produktionsprozess für Kunststoff ein wichtiges Qualitätsmaß die Zugfestigkeit sein, die 24 Stunden nach dem Aushärten gemessen wird. Falls wir nach 24 Stunden feststellen, dass die Qualität nicht ausreichend ist, kann es sein, dass wir die komplette Produktion verwerfen müssen. Wenn die Beziehungen zwischen den Prozessparametern (wie Produktionstemperatur) und der resultierenden Qualität zu komplex sind, um ein leading KPI zu identifizieren, könnten wir stattdessen ein ML-Modell mit vergangenen Prozessparametern (Inputs) und dazugehörigen Qualitätsmessungen (Outputs) trainieren, um dann kontinuierlich die Qualität während der Produktion vorherzusagen. Wenn das Modell vorhersagt, dass die Qualität nicht im Zielbereich liegt, können die Bediener sofort eingreifen und das Problem beheben, bevor weitere Produktionszeit verschwendet wird.\nDamit wir uns auf die Vorhersagen verlassen können, müssen wir nun allerdings zusätzlich die Performance des Modells überwachen. Die Vorhersagegenauigkeit auf neuen Daten dient hier als lagging KPI, da wir erst 24h später, also nachdem die echten Messungen eingetroffen sind, wissen, ob die Vorhersagen korrekt waren. Als leading KPI können wir die Diskrepanz zwischen den für das Modelltraining verwendeten Prozessparameterwerten und den in der Produktion gemessenen Werten berechnen, um Datendrifts zu identifizieren. Solche Drifts, wie z.B. Änderungen in den Werten aufgrund von Sensorstörungen, können zu falschen Vorhersagen für betroffenen Proben führen.\n\nDer erste Schritt bei einer datengestützen Entscheidung ist zu erkennen, dass wir handeln sollten, indem wir unsere KPIs überwachen, um festzustellen, ob wir dabei sind, unsere Ziele zu verfehlen.\nIdealerweise werden diese Metriken mit Schwellwerten für Warnungen kombiniert, um uns automatisch zu benachrichtigen, wenn etwas schief geht und eine Korrekturmaßnahme erforderlich ist. Beispielsweise könnten wir eine Warnung zum Zustand eines Systems oder einer Maschine einrichten, um einen Techniker zu benachrichtigen, wenn eine Wartung erforderlich ist. Um Alarmmüdigkeit zu vermeiden, ist es wichtig, falsche Alarme zu reduzieren, also die Warnung so zu konfigurieren, dass die verantwortliche Person sagt: “Wenn dieser Schwellwert erreicht ist, lasse ich alles stehen und liegen und behebe das Problem” (nicht “an diesem Punkt sollten wir es wahrscheinlich im Auge behalten”).\nJe nachdem, wie häufig sich der Wert des KPI ändert und wie schnell Korrekturmaßnahmen Wirkung zeigen, möchten wir die Alarmbedingung entweder alle paar Minuten überprüfen, um jemanden in Echtzeit zu benachrichtigen, oder zum Beispiel jeden Morgen, jeden Montag oder einmal im Monat, wenn sich die Werte langsamer ändern.\n\nIst das signifikant?\nKleine Schwankungen in den KPIs sind normal, und wir sollten nicht überreagieren, wenn es sich um zufälliges Rauschen handelt. Statistik kann uns sagen, ob die beobachtete Abweichung von dem, was wir erwartet haben, signifikant ist.\nStatistische Inferenz ermöglicht es uns Schlussfolgerungen zu ziehen, die über die vorliegenden Daten hinausgehen. Oft möchten wir eine Aussage über eine ganze Population treffen (z.B. alle Menschen, die derzeit auf dieser Erde leben), aber wir haben nur Zugriff auf einige (hoffentlich repräsentative) Beobachtungen, aus denen wir unsere Schlussfolgerung ziehen können. Bei der statistischen Inferenz geht es darum, unsere Meinung trotz dieser Unsicherheit zu ändern: Wir nehmen eine Nullhypothese (= unsere Erwartung bevor wir in die Daten geschaut haben) an und prüfen dann, ob das, was wir in unserer Stichprobe beobachten, diese Nullhypothese lächerlich erscheinen lässt. Ist dies der Fall, verwerfen wir sie und nehmen stattdessen die alternative Hypothese an.\nBeispiel: Euer Unternehmen hat einen Online-Shop und möchte ein neues Empfehlungssystem einführen, aber ihr seid euch nicht sicher, ob Kunden diese Empfehlungen hilfreich finden und mehr kaufen werden. Bevor ihr mit dem neuen System live geht, führt ihr daher einen A/B-Test durch, bei dem ein Prozentsatz zufällig ausgewählter Nutzer die neuen Empfehlungen sieht, während die anderen zur ursprünglichen Version des Online-Shops weitergeleitet werden. Die Nullhypothese lautet, dass die neue Version nicht besser ist als das Original. Aber es stellt sich heraus, dass das durchschnittliche Verkaufsvolumen der Kunden, die die neuen Empfehlungen sehen, viel höher ist als das der Kunden, die die ursprüngliche Website besuchen. Dieser Unterschied ist so groß, dass es in einer Welt, in der die Nullhypothese wahr wäre, äußerst unwahrscheinlich wäre, dass eine zufällige Stichprobe uns diese Ergebnisse liefern würde. Wir verwerfen daher die Nullhypothese und gehen von der Alternativhypothese aus, dass die neuen Empfehlungen höhere Umsätze generieren.\nNeben strengen statistischen Tests gibt es auch einige Faustregeln, um zu bestimmen, ob Veränderungen in den Daten unsere Aufmerksamkeit erfordern: Wenn eine einzelne Stichprobe drei Standardabweichungen (\\(\\sigma\\)) über oder unter dem Mittelwert liegt oder sieben aufeinanderfolgende Punkte über oder unter dem Durchschnittswert liegen, ist dies ein Grund für weitere Untersuchungen.\n\n\n\nEin Kontrollchart zeigt Messungen über die Zeit, die um ihren Mittelwert schwanken, wobei interessante Punkte in Rot markiert sind.\n\n\n\nLies diesen Artikel wenn du mehr über den Unterschied zwischen Analysten und Statistikern erfahren möchtest und warum diese immer mit unterschiedlichen Teilen eines Datensatzes arbeiten sollten.\n\n\n\nFür jede eingerichtete Alarmbedingung, also immer wenn klar ist, dass eine Korrekturmaßnahme erforderlich ist, sollten wir uns überlegen, ob diese Maßnahme automatisiert werden kann und die automatisierte Aktion direkt zusammen mit dem Alarm auslösen (zum Beispiel wenn die Genauigkeit eines ML-Modells unter einen bestimmten Schwellwert fällt, könnten wir das Modell automatisch mit den neusten Daten nachtrainieren anstatt nur den Data Scientist zu benachrichtigen). Wenn dies nicht möglich ist, zum Beispiel, weil nicht klar ist, was genau passiert ist und welche Maßnahme ergriffen werden sollte, benötigen wir eine tiefere Analyse.\nEin tieferes Eintauchen in die Daten kann uns helfen, Fragen wie “Warum haben wir dieses Ziel nicht erreicht und was können wir besser machen?” (oder, in selteneren Fällen, “Warum haben wir dieses Ziel übertroffen und wie können wir das wiederholen?”) zu beantworten, um zu entscheiden welche Korrekturmaßnahme ergriffen werden soll.\n\n\n\n\n\n\nVorsicht\n\n\n\nDurchsuche die Daten nicht nur nach Erkenntnissen, die das bestätigen, was du dir schon vorher gedacht hast (Bestätigungsfehler / Confirmation Bias)! Sei stattdessen offen und versuche aktiv, deine Hypothese zu widerlegen.\n\n\nSolch eine explorative Analyse ist oft ein ‘quick and dirty’ Prozess, bei dem wir viele Diagramme erstellen, um die Daten besser zu verstehen und um zu erkennen, woher der Unterschied zwischen dem, was wir erwartet haben, und dem, was wir in den Daten sehen, kommt, z.B. indem wir andere korrelierte Variablen untersuchen. Zufriedenstellende Antworten zu finden ist allerdings oft mehr Kunst als Wissenschaft.\n\n\n\n\n\n\nTipp\n\n\n\nWenn wir ein ML-Modell verwenden, um KPIs vorherzusagen, können wir dieses Modell und seine Vorhersagen interpretieren, um besser zu verstehen, welche Variablen die KPIs beeinflussen könnten. In dem wir zuerst die vom ML-Modell als wichtig erachteten Features untersuchen, können wir Zeit sparen, wenn unser Datensatz Hunderte von Variablen enthält. Aber Achtung – das Modell hat nur aus Korrelationen in den Daten gelernt; diese repräsentieren nicht unbedingt wahre kausale Zusammenhänge zwischen den Variablen.\n\n\n\n\nErkenntnisse kommunizieren\nDie Diagramme, die wir während der explorativen Analyse erstellt haben, sollten nicht die Diagramme sein, die wir unserem Publikum zeigen, um unsere Erkenntnisse zu kommunizieren. Da unsere Zielgruppe mit den Daten viel weniger vertraut ist als wir und wahrscheinlich auch kein Interesse / keine Zeit hat, tiefer in die Daten einzutauchen, müssen wir ihnen die Ergebnisse leichter zugänglich machen – ein Prozess, der oft als erklärende Analyse bezeichnet wird.\n\n\n\n\n\n\nWarnung\n\n\n\n“Einfach alle Daten zeigen” und hoffen, dass das Publikum schon irgendwas daraus machen wird, ist oft der Anfang vom Ende vieler Dashboards. Es ist wichtig, dass du verstehst, welches Ziel dein Publikum erreichen möchte und welche Fragen dafür beantwortet werden müssen.\n\n\n\nSchritt 1: Wähle den richtigen Diagrammtyp\n\nLass dich von Visualisierungsbibliotheken inspirieren (z.B. hier oder hier), aber vermeide den Drang, ausgefallene Grafiken zu erstellen; gängigen Visualisierungen machen es dem Publikum einfacher, die Informationen korrekt zu entschlüsseln\nVerwende keine 3D-Effekte!\nVermeide Torten- oder Donutdiagramme (Winkel sind schwer zu interpretieren)\nVerwende Liniendiagramme für Zeitreihendaten\nVerwende horizontale statt vertikaler Balkendiagramme für Zielgruppen, die von links nach rechts lesen\nBei Flächen- und Balkendiagrammen sollte die y-Achse bei 0 beginnen\nBenutze evtl. ‘Small Multiples’ oder Sparklines, wenn ein einzelnes Diagramm zu vollgestopft wirkt\n\n\n\n\nLinks: Balkendiagramme (insbesondere in 3D) erschweren den Vergleich von Zahlen über einen längeren Zeitraum. Rechts: Trends über die Zeit lassen sich in Liniendiagrammen leichter erkennen. [Beispiel adaptiert von: Storytelling with Data von Cole Nussbaum Knaflic]\n\n\n\n\nSchritt 2: Unnötiges weglassen / Daten-zu-Tinte-Verhältnis maximieren\n\nRand entfernen\nGitterlinien entfernen\nDatenmarker entfernen\nAchsenbeschriftungen aufs Wesentliche reduzieren\nLinien direkt beschriften\n\n\n\n\nLass Unnötiges weg! [Beispiel adaptiert von: Storytelling with Data von Cole Nussbaum Knaflic]\n\n\n\n\nSchritt 3: Aufmerksamkeit fokussieren\n\nBeginne mit grau, also schiebe erstmal alles in den Hintergrund\nVerwende präattentive Attribute wie Farben strategisch um das wichtigste hervorzuheben\nVerwende Datenmarker und Labels sparsam\n\n\n\n\nBeginne mit grau und verwende präattentive Attribute strategisch, um die Aufmerksamkeit des Publikums zu lenken. [Beispiel adaptiert von: Storytelling with Data von Cole Nussbaum Knaflic]\n\n\n\n\nSchritt 4: Daten zugänglich machen\n\nKontext hinzufügen: Welche Werte sind gut (Zielzustand), welche schlecht (Alarmschwelle)? Sollten die Daten mit einer anderen Variable vergleichen werden (z.B. gemessene Werte und Vorhersagen)?\nVerwende konsistente Farben, wenn Informationen über mehrere Diagramme verteilt sind (z.B. Daten von einem Land immer in derselben Farbe darstellen)\nFüge erklärenden Text hinzu, um die wichtigsten Schlussfolgerungen und Handlungsempfehlungen herauszustellen (falls dies nicht möglich ist, z.B. in Dashboards, in denen sich die Daten ständig ändern, kann der Titel stattdessen die Frage enthalten, die das Diagramm beantworten soll, z.B. “Folgt unser Umsatz den Prognosen?”)\n\n\n\n\nErzähl eine Geschichte. [Beispiel adaptiert von: Storytelling with Data von Cole Nussbaum Knaflic]\n\n\n\nWeiterführende Literatur\n\nShow Me the Numbers: Designing Tables and Graphs to Enlighten von Stephen Few\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks von Jonathan Schwabish\nEffective Data Storytelling: How to drive change with data, narrative, and visuals von Brent Dykes\nStorytelling with Data: A data visualization guide for business professionals von Cole Nussbaum Knaflic\nData Visualization: A successful design process von Andy Kirk\nVerschiedene Blogartikel von Cassie Kozyrkov",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Datenanalyse & Preprocessing</span>"
    ]
  },
  {
    "objectID": "02_data.html#sec-data-garbage",
    "href": "02_data.html#sec-data-garbage",
    "title": "Datenanalyse & Preprocessing",
    "section": "Garbage in, Garbage out!",
    "text": "Garbage in, Garbage out!\nVergiss nie: Daten sind unser Rohstoff, um mit ML etwas Wertvolles zu schaffen. Ist die Qualität oder Quantität der Daten nicht ausreichend, haben wir ein “Garbage in, Garbage out”-Szenario und egal welche Art von ausgefallenem ML-Algorithmus wir verwenden, wir werden kein zufriedenstellendes Ergebnis erreichen. Im Gegenteil, je aufwändiger die Algorithmen (z.B. Deep Learning), desto mehr Daten werden benötigt.\nNachfolgend findest du eine Zusammenfassung einiger allgemeiner Risiken im Zusammenhang mit Daten, die die Anwendung von ML erschweren oder sogar unmöglich machen:\n\nRohdaten können sehr chaotisch sein:\n\nRelevante Daten sind auf mehrere Datenbanken/Excel-Tabellen verteilt, die zusammengeführt werden müssen. Worst Case: Datenpunkte haben keine eindeutige ID, anhand derer sich die verschiedenen Einträge verknüpfen lassen. Stattdessen muss man auf eine fehleranfällige Strategie zurückgreifen, wie den Datenabgleich anhand von Zeitstempeln.\nManuell eingegebene Werte enthalten Fehler, z.B. falsch platzierte Dezimalkommas.\nFehlende Werte können nicht korrigiert werden. Worst Case: Fehlende Werte sind nicht zufällig. Beispielsweise versagen Sensoren genau dann, wenn während des Produktionsprozesses etwas schief geht. Oder im Rahmen einer Umfrage zum Einkommen verweigern reiche Personen im Vergleich zu armen oder mittelständischen Personen häufiger die Antwort. Dies kann zu systematischen Verzerrungen im Datensatz führen.\nDer Datensatz besteht aus verschiedenen Datentypen (strukturiert und/oder unstrukturiert) mit unterschiedlichen Skalierungen.\nDer Prozess ändert sich im Laufe der Zeit, z.B. aufgrund externer Bedingungen wie dem Austausch eines Sensors oder einem anderen Wartungsereignis, was bedeutet, dass die über verschiedene Zeiträume gesammelten Daten nicht kompatibel sind. Worst Case: Diese externen Veränderungen wurden nirgends dokumentiert und wir bemerken erst am Ende der Analyse, dass die verwendeten Daten unsere Annahmen verletzten.\n\n→ Preprocessing von Daten dauert sehr lange.\n→ Der resultierende Datensatz (nach der Bereinigung) ist viel kleiner als erwartet – möglicherweise zu klein, um eine sinnvolle Analyse durchzuführen.\n⇒ Bevor du mit ML anfängst, überlege erst, ob du die Datenerfassungspipeline und Infrastruktur verbessern kannst!\n\n\nNicht genug / nicht die richtigen Daten für ML:\n\nEin kleiner Datensatz und/oder zu wenig Variation, z.B. werden nur sehr wenige defekte Produkte produziert oder ein Prozess läuft die meiste Zeit im stationären Zustand, d.h. es gibt nur wenig oder keine Variation bei den Inputs.\n→ Auswirkung verschiedener Eingangsgrößen auf die Zielvariable kann aus wenigen Beobachtungen nicht zuverlässig abgeschätzt werden.\n⇒ Führe gezielt Experimente durch, bei denen verschiedene Inputs systematisch variiert werden, um einen aussagekräftigeren Datensatz zu generieren.\nDaten sind inkonsistent / unvollständig, also es gibt Datenpunkte mit gleichen Inputs aber unterschiedlichen Labels, z.B. zwei Produkte wurden unter den gleichen (gemessenen) Bedingungen produziert, eins ist in Ordnung, eins fehlerhaft.\nDies kann zwei Gründe haben:\n\nDie Labels sind sehr verrauscht, z.B. weil die menschlichen Annotatoren nicht dieselben klaren Regeln befolgten oder einige Beispiele mehrdeutig waren (z.B. ein QA-Experte sagt ein kleiner Kratzer ist noch in Ordnung, der andere sortiert das Produkt aus).\n⇒ Etablierung klarer Regeln nach welchen Kriterien Daten gelabelt werden sollen und Daten durch Neuannotation bereinigen. Dies kann einige Zeit dauern, lohnt sich aber! Siehe auch dieser tolle MLOps / data-centric AI Vortrag von Andrew Ng darüber, wie ein kleiner sauberer Datensatz wertvoller sein kann als ein großer verrauschter Datensatz.\nRelevante Input Features fehlen: Menschen können zwar relativ einfach beurteilen, ob alle relevanten Informationen in unstrukturierten Daten enthalten sind (z.B. Bilder: entweder sehen wir eine Katze oder nicht), strukturierte Daten haben dagegen oft zu viele verschiedene Variablen und komplexe Interaktionen, um sofort zu erkennen, ob alle relevanten Eingangsgrößen gemessen wurden.\n⇒ Sprich mit einem Fachexperten darüber, welche zusätzlichen Features hilfreich sein könnten und nimm diese in den Datensatz mit auf. Worst Case: Es muss ein neuer Sensor in die Maschine eingebaut werden um diese Daten zu sammeln, d.h. alle in der Vergangenheit gesammelten Daten sind im Grunde nutzlos. ABER: Der Einsatz des richtigen Sensors kann das Problem immens vereinfachen und der Einbau lohnt sich!\n\n\n\nBei vielen Anwendungen ist unser erster Gedanke oft, die Eingabedaten mithilfe einer Kamera zu generieren, da wir Menschen hauptsächlich auf unser visuelles System angewiesen sind, um viele Aufgaben zu lösen. Auch wenn Bilderkennungsalgorithmen mittlerweile sehr ausgereift sind, kann die Verwendung eines solchen Setups anstelle eines spezialisierteren Sensors die Lösung komplizierter machen.\nWillst du überreife Erdbeeren erkennen? Machs wie Amazon Fresh und verwende einen Nahinfrarotsensor (NIR) anstelle einer normalen Kamera. Dabei wird immer noch maschinelles Lernen verwendet um die resultierenden Daten zu analysieren, aber in den NIR-Bildern sind die vergammelten Teile der Früchte viel besser sichtbar als auf normalen Fotos.\nVersuchst du zu erkennen, ob eine Tür geschlossen ist? Mit einem einfachen Magneten und Detektor lässt sich diese Aufgabe ohne aufwändige Analyse oder Trainingsdaten lösen! (Du kennst bestimmt das Sprichwort: “Wenn du einen Hammer hast, sieht alles aus wie ein Nagel”. → Vergiss nicht, auch Lösungen außerhalb deiner ML-Toolbox in Betracht zu ziehen! ;-))\n\n→ Wenn der Datensatz nicht entsprechend aufgewertet wird, wird kein ML-Modell eine gute Performance liefern!\n\n\n\n\n\n\n\nTipp\n\n\n\nBeobachte wenn möglich wie die Daten erfasst werden, im Sinne von: Stehe tatsächlich physisch da und beobachte, wie jemand die Werte in ein Programm eingibt oder wie die Maschine arbeitet, wenn die Sensoren etwas messen. Sicherlich werden dir einige Dinge auffallen, die man direkt bei der Datenerhebung optimieren könnte. Dies erspart in Zukunft viel Preprocessing Arbeit.\n\n\n\nBest Practice: Datenkatalog\nUm Datensätze leichter zugänglich zu machen, sollten diese dokumentiert werden. Für strukturierte Datensätze sollte es für jede Variable Zusatzinformationen geben wie:\n\nName der Variable\nBeschreibung\nEinheit\nDatentyp (z.B. numerische oder kategorische Werte)\nDatum der ersten Messung (z.B. falls ein Sensor erst später eingebaut wurde)\nNormaler/erwarteter Wertebereich (→ “Wenn die Variable unter diesem Schwellwert liegt, dann ist die Maschine ausgeschaltet und die Datenpunkte können ignoriert werden”)\nWie werden fehlende Werte aufgezeichnet, d.h. werden sie tatsächlich als fehlende Werte aufgezeichnet oder stattdessen durch einen unrealistischen Wert ersetzt, was passieren kann, da einige Sensoren kein Signal für “Not a Number” (NaN) senden können oder die Datenbank es nicht zulässt, dass das Feld leer bleibt.\nAnmerkungen zu sonstigen Vorfällen oder Ereignissen, z.B. eine Fehlfunktion des Sensors während eines bestimmten Zeitraums oder ein anderer Fehler, der zu falschen Daten geführt hat. Diese sind sonst oft schwer zu erkennen, z.B. wenn jemand stattdessen manuell Werte eingibt oder kopiert hat, die auf den ersten Blick normal aussehen.\n\nWeitere Empfehlungen, was bei der Dokumentation von Datensätzen speziell für Machine Learning Anwendungen wichtig ist, findest du im Data Cards Playbook.\n\n\n\n\n\n\nHinweis\n\n\n\nNeben der Dokumentation von Datensätzen als Ganzes ist es auch sehr hilfreich, Metadaten für einzelne Proben zu speichern. Bei Bilddaten können dies beispielsweise der Zeitstempel der Bildaufnahme, die Geolokalisierung (oder wenn die Kamera in einer Fertigungsmaschine verbaut ist, dann die ID dieser Maschine), Informationen zu den Kameraeinstellungen etc. sein. Dies kann bei der Analyse von Modellvorhersagefehlern sehr hilfreich sein, da sich beispielsweise herausstellen kann, dass Bilder, die mit einer bestimmten Kameraeinstellung aufgenommen wurden, besonders schwer zu klassifizieren sind, was uns wiederum Hinweise liefert, wie wir den Datenerfassungsprozesses verbessern könnten.\n\n\n\n\nDaten als Asset\nMit den richtigen Prozessen (z.B. Etablierung von Rollen wie “Data Owner” und “Data Controller”, die für die Datenqualität verantwortlich sind, und Aufbau einer konsistenten Dateninfrastruktur, inkl. eines Monitoring-Prozesses zur Validierung neuer Daten) ist es für eine Organisation möglich, von “Garbage in, Garbage out” zu “Daten sind das neue Öl” zu kommen:\n\n\n\n\n\n\n\nMit (Big) Data kommt große Verantwortung!\nEinige Daten erscheinen auf den ersten Blick manchmal nicht sehr wertvoll, können aber für andere (d.h. mit einem anderen Anwendungsfall) von großem Nutzen sein!\n\n\n\nEin Fitness-Tracker-Startup hielt es für eine coole Idee, beliebte Joggingrouten basierend auf den von ihren Nutzern generierten Daten zu veröffentlichen. Da aber auch viele US-Soldaten den Tracker benutzten und häufig um ihre Militärstützpunkte joggten, hat dieses Startup dadurch versehentlich einen geheimen US-Armeestützpunkt in Afghanistan geoutet, der auf ihrer interaktiven Karte als heller Punkt in einem Gebiet auftauchte, wo sonst nur wenige andere Nutzer joggten. Auch wenn deine Daten erstmal harmlos erscheinen, denk bitte darüber nach, was bei einer Veröffentlichung (auch in aggregierter, anonymisierter Form) schief gehen könnte!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Datenanalyse & Preprocessing</span>"
    ]
  },
  {
    "objectID": "02_data.html#sec-preprocessing",
    "href": "02_data.html#sec-preprocessing",
    "title": "Datenanalyse & Preprocessing",
    "section": "Preprocessing",
    "text": "Preprocessing\nJetzt, da wir unsere Daten besser verstehen und sicher gestellt haben, dass sie (hoffentlich) von guter Qualität sind, können wir sie für unsere Machine Learning Algorithmen aufbereiten.\n\nRohdaten können in vielen verschiedenen Formaten vorliegen, z.B. Sensormessungen, Pixelwerte, Text (z.B. HTML-Seite), SAP-Datenbank, …\n→ n Datenpunkte, gespeichert als Zeilen in einem Excel-Spreadsheet, als einzelne Dateien, usw.\n\n\n\n\n\n\nWichtig\n\n\n\nWas ist ein Datenpunkt? Es ist äußerst wichtig sich darüber im Klaren zu sein, was eigentlich ein Datenpunkt ist, d.h. wie die Inputs aussehen und welches Ergebnis das Modell für jede Probe/Beobachtung zurückgegeben soll. Um das zu bestimmen, hilft es sich zu überlegen, wie das ML-Modell später in den Rest des Workflows integriert werden soll: Welche Daten werden im vorherigen Schritt erzeugt und können als Input für den ML-Teil verwendet werden, und was für einen Output braucht man für den darauffolgenden Schritt?\n\n\n\nPreprocessing\nTransformation und Anreicherung der Rohdaten vor der Anwendung von ML, zum Beispiel:\n\nFehlende oder falsch eingetragene Werte entfernen / korrigieren (z.B. falsch gesetztes Dezimalkomma)\nNull-Varianz-Features (d.h. Variablen mit immer gleichem Wert) und unsinnige Variablen (z.B. IDs) ausschließen\nFeature Extraktion: Rohdaten in numerische Werte umwandeln (z.B. Text)\nFeature Engineering: Berechnen zusätzlicher/besserer Features aus den ursprünglichen Variablen\n\n⇒ Feature Matrix \\(\\,X \\in \\mathbb{R}^{n\\times d}\\): n Datenpunkte; jeder repräsentiert als ein d-dimensionaler Vektor (d.h. mit d Features)\nVorhersage-Targets?\n→ Label Vektor \\(\\,\\mathbf{y}\\): n-dimensionaler Vektor mit einem Target Wert (Label) pro Datenpunkt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Datenanalyse & Preprocessing</span>"
    ]
  },
  {
    "objectID": "05_supervised_models.html",
    "href": "05_supervised_models.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Neuronale Netze\n“Deep Learning” beschreibt das Teilgebiet des maschinellen Lernens, das sich mit neuronalen Netzen beschäftigt.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "05_supervised_models.html#neuronale-netze",
    "href": "05_supervised_models.html#neuronale-netze",
    "title": "Deep Learning",
    "section": "",
    "text": "Intuitive Erklärung Neuronaler Netze\n\n[Adaptiert von: “AI for everyone” von Andrew Ng (coursera.org)]\n\nAngenommen, wir haben einen Online-Shop und versuchen vorherzusagen, wie viel wir von einem Produkt im nächsten Monat verkaufen werden. Der Preis, zu dem wir bereit sind, das Produkt anzubieten, beeinflusst offensichtlich die Nachfrage, da die Leute versuchen, ein gutes Geschäft zu machen, d.h. je niedriger der Preis, desto höher die Nachfrage. Es handelt sich um eine negative Korrelation, die durch ein lineares Modell beschrieben werden kann. Die Nachfrage ist jedoch nie kleiner als Null (d.h. wenn der Preis sehr hoch ist, werden die Kunden das Produkt nicht plötzlich zurückgeben), also müssen wir das Modell so anpassen, dass der vorhergesagte Output nie negativ ist. Dies erreichen wir durch eine Max-Funktion (in diesem Zusammenhang auch nichtlineare Aktivierungsfunktion genannt), die auf das Ergebnis des linearen Modells angewendet wird, sodass wenn das lineare Modell einen negativen Wert berechnet stattdessen 0 vorhergesagt wird.\n\n\n\nEin sehr einfaches lineares Modell mit einer Input und einer Output Variablen und einer nichtlinearen Aktivierungsfunktion (der Max-Funktion).\n\n\nDiese funktionale Beziehung kann auch als Kreis mit einem Input (Preis) und einem Output (Nachfrage) visualisiert werden, wobei die S-Kurve im Kreis anzeigt, dass auf das Ergebnis eine nichtlineare Aktivierungsfunktion angewendet wird. Wir werden dieses Symbol später als einzelne Einheit oder “Neuron” eines neuronalen Netzes (NN) sehen.\nUm die Vorhersage zu verbessern, können wir das Modell erweitern und mehrere Input Features für die Vorhersage verwenden:\n\n\n\nEin einfaches lineares Modell mit mehreren Inputs, bei dem die Vorhersage als gewichtete Summe der Inputs berechnet wird, zusammen mit der Max-Funktion um negative Werte zu vermeiden.\n\n\nUm die Performance des Modells noch weiter zu verbessern, können wir aus den ursprünglichen Inputs manuell informativere Features generieren, indem wir sie sinnvoll kombinieren (→ Feature Engineering), bevor wir den Output berechnen:\n\n\n\nIn diesem Beispiel geht es um einen Online-Shop und die Kunden müssen Versandkosten bezahlen, d.h. die tatsächliche Erschwinglichkeit des Produkts ergibt sich aus der Summe des Produktpreises und den Versandkosten. Außerdem interessieren sich die Kunden für qualitativ hochwertige Produkte, doch die Produktwahrnehmung ergibt sich nicht nur aus der tatsächlichen Rohstoffqualität. Dass unser Produkt hochwertiger ist als andere wird auch durch eine entsprechende Marketingkampagne und einen hohen Preis vermittelt. Durch die Berechnung dieser zusätzlichen Zwischenfeatures kann der Preis somit in zweierlei Hinsicht zur endgültigen Vorhersage beitragen: Während einerseits ein niedrigerer Preis für die Erschwinglichkeit des Produkts von Vorteil ist, führt andererseits ein höherer Preis zu der Wahrnehmung einer höheren Qualität.\n\n\nWährend es in diesem Anschauungsbeispiel möglich war, solche Features manuell zu konstruieren, ist das Vorteilhafte an neuronalen Netzen, dass sie genau das automatisch tun: Indem wir mehrere Zwischenschichten (Layers) verwenden, d.h. mehrere lineare Modelle (mit nichtlinearen Aktivierungsfunktionen) verbinden, werden immer komplexere Kombinationen der ursprünglichen Input Features generiert, die die Performance des Modells verbessern können. Je mehr Layers das Netzwerk verwendet, d.h. je “tiefer” es ist, desto komplexer sind die resultierenden Feature Repräsentationen.\nDa verschiedene Problemstellungen und insbesondere verschiedene Arten von Input Daten von unterschiedlichen Feature Repräsentationen profitieren, gibt es verschiedene Arten neuronaler Netzarchitekturen, um diese aussagekräftigeren Zwischenfeatures zu berechnen, z.B.\n\nFeed Forward Neural Networks (FFNNs) für ‘normale’ (z.B. strukturierte) Daten\nConvolutional Neural Networks (CNNs) für Bilder\nRecurrent Neural Networks (RNNs) für sequenzielle Daten wie Text oder Zeitreihen\n\n\n\nNN Architekturen\nÄhnlich wie domänenspezifisches Feature Engineering zu erheblich verbesserten Modellvorhersagen beitragen kann, lohnt es sich gleichermaßen, eine auf die jeweilige Aufgabe zugeschnittene neuronale Netzwerkarchitektur zu konstruieren.\n\nFeed Forward Neural Network (FFNN)\nDas FFNN ist die ursprüngliche und einfachste neuronale Netzwerkarchitektur, die auch im ersten Beispiel verwendet wurde. Allerdings bestehen diese Modelle in der Praxis normalerweise aus mehr Layers und Neuronen pro Layer:\n\n\n\nFeed Forward Neural Network (FFNN) Architektur: Der Input-Feature-Vektor \\(\\mathbf{x}\\), der einen Datenpunkt darstellt, wird mit der ersten Gewichtungsmatrix \\(W_1\\) multipliziert, um einen neuen Vektor zu erzeugen, der nach Anwendung der nichtlinearen Aktivierungsfunktion (z.B. der Max-Funktion wie im ersten Beispiel) zur ersten Hidden-Layer-Repräsentation \\(\\mathbf{x}'\\) wird. Dieser neue Vektor wird dann mit der zweiten Gewichtungsmatrix \\(W_2\\) multipliziert und wieder wird eine nichtlineare Aktivierungsfunktion angewandt, um die zweite Hidden-Layer-Repräsentation des Datenpunkts, \\(\\mathbf{x}''\\), zu erzeugen. Je nachdem, wie viele Schichten das Netzwerk hat (d.h. wie tief es ist), wiederholt sich dies nun mehrmals, bis schließlich die letzte Schicht den vorhergesagten Output \\(\\mathbf{\\hat{y}}\\) berechnet. Während das Netzwerk trainiert wird, nähern sich diese vorhergesagten Outputs immer mehr den wahren Labels der Trainingsdaten an in dem die Gewichtsmatrizen entsprechend angepasst werden.\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDu kannst hier auch selbst mit einem kleinen neuronalen Netz herumspielen um z.B. zu schauen wie es sich verhält wenn du mehr Neuronen oder Layers verwendest.\n\n\n\n\nConvolutional Neural Network (CNN)\nManuelles Feature Engineering für Computer Vision Aufgaben ist sehr schwierig. Während der Mensch mühelos eine Vielzahl von Objekten in Bildern erkennt, ist es schwer zu beschreiben, warum wir erkennen was wir sehen, z.B. anhand welcher Merkmale wir eine Katze von einem kleinen Hund unterscheiden. Das Deep Learning hatte seinen ersten bahnbrechenden Erfolg auf diesem Gebiet, da neuronale Netze, insbesondere CNNs, es durch eine Hierarchie von Layern schaffen, sinnvolle Feature Repräsentationen aus visuellen Informationen zu lernen.\nConvolutional Neural Networks eignen sich sehr gut für die Verarbeitung visueller Informationen, da sie direkt mit 2D-Bildern arbeiten können und die Tatsache nutzen, dass Bilder viele lokale Informationen beinhalten (z.B. sind Augen, Nase und Mund lokalisierte Komponenten eines Gesichts).\n\n\n\nEine convolutional neuronale Netzarchitektur zur Gesichtserkennung: Die gelernten Gewichte des Netzes sind die unterhalb des Netzes angezeigten kleinen Filterpatches, die beispielsweise im ersten Schritt Kanten im Bild erkennen. Gegen Ende des Netzwerks werden die Feature Repräsentation zu einem Vektor zusammengefasst und als Input an ein FFNN (hier “Fully Connected Layer”) gegeben, um die endgültige Klassifizierung durchzuführen.\n\n\n\n\nAllgemeine Prinzipien & fortgeschrittene Architekturen\nBei der Lösung eines Problems mit einem NN muss man immer berücksichtigen, dass das Netzwerk sowohl die Input Daten verstehen als auch die gewünschten Ausgaben generieren muss:\n\n\n\nWie wir oben beim CNN für Gesichtserkennung (Bildklassifizierung) gesehen haben, wird die vom CNN generierte Repräsentation irgendwann zusammengeführt und ein FFNN berechnet daraus die endgültige Vorhersage für die Klassifikation. Genauso kann auch der letzte Hidden State eines RNN, der die in einem Satz enthaltenen Informationen repräsentiert, an ein FFNN übergeben werden, um die finale Klassifikation zu generieren (z.B. für eine Sentimentanalyse). Einige Probleme fallen jedoch nicht in die Kategorie einfacher Supervised Learning Aufgaben (also Regression oder Klassifikation) und erfordern eine andere Art von Output. Bei der maschinellen Übersetzung ist der Output beispielsweise ein in eine andere Sprache übersetzter Satz. Dies kann durch die Kopplung zweier RNNs erreicht werden: Das erste ‘versteht’ den Satz in der Originalsprache und diese Repräsentation der Bedeutung des Satzes wird an ein zweites RNN übergeben, das daraus Wort für Wort den übersetzten Satz generiert. Ein weiteres Beispiel ist Image Captioning (d.h. das Generieren einer Bildbeschreibung, z.B. um das Online-Erlebnis für Menschen mit Sehbehinderung zu verbessern), wobei das Bild zuerst von einem CNN ‘verstanden’ wird und dann diese Repräsentation des Eingabebildes an ein RNN übergeben wird, um den passenden Text zu erzeugen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html",
    "href": "06_pitfalls.html",
    "title": "Häufige Fehler vermeiden",
    "section": "",
    "text": "[Fehler #1] Irreführende Modellevaluierung\nDas obige Zitat wird auch in diesem xkcd Comic schön veranschaulicht:\nEin Supervised Learning Modell versucht, den Zusammenhang zwischen Inputs und Outputs aus den gegebenen Datenpunkten abzuleiten. Was für ein Zusammenhang gelernt wird, wird vor allem durch den gewählten Modelltyp und seinen internen Optimierungsalgorithmus bestimmt. Man kann (und sollte) jedoch einiges tun, um sicherzustellen, dass das Ergebnis nicht offensichtlich falsch ist.\nWas wollen wir?\nEin Modell, das …\nIm Folgenden besprechen wir einige häufige Fallstricke und wie wir sie vermeiden können.\nVorhersagemodelle müssen evaluiert werden, d.h. ihre Performance muss mit einer geeigneten Evaluierungsmetrik quantifiziert werden. Dies ist notwendig, um realistisch abzuschätzen, wie nützlich ein Modell in der Praxis sein wird und mit wie vielen Vorhersagefehlern wir rechnen müssen.\nDa man bei Supervised Learning Problemen die Ground Truth, also die echten Labels, kennt, kann man verschiedene Modelle objektiv bewerten und miteinander vergleichen.\nBeim Evaluieren eines Modells kann man allerdings leicht ein zu optimistisches Bild zeichnen, weshalb man die Ergebnisse immer kritisch hinterfragen und die Performance eines Modells mit der einer Baseline vergleichen sollte. Der einfachste Vergleich wäre mit einem sehr dummen Modell, das immer den Mittelwert (→ Regression) bzw. die häufigste Klasse (→ Klassifikation) vorhersagt.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html#sec-pitfall-evaluation",
    "href": "06_pitfalls.html#sec-pitfall-evaluation",
    "title": "Häufige Fehler vermeiden",
    "section": "",
    "text": "Ist das Modell für die Aufgabe geeignet?\nD.h. generiert das Modell zuverlässige Vorhersagen für neue Datenpunkte?\n\nTeile die verfügbaren Daten in einen Trainings- und einen Testteil auf, um abschätzen zu können, wie gut die Vorhersagen des Modells sein werden, wenn es später auf neue Datenpunkte angewendet wird, auf denen es nicht trainiert wurde.\nQuantifiziere die Güte der Modellvorhersagen auf dem Testset mit einer geeigneten Evaluierungsmetrik (je nach Problemtyp).\n\n⇒ Sind manche Fehler schwerwiegender als andere (z.B. bei medizinischen Tests falsch positive vs. falsch negative Ergebnisse)?\n⇒ Lege dich auf eine Metrik/KPI fest, anhand derer du unterschiedliche Modelle vergleichst und auswählst (evtl. unter Berücksichtigung zusätzlicher Einschränkungen wie Laufzeit).\n\n\n\nEvaluierungsmetriken bei unausgeglichenen Klassenverteilungen\nDie Accuracy (Genauigkeit) ist ein sehr häufig verwendetes Evaluierungsmaß für Klassifikationsprobleme:\nAccuracy: Anteil der Proben, die richtig klassifiziert wurden.\nUnten sind die Entscheidungsgrenzen von zwei Modellen in einem Beispieldatensatz eingezeichnet, wobei die Farbe des Hintergrunds angibt, ob das Modell die blaue oder rote Klasse für einen Datenpunkt in diesem Bereich vorhersagt. Welches Modell hältst du für sinnvoller?\n\n\n\n\n\nBei unausgeglichenen Klassenverteilungen, wie in diesem Fall mit viel mehr Datenpunkten aus der blauen im Vergleich zur roten Klasse, ist die Accuracy eines Modells, das einfach immer die häufigste Klasse vorhersagt, schon sehr groß. Eine Accuracy von 90% mag zwar beeindruckend klingen, wenn man den Stakeholdern des Projekts von der Performance seines Modells berichtet, bedeutet jedoch nicht automatisch, dass das Modell tatsächlich in der Praxis nützlich ist, zumal uns bei realen Problemen oft die seltenere Klasse mehr interessiert, z.B. Menschen mit einer seltenen Krankheit oder Produkte, die einen Defekt aufweisen.\nEine aussagekräftigere Evaluierungsmetrik für Klassifikationsmodelle ist die Balanced Accuracy, mit der wir zwischen einem Modell, das tatsächlich etwas gelernt hat, und der ‘dummen Baseline’ unterscheiden können:\nBalanced Accuracy: Zuerst wird für jede Klasse einzeln der Anteil der richtig klassifizierten Stichproben berechnet und dann der Durchschnitt dieser Werte gebildet.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html#sec-pitfall-generalize",
    "href": "06_pitfalls.html#sec-pitfall-generalize",
    "title": "Häufige Fehler vermeiden",
    "section": "[Fehler #2] Modell generalisiert nicht",
    "text": "[Fehler #2] Modell generalisiert nicht\nWir wollen ein Modell, das den ‘Input → Output’-Zusammenhang in den Daten erfasst und interpolieren kann, d.h. wir müssen prüfen:\nGeneriert das Modell zuverlässige Vorhersagen für neue Datenpunkte aus derselben Verteilung wie die der Trainingsdaten?\nWenn ja, garantiert dies zwar noch nicht, dass das Modell tatsächlich den echten kausalen Zusammenhang zwischen Inputs und Outputs gelernt hat und über den Trainingsbereich hinaus extrapolieren kann (dazu kommen wir im nächsten Abschnitt). Zumindest generiert das Modell aber zuverlässige Vorhersagen für neue Datenpunkte, die den Trainingsdaten ähnlich sind. Ist dies nicht gegeben, ist das Modell nicht nur falsch, sondern auch nutzlos.\nAber warum macht ein Modell überhaupt Fehler? Eine schlechte Performance auf dem Testset kann zwei Gründe haben: Overfitting oder Underfitting.\n\n\n\nWenn man bei den verschiedenen Modellen hier nur den Fehler auf den Testdaten betrachtet, könnte man schlussfolgern, dass die Modelle links (overfitting) und rechts (underfitting) gleichermaßen falsch sind. Das stimmt zwar in gewisser Weise, aber der Testfehler allein sagt uns nicht, warum die Modelle falsch liegen oder wie wir ihre Performance verbessern können. Offensichtlich machen die beiden Modelle auf dem Testset aus völlig unterschiedlichen Gründen Fehler: Das overfittete Modell hat die Trainingsdaten quasi auswendig gelernt und kann nicht auf neue Datenpunkte verallgemeinert werden, während das underfittete Modell zu einfach ist, um den Zusammenhang zwischen den Inputs und Outputs überhaupt abzubilden.\n\n\nDiese beiden Fälle erfordern sehr unterschiedliche Ansätze, um die Modellperformance zu verbessern.\nDa die meisten Datensätze sehr viele Inputvariablen haben, kann man das Modell in der Regel nicht einfach wie oben aufmalen, um zu sehen, ob es over- oder underfittet. Stattdessen muss man sich den mit einer aussagekräftigen Evaluierungsmetrik berechneten Fehler sowohl auf dem Trainings- als auch dem Testset anschauen um zu bestimmen, ob man es mit Overfitting oder Underfitting zu tun hat:\nOverfitting: super Trainingsperformance, inakzeptabel auf den Testdaten\nUnderfitting: schlechte Trainings- UND Testperformance\n\n\n\n\n\nJe nachdem, ob ein Modell over- oder underfittet, gibt es verschiedene Möglichkeiten die Performance zu verbessern. Eine perfekte Modellperformance ist jedoch unrealistisch, da manche Aufgaben einfach schwierig sind, zum Beispiel weil die Daten sehr verrauscht sind.\n\n\n\n\n\n\nTipp\n\n\n\nSchaue dir immer die Daten an! Gibt es ein Muster unter den falschen Vorhersagen, z.B. eine Diskrepanz zwischen der Performance für verschiedene Klassen?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html#sec-pitfall-spurious",
    "href": "06_pitfalls.html#sec-pitfall-spurious",
    "title": "Häufige Fehler vermeiden",
    "section": "[Fehler #3] Modell missbraucht Scheinkorrelationen",
    "text": "[Fehler #3] Modell missbraucht Scheinkorrelationen\nSelbst wenn ein Modell in der Lage ist, richtige Vorhersagen für neue Datenpunkte zu generieren, die den Trainingsdaten ähnlich sind, bedeutet dies nicht, dass das Modell tatsächlich den wahren kausalen Zusammenhang zwischen den Inputs und Outputs gelernt hat!\n\n\n\n\n\n\nWarnung\n\n\n\nML-Modelle machen es sich oft leicht und schummeln! Sie nutzen häufig Scheinkorrelationen, statt die wahren kausalen Zusammenhänge zu lernen. Dies macht sie anfällig für Adverserial Attacks und Daten-Drifts, die das Modell zwingen, zu extrapolieren statt zu interpolieren.\n\n\n\nEine richtige Vorhersage wird nicht immer aus den richtigen Gründen gemacht!\nDie Grafik unten stammt aus einem Paper, in dem die Autoren festgestellt haben, dass ein vergleichsweise einfaches ML-Modell, das auf einem Standard-Bildklassifizierungsdatensatz trainiert wurde, für alle zehn Klassen im Datensatz schlecht abschnitt – bis auf die Klasse ‘Pferd’! Als sie den Datensatz genauer untersuchten und analysierten, warum das Modell eine bestimmte Klasse vorhersagt, d.h. welche Bildmerkmale in der Vorhersage verwendet wurden (angezeigt als Heatmap auf der rechten Seite), machten sie folgende Feststellung: Die meisten Bilder von Pferden im Datensatz stammten vom selben Fotografen und enthielten alle einen charakteristischen Copyright-Vermerk in der linken unteren Ecke.\n\n\n\nLapuschkin, Sebastian, et al. “Analyzing classifiers: Fisher vectors and deep neural networks.” IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n\n\nAnhand dieses Artefaktes konnte das Modell mit hoher Genauigkeit Pferdebilder in diesem Datensatz identifizieren – und zwar sowohl im Trainings- als auch im Testset, das auch Bilder desselben Fotografen enthielt. Trotzdem hat das Modell natürlich nicht gelernt, was ein Pferd eigentlich ausmacht, und es kann nicht extrapolieren und andere Fotos von Pferden ohne diesen Copyright-Hinweis korrekt identifizieren. Andersrum könnte man nun auch ein Bild von einem anderen Tier mit einem solchen Copyright-Vermerk versehen und das Modell würde darauf dann irrtümlich ein Pferd erkennen. Man kann das Modell so also absichtlich austricksen, was man auch als “Adverserial Attack” bezeichnet.\n\nDies ist bei weitem nicht das einzige Beispiel, bei dem ein Modell “geschummelt” hat, indem es Scheinkorrelationen in den Trainingsdaten ausnutzte. Ein weiteres beliebtes Beispiel: Ein Datensatz mit Bildern von Hunden und Wölfen, bei dem alle Wölfe auf verschneitem Hintergrund und die Hunde auf Gras oder anderen nicht-weißen Hintergründen fotografiert wurden. Modelle, die auf so einem Datensatz trainiert werden, können eine gute Vorhersagegenauigkeit aufweisen, ohne dass sie die wahren kausalen Zusammenhang zwischen den Features und Labels erkannt haben.\nUm solche Pannen rechtzeitig zu erkennen, ist es wichtig, das Modell zu interpretieren und seine Vorhersagen zu erklären (wie im oben genannten Paper), um zu sehen, ob das Modell zur Vorhersage die Features verwendet, die wir (oder ein Domänenexperte) erwartet hätten.\n\n\nAdversarial Attacks: ML-Modelle absichtlich täuschen\nBei einem ‘feindlichen Angriff’ auf ein ML-Modell werden die Inputdaten subtil verändert, sodass ein Mensch diese Änderungen nicht bemerkt und immer noch zum richtigen Ergebnis kommt, aber das Modell seine Vorhersage ändert.\nWährend zum Beispiel ein ML-Modell das ‘Stop’-Schild auf dem linken Bild leicht erkennen kann, wird das Schild rechts aufgrund der strategisch platzierten, unscheinbar aussehenden Aufkleber (die ein Mensch einfach ignorieren würde) mit einem Geschwindigkeitsbegrenzungsschild verwechselt:\n\n\n\n\n\nDer Grund dafür ist, dass das Modell nicht die wahren Merkmale gelernt hat, anhand derer Menschen ein Stoppschild als solches identifizieren, z.B. die achteckige Form und die vier weißen Buchstaben ‘STOP’ vor rotem Hintergrund. Stattdessen verlässt sich das Modell auf bedeutungslose Korrelationen, um das Stoppschild von anderen Schildern zu unterscheiden.\nDa ein Convolutional Neural Network (CNN), die neuronale Netzarchitektur, die typischerweise für Bildklassifizierungsaufgaben verwendet wird, sich sehr auf lokale Muster fokussiert, lässt es sich leicht täuschen. Dies geschieht, indem man die globale Form von Objekten, welche Menschen zur Identifikation verwenden, intakt lässt, und die Bilder mit bestimmten Texturen oder anderen Hochfrequenzmustern überlagert, wodurch das Modell eine andere Klasse vorhersagt.\n\n\nGenAI & Adversarial Prompts\nWegen ihrer Komplexität ist es besonders schwierig, den Output von generativen KI-Modellen (GenAI) wie ChatGPT zu kontrollieren. Diese Modelle können zwar in “Human-in-the-Loop” Szenarien sehr nützlich sein (z.B. um eine E-Mail oder Code-Schnipsel zu schreiben, die dann nochmal von einem Menschen überprüft werden), doch es ist schwer, die notwendigen Sicherheitsvorkehrungen zu treffen, damit der Chatbot nicht missbraucht werden kann.\nDer ChatGPT-basierte Kundensupport-Chat eines Chevrolet-Autohändlers ist nur ein Beispiel von vielen frühen GenAI Anwendungen, die bestenfalls gemischte Ergebnisse lieferten:\n\n\n\nScreenshot: https://twitter.com/ChrisJBakke/status/1736533308849443121 (12.1.2024)\n\n\n\n\nWie man robuste Kausalmodelle, die den wahren ‘Input → Output’-Zusammenhang in den Daten erfassen, findet, wird nach wie vor aktiv erforscht und ist weitaus schwieriger als ein Modell zu finden, das “nur” verallgemeinert und gute Vorhersagen für die Testdaten generiert.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html#sec-pitfall-biased",
    "href": "06_pitfalls.html#sec-pitfall-biased",
    "title": "Häufige Fehler vermeiden",
    "section": "[Fehler #4] Modell diskriminiert",
    "text": "[Fehler #4] Modell diskriminiert\nEin Modell, welches echte kausale Zusammenhänge zwischen den Variablen aufgegriffen hat, generiert zwar robustere Vorhersagen, doch es kann auch kausale Zusammenhänge in den historischen Daten geben, die ein Modell besser nicht lernen sollte. Wenn in der Vergangenheit Menschen aufgrund ihres Geschlechts oder ihrer Hautfarbe diskriminiert wurden, kann sich dies auch in den Trainingsdaten widerspiegeln und wir müssen zusätzliche Maßnahmen ergreifen, damit diese Muster nicht in unserem Modell weiterbestehen – obwohl es in der Vergangenheit vielleicht echte kausale Zusammenhänge waren.\n\nSystematisch verzerrte Daten führen zu (stark) verzerrten Modellen\nIm Folgenden sind einige Beispiele aufgeführt, bei denen Menschen mit den besten Absichten ein ML-Modell entwickelt haben, das problematische Dinge aus realen Daten gelernt hat.\n\n\n\n\nWas als Forschungsprojekt begann, um herauszufinden, wie Menschen mit einem KI-basierten Chatbot interagieren, endete für Microsoft als PR-Albtraum. Der Chatbot ‘Tay’ sollte aus den an sie geschriebenen Nachrichten lernen. Aber da die Entwickler offenbar mehr über ihre ML-Modelle als über menschliches Verhalten im Internet nachdachten, wiederholte Tay vor allem rassistische und sexistische Aussagen, die andere ihr gegenüber twitterten.\n\n\n\n\n\n\n\nDa viele auf Twitter gepostete Bilder größer sind als der verfügbare Platz für das Vorschaubild, wollte Twitter “den relevantesten Teil” eines Bildes für die Vorschau mit einem ML-Modell auswählen. Da sie dieses Modell leider auf einem Datensatz trainierten, der mehr Bilder von Menschen mit weißer als dunkler Hautfarbe enthielt, wurde das Modell rassistisch und wählte beispielsweise bei einem Bild von Barack Obama und einem zufälligen unwichtigen weißen Politiker immer den weißen Politiker für das Vorschaubild. Des weiteren fiel auf, dass diese Zuschneide-Algorithmen häufiger Gesichter als Vorschaubilder für Männer und den Körper (insbesondere – wer hätte es gedacht – Brüste) als Vorschaubilder für Frauen auswählten.\n\n\n\n\n\n\n\nDie meisten Computer Vision Modelle werden auf dem ImageNet-Datensatz (vor-)trainiert, der über 14 Millionen handgelabelte Bilder enthält, die in mehr als 20.000 Kategorien organisiert sind. Da diese Bilder jedoch aus dem Internet stammen und mehr Menschen aus Industrieländern als aus Entwicklungsländern dazu neigen, Bilder online zu stellen, sind beispielsweise gängige Haushaltsgegenstände aus reicheren Ländern stark überrepräsentiert. Als Folge verwechseln diese Modelle z.B. Seifenstücke, wie sie in einem ärmeren Land verwendet werden, mit Lebensmitteln (z.B. könnte man argumentieren, dass diese tatsächlich eine gewisse Ähnlichkeit haben mit einem Teller mit Essen in einem Sterne-Restaurant).\nde Vries, Terrance, et al. “Does object recognition work for everyone?” IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019.\n\n\n\nDie oben genannten Probleme traten alle auf, weil die Daten nicht gleichmäßig verteilt waren:\n\nTay hat viel mehr rassistische und hasserfüllte Kommentare und Tweets gesehen als neutrale oder wertschätzende Äußerungen.\nDer Bilddatensatz, auf dem Twitter sein Modell trainierte, enthielt mehr Bilder von weißen als von nicht-weißen Personen.\nBei einer zufälligen Stichprobe von Fotos aus dem Internet wurden diese Bilder meist von Menschen aus Industrieländern hochgeladen, d.h. Bilder, die den Status Quo in Entwicklungsländern zeigen, sind unterrepräsentiert.\n\nNoch problematischer als eine bloße Unterrepräsentation bestimmter Untergruppen (verzerrte Eingabeverteilung) ist ein Muster der systematischen Diskriminierung dieser Untergruppen in historischen Daten (diskriminierende Verschiebung der zugewiesenen Labels).\n\n\n\n\nIn vielen Datensätzen, die zum Trainieren von Modellen für die Vergabe von Kreditscores oder zur Bestimmung von Zinssätzen für Hypotheken oder Kredite verwendet werden, ist oft eine Menge expliziter Diskriminierung kodiert. Da diese Anwendungsbereiche einen direkten und starken Einfluss auf das Leben der Menschen haben, muss man hier besonders vorsichtig sein. Beispielsweise sollte man überprüfen, ob ein Modell für einen Mann und eine Frau den gleichen Score vorhersagt, wenn alle Merkmale mit Ausnahme des Geschlechts bei einem Datenpunkt übereinstimmen.\n\n\n\nZusammenfassend: Ein verzerrtes Modell kann sich auf zwei Arten negativ auf die Nutzer auswirken:\n\nUnverhältnismäßige Produktausfälle aufgrund unterrepräsentierter Stichproben. Beispielsweise funktionieren Spracherkennungsmodelle für Frauen oft weniger zuverlässig, weil sie mit mehr Daten von Männern trainiert wurden (z.B. transkribierte politische Reden).\nSchaden durch Benachteiligung / Verweigerung von Chancen aufgrund von in historischen Daten kodierten Stereotypen. Beispielsweise müssen Frauen höhere Kreditzinsen zahlen als Männer oder im Ausland geborene Personen gelten als weniger qualifiziert für eine Stelle, wenn ihre Lebensläufe von einem automatisierten Screening-Tool bewertet werden.\n\n\n\n\n\n\n\nVorsicht\n\n\n\nWenn man Modelle mit Daten neu trainiert, die von Vorhersagen eines verzerrten Vorgängermodells beeinflusst wurden, können bestehende Vorurteile noch verstärkt werden. Wenn beispielsweise ein Lebenslauf-Screeningtool ein häufiges Merkmal (z.B. “hat die Stanford University besucht”) bei aktuellen Mitarbeitern erkennt, könnte es konsequent Lebensläufe mit diesem Merkmal empfehlen. Daraus resultiert, dass noch mehr Leute mit diesem Merkmal zu Vorstellungsgesprächen eingeladen und eingestellt werden, was die Dominanz dieses Merkmals in nachfolgenden Modellen, die auf diesen Mitarbeiterprofilen trainiert werden, weiter verstärkt.\n\n\n\n\nAuf dem Weg zu fairen Modellen\nBevor wir diese Probleme beheben können, müssen wir uns ihrer erstmal bewusst werden. Daher ist es wichtig, die Performance eines Modells immer für jede (bekannte) Untergruppe in den Daten einzeln zu bewerten, um sicherzustellen, dass die Vorhersagefehler des Modells zufällig sind und das Modell nicht für einige Untergruppen (z.B. Frauen) systematisch schlechter funktioniert.\nAußerdem ist grundsätzlich Vorsicht geboten, wenn wir Variablen in das Modell aufnehmen, die Attribute wie Geschlecht oder Herkunft kodieren. Zum Beispiel wird die Performance eines Modells zur Diagnose von Herzinfarkten durch die Einbeziehung von ‘Geschlecht’ als Merkmal höchstwahrscheinlich verbessert, da Männer und Frauen bei einem Herzinfarkt unterschiedliche Symptome zeigen. Andererseits sollte ein Modell, das jemandem eine Kreditwürdigkeit zuweist, bei dieser Entscheidung das Geschlecht der Person eher nicht berücksichtigen, da ansonsten die in historischen Daten kodierten Stereotypen weiterleben.\nDas Geschlecht oder die Hautfarbe einer Person kann jedoch auch mit anderen Variablen wie beispielsweise Einkommen oder Wohngegend korreliert sein, sodass selbst Features, die auf den ersten Blick harmlos erscheinen, problematische Informationen an das Modell weitergeben können. In solchen Fällen sind zusätzliche Maßnahmen nötig, um zu vermeiden, dass das Modell diskriminiert.\n\nWeitere Negativbeispiele findest du auf der Seite AI Incidence Database und in\n\n\n\n\n\n\nBuch Empfehlung:\nWeapons of Math Destruction von Cathy O’Neil (2016)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "06_pitfalls.html#sec-pitfall-drifts",
    "href": "06_pitfalls.html#sec-pitfall-drifts",
    "title": "Häufige Fehler vermeiden",
    "section": "[Fehler #5] Daten & Konzept Drifts",
    "text": "[Fehler #5] Daten & Konzept Drifts\nWir dürfen nie vergessen, dass sich die Welt permanent verändert und Modelle regelmäßig mit neuen Daten nachtrainiert werden müssen. Nur so können sie sich an diese geänderten Umstände anpassen.\n\n\n\n\n\n\nVorsicht\n\n\n\nML versagt leise! D.h. auch wenn alle Vorhersagen falsch sind, stürzt das Programm nicht einfach mit einer Fehlermeldung ab.\n→ Wir brauchen ein konstantes Monitoring, um Veränderungen zu erkennen, die zu einer Verschlechterung der Modellperformance führen!\n\n\nEins der größten Probleme in der Praxis: Daten und Konzept-Drifts:\nDie Vorhersagegenauigkeit eines Modells lässt schnell nach, wenn die Daten im Produktivbetrieb von den Trainingsdaten abweichen. Dabei unterscheiden wir zwischen:\n\nDaten-Drift: Verteilung der gemessenen Werte einer oder mehrerer Variablen verändert sich. Dies kann entweder die Inputs \\(X\\) betreffen, dann nennt sich das Covariate Shift oder die Outputs \\(y\\), dann sprechen wir von Label Shift.\nKonzept-Drift: Input/Output Zusammenhang \\(X \\to y\\) ändert sich, d.h. exakt die gleichen Inputs \\(X\\) resultieren plötzlich in einem anderen Output \\(y\\).\n\nIn beiden Fällen ändert sich etwas, das für unsere ML Anwendung wichtig ist, in der Welt. Wenn unsere gesammelten Daten diese Veränderung abbilden, spricht man von Daten-Drift. Wenn wir diese Veränderung nicht in unseren Inputdaten sehen können, handelt es sich um einen Konzept-Drift.\nBeispiel: Anhand der Produktionsbedingungen inkl. der Größe des produzierten Teils (\\(X\\)) möchten wir vorhersagen ob das jeweilige Produkt in Ordnung oder Ausschuss ist (\\(y\\)):\n\nDaten-Drift: Der Hersteller produzierte früher nur kleine Teile, nun aber auch größere Teile.\nKonzept-Drift: Während früher 10% Ausschuss produziert wurden, wird nach einer Reparatur der Maschine bei gleichen Produktionsbedingungen (\\(X\\)) nur noch 5% Ausschuss (\\(y\\)) produziert.\n\n\n\n\n\n\n\nTipp\n\n\n\nCovarite Shifts können, ohne dass es einen Konzept-Drift gibt, zu Label Shifts führen, wenn die Inputvariable kausal mit der Outputvariable verbunden ist. Zum Beispiel wurde ein Modell, das Krebs (\\(y\\)) bei Patienten basierend auf dem Alter (\\(x\\)) vorhersagt, mit einem Datensatz trainiert, der größtenteils aus älteren Menschen besteht, die naturgemäß auch häufiger an Krebs erkranken. In der Praxis wird das Modell dann auf Patienten jeden Alters angewendet (Covariate Shift), also auch auf mehr junge Menschen, die seltener an Krebs erkranken (Label Shift).\n\n\n\nGründe für Drifts & Vorbeugende Maßnahmen\nDaten- und Konzeptdrifts entstehen sowohl durch die Art wie die Daten gesammelt werden, als auch durch externe Ereignisse außerhalb unserer Kontrolle.\n\n\n\n\n\n\nHinweis\n\n\n\nDiese Veränderungen können entweder graduell sein (z.B. Sprachen ändern sich schrittweise wenn neue Wörter geprägt werden; ein Kameraobjektiv staubt mit der Zeit ein), oder sie können als plötzlicher Schock auftreten (z.B. jemand reinigt das Kameraobjektiv; als die COVID-19-Pandemie ausbrach, wechselten plötzlich viele Menschen zum Online-Shopping, wodurch Systeme zur Erkennung von Kreditkartenbetrug erstmal irrtümlich Alarm schlugen).\n\n\nGeändertes Datenschema\nViele Probleme entstehen intern und könnten vermieden werden, zum Beispiel:\n\nDie Benutzeroberfläche zur Datensammlung ändert sich, zum Beispiel wurde eine Größe zuvor in Metern erfasst und wird jetzt in Zentimetern erfasst.\nDie Sensor-Konfiguration ändert sich, beispielsweise wird in einer neuen Version eines Geräts ein anderer Sensor verwendet, der jedoch weiterhin Werte unter dem gleichen Variablennamen wie der alte Sensor aufzeichnet.\nDie als Inputs für das Modell verwendeten Features ändern sich, zum Beispiel werden zusätzliche erstellte Features eingeführt, jedoch wurde die Feature-Transformations-Pipeline nur im Trainingscode geändert, noch nicht im Produktionscode.\n\n⇒ In diesen Fällen sollte idealerweise ein Fehler geworfen werden, zum Beispiel könnten wir einige Tests vor der Anwendung des Modells einbauen, um sicherzustellen, dass wir die erwartete Anzahl von Features erhalten, deren Datentypen (z.B. Text oder Zahlen) wie erwartet sind und die Werte grob im erwarteten Bereich für das jeweilige Feature liegen. Darüber hinaus müssen andere Teams im Unternehmen darüber informiert werden, dass ein ML-Modell auf ihren Daten basiert, damit sie das Data-Science-Team rechtzeitig über Änderungen informieren können.\nDaten-Drifts\nDaten-Drifts treten auf, wenn unser Modell Vorhersagen für Stichproben treffen muss, die sich von den Trainingsdaten unterscheiden. Das kann zum Beispiel daran liegen, dass bestimmte Bereiche der Trainingsdomäne unterrepräsentiert waren. Im Extremfall könnte das Modell sogar gezwungen sein, über die Trainingsdomäne hinaus zu extrapolieren. Dies könnte beispielsweise durch folgende Gründe verursacht werden:\n\nVeränderte Stichprobenauswahl, zum Beispiel wenn das Unternehmen kürzlich in ein anderes Land expandiert ist oder nach einer gezielten Marketingkampagne die Website von einer neuen Nutzergruppe besucht wird.\nFeindseliges Verhalten, zum Beispiel wenn Spammer ständig ihre Nachrichten anzupassen, um Spam-Filter zu umgehen. Vor zehn Jahren hätte ein Mensch eine Spam-Nachricht von heute auch als Spam erkannt (die Bedeutung von Spam hat sich also nicht geändert), aber diese ausgefeilteren Nachrichten waren damals nicht im Trainingsdatensatz enthalten. Das macht es für ML-Modelle schwierig, diese Muster zu erkennen.\n\n⇒ Daten-Drifts können als Gelegenheit betrachtet werden, unseren Trainingsdatensatz zu erweitern und das Modell mit mehr Daten von unterrepräsentierten Untergruppen neu zu trainieren. Wie jedoch im vorherigen Abschnitt zu modellbasierter Diskriminierung erläutert wurde, bedeutet dies oft, dass diese unterrepräsentierten Untergruppen zunächst mit einem weniger effektiven Modell arbeiten müssen, beispielsweise eine Spracherkennungsfunktion, die bei Frauen schlechter funktioniert als bei Männern. Daher ist es wichtig, Untergruppen zu identifizieren, bei denen das Modell möglicherweise schlechtere Ergebnisse liefert, idealerweise mehr Daten aus diesen Gruppen zu sammeln oder zumindest beim Modelltraining und -evaluierung diesen Datenpunkten größere Beachtung zu schenken.\nKonzept-Drifts\nKonzept-Drifts treten auf, wenn externe Veränderungen oder Ereignisse eintreten, die wir nicht in unseren Daten erfasst haben oder die die Bedeutung unserer Daten verändern. Das bedeutet, dass genau dieselben Input Features plötzlich zu unterschiedlichen Outputs führen. Ein Grund kann sein, dass uns eine Variable fehlt, die einen direkten Einfluss auf den Output hat, zum Beispiel:\n\nUnser Prozess reagiert auf Temperatur und Luftfeuchtigkeit, aber wir haben nur die Temperatur aufgezeichnet und nicht die Luftfeuchtigkeit. Wenn sich also die Luftfeuchtigkeit ändert, führen dieselben Temperaturwerte zu unterschiedlichen Outputs. ⇒ Luftfeuchtigkeit zusätzlich als Input Feature im Modell aufnehmen.\nSaisonale Trends führen zu Veränderungen in der Beliebtheit von Sommer- gegenüber Winterkleidung. ⇒ Monat / Außentemperatur als zusätzliches Input Feature hinzufügen.\nBesondere Ereignisse, wie zum Beispiel wenn ein Prominenter unser Produkt in den sozialen Medien erwähnt oder Menschen aufgrund von Lockdowns während einer Pandemie ihr Verhalten ändern. ⇒ Obwohl es schwer sein kann, diese Ereignisse im Voraus zu prognostizieren, können wir, wenn sie eintreten, ein zusätzliches Feature wie ‘während des Lockdowns’ aufnehmen, um die in diesem Zeitraum gesammelten Daten von den übrigen zu unterscheiden.\nDegenerative Feedbackschleifen, d.h., die Existenz des Modells ändert das Verhalten der Benutzer, zum Beispiel veranlasst ein Empfehlungssystem Benutzer dazu, auf Videos zu klicken, nur weil sie empfohlen wurden. ⇒ Als zusätzliches Feature aufnehmen, ob das Video empfohlen wurde oder nicht, um herauszufinden, wie viel von “Benutzer hat auf Element geklickt” auf die Empfehlung zurückzuführen ist und wie viel auf das natürliche Verhalten des Benutzers.\n\nZusätzlich können Konzept-Drifts durch Ereignisse verursacht werden, die die Bedeutung der aufgezeichneten Daten ändern, zum Beispiel:\n\nInflation: 1 Euro im Jahr 1990 hatte einen höheren Wert als 1 Euro heute. ⇒ Daten Inflationsbereinigen oder die Inflationsrate als zusätzliches Input Feature aufnehmen.\nEin in Wasser getauchter Temperatursensor sammelt Kalkablagerungen, und nach einer Weile ist die Temperaturmessung nicht mehr genau. Zum Beispiel misst ein sauberer Sensor bei einer tatsächlichen Temperatur von 90 Grad die exakten 90 Grad, aber nachdem er einige Schichten Kalk angesammelt hat, misst er unter denselben Bedingungen nur noch 89 Grad. Während unser Output von der wahren Temperatur beeinflusst wird, haben wir nur Zugriff auf die Sensormessung für die Temperatur, die auch durch den Zustand des Sensors selbst bestimmt wird. ⇒ Versuche, die Kalkablagerung zu schätzen, zum Beispiel basierend auf der Anzahl der Tage seit der letzten Reinigung des Sensors (was auch bedeutet, dass solche Wartungsereignisse irgendwo erfasst werden müssen!).\n\n\n\n\nKausaldiagramm, das zeigt, wie unser beobachteter Input \\(x\\) (Temperaturmessung) und Output \\(y\\) durch versteckte Variablen beeinflusst werden (auf die wir keinen direkten Zugriff haben). Diese versteckten Variablen sind der Zustand des Temperatursensors (d.h. wie viel Kalk sich angesammelt hat), die tatsächliche Temperatur und die Luftfeuchtigkeit (für die wir noch keinen Sensor installiert haben). Wenn der Sensorzustand und die Luftfeuchtigkeit konstant bleiben, können wir den Output aus der Temperaturmessung vorhersagen. Wenn sich jedoch einer dieser Werte ändert, treten Konzept-Drifts auf. Daher sollten wir versuchen, Schätzungen dieser versteckten Variablen in unser Modell aufzunehmen, um diese Veränderungen zu berücksichtigen.\n\n\n⇒ Vor dem Training eines Modells sollten die Daten untersucht werden, um Fälle zu identifizieren, bei denen identische Inputs unterschiedliche Outputs ergeben. Wenn möglich, sollten zusätzliche Input Features aufgenommen werden, um diese Variationen zu berücksichtigen. Eine schlechte Performance des Modells auf dem Testdatensatz deutet häufig darauf hin, dass relevante Inputs fehlen, was die Anfälligkeit für zukünftige Konzept-Drifts erhöht. Selbst wenn die richtigen Variablen verwendet werden, um einen Konzept-Drift zu erfassen, kann häufiges Nachtrainieren der Modelle dennoch notwendig sein. Zum Beispiel können unterschiedliche Zustände des Konzepts ungleichmäßig in den Trainingsdaten vorhanden sein, was zu Daten-Drifts führen kann (z.B. mehr Daten, die im Winter gesammelt wurden als in den frühen Sommermonaten). Wenn es nicht möglich ist, Variablen einzubeziehen, die den Konzept-Drift abbilden, könnte es notwendig sein, Datenpunkte aus dem ursprünglichen Trainingsdatensatz zu entfernen, die nicht der neuen Input/Output-Beziehung entsprechen, bevor das Modell erneut trainiert wird.\n\n\n\n\n\n\nTipp\n\n\n\nDer beste Weg, Daten und Konzept-Drifts entgegenzuwirken, besteht darin, das Modell häufig mit neuen Daten zu trainieren. Dies kann entweder nach einem fixen Zeitplan erfolgen (z.B. jedes Wochenende, je nachdem, wie schnell sich die Daten ändern) oder wenn das Monitoringsystem Alarm schlägt, weil es Drifts in den Inputs oder eine verschlechterte Modellperformance festgestellt hat.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Häufige Fehler vermeiden</span>"
    ]
  },
  {
    "objectID": "08_conclusion.html",
    "href": "08_conclusion.html",
    "title": "Fazit",
    "section": "",
    "text": "Hype vs. Realität\nNachdem wir nun viel über die Theorie des maschinellen Lernens (ML) gesprochen haben, ist es Zeit für einen Realitätscheck.\nIn der Einleitung haben wir viele Beispiele gesehen, die zum ML-Hype beigetragen haben. Doch beim Einsatz von ML beispielsweise in der Fertigungs- oder Prozessindustrie sieht die Realität oft ganz anders aus und nicht jede Idee funktioniert wie erhofft:\nWenn du jetzt neugierig geworden bist und mehr darüber erfahren möchtest, wie die verschiedenen ML-Algorithmen im Detail funktionieren, wirf einen Blick in die Vollversion dieses Buches!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fazit</span>"
    ]
  },
  {
    "objectID": "08_conclusion.html#sec-ai-transformation",
    "href": "08_conclusion.html#sec-ai-transformation",
    "title": "Fazit",
    "section": "KI Transformation eines Unternehmens",
    "text": "KI Transformation eines Unternehmens\nDer berühmte ML-Forscher Andrew Ng hat einen fünfstufigen Prozess vorgeschlagen, um ein Unternehmen in ein datengesteuertes Unternehmen zu verwandeln, welches in der Lage ist, KI produktiv zur Wertschöpfung einzusetzen.\n\nFünf Schritte für eine erfolgreiche KI-Transformation nach Andrew Ng\n\n\n\n\nDurchführung von Pilotprojekten, um in Schwung zu kommen\nAufbau eines internen KI-Teams und der Dateninfrastruktur\nAngebot eines breiten KI-Trainings (für alle Mitarbeiter)\nEntwicklung einer KI- und Datenstrategie\nAusbau der internen und externen Kommunikation\n\nEmpfohlene Materialien:\n→ “AI for everyone” Coursera Kurs\n→ AI Transformation Playbook\n\n\n\n\n\nProf. Dr. Andrew Ng\nCo-Founder Google Brain\nVice President Baidu\nCo-Founder Coursera\nProfessor @ Stanford University\n\n\n\n\n\n\n\n[Schritt 1] Beginne mit kleinen Pilotprojekten, um das Potenzial und die Herausforderungen bei der Verwendung von ML zu verstehen\nML-Projekte sind anders als herkömmliche Softwareprojekte, bei denen man normalerweise zumindest weiß, dass eine Lösung existiert, und man muss nur einen effizienten Weg finden diese umzusetzen. Stattdessen ist ML abhängig von den verfügbaren Daten. Auch wenn es theoretisch möglich wäre, ein Problem mit ML zu lösen, könnte dies an der Datenqualität oder -quantität scheitern. Bevor man eine unternehmensübergreifende KI-Initiative umsetzt, ist es daher ratsam, mit mehreren kleineren Pilotprojekten zu starten, um ein besseres Gefühl dafür zu bekommen, was es bedeutet wenn man sich bei der Lösung seiner Probleme auf eine KI verlässt.\nBei der Auswahl eines Pilotprojekts ist der Return on Investment (ROI) des Projekts nicht der wichtigste Faktor, sondern hier stehen die bei der Umsetzung gesammelten Erfahrungen mit ML im Vordergrund. Es ist jedoch wichtig, ein Projekt zu wählen, das technisch machbar ist. Es sollten also bereits ML-Algorithmen für diese Problemstellung existieren, sodass keine jahrelange Forschung nötig ist, um eine eigene ausgefallene neuronale Netzwerkarchitektur zu entwickeln. Weiterhin sollte man über genügend hochwertige Daten verfügen, um zügig loszulegen zu können, damit man nicht Monate nur mit der Datenvorverarbeitung verbringt, weil z.B. Daten aus verschiedenen Quellen einer fragmentierten Dateninfrastruktur kombiniert werden müssen.\nFehlende KI-Resourcen kann man in diesem Schritt mit externen Berater:innen ausgleichen, die die ML-Expertise bereitstellen, während man selbst das Domänenwissen für den Erfolg des Pilotprojekts beisteuert.\n\n\n[Schritt 2] Richte ein zentralisiertes KI-Team und eine Dateninfrastruktur ein, um größere Projekte effizient und effektiv durchzuführen\nWir haben bereits gesehen, dass wir in der Praxis an der Schnittstelle von Theorie, Programmierung und Fachwissen arbeiten, also Data Science brauchen. Es ist jedoch unwahrscheinlich, dass man eine einzelne Person findet, die in allen drei Bereichen wirklich kompetent ist. Stattdessen haben die Menschen immer einen gewissen Fokus, weshalb wir hier drei unterschiedliche Rollen vorschlagen, die auch sehr gut zu den drei Hauptschritten für die erfolgreiche Durchführung eines ML-Projekts passen:\n\n\n\nWährend Data Strategists in ihren jeweiligen Abteilungen geeignete Probleme identifizieren, die von ML profitieren würden, können Data Scientists experimentieren und prototypische Lösungen für diese Probleme entwickeln, welche dann von Data & ML Engineers produktiv in den Einsatz gebracht werden.\n\n\nIdealerweise sind Data Scientists und Engineers in einem eigenen separaten Team (dem “KI-Team”) und arbeiten an Projekten aus unterschiedlichen Abteilungen wie bei einer internen Beratung:\n\n\n\n[Adaptiert von: “AI for everyone” von Andrew Ng (coursera.org)]\n\n\nDies hat mehrere Vorteile:\n\nData Scientists können mit anderen ML-Expert:innen Lösungen diskutieren → viele Probleme aus unterschiedlichen Geschäftsbereichen benötigen ähnliche Algorithmen.\nDaten aus dem gesamten Unternehmen können für eine ganzheitliche Analyse kombiniert werden.\nDie Finanzierung ist unabhängig von einzelnen Geschäftsbereichen, z.B. erforderlich für die Vorabinvestitionen in eine Dateninfrastruktur und zusätzliche Weiterbildungsmaßnahmen um mit neuer ML-Forschung Schritt zu halten.\n\nWie wir in der Einleitung besprochen haben, werden in einem ML-Projekt etwa 90% der Zeit mit Data Wrangling und Preprocessing verbracht. Daher sollte das KI-Team besonders am Anfang mehr Data Engineers als Data Scientists haben, damit diese eine solide Dateninfrastruktur aufbauen können, die den Data Scientists später viel Zeit und Kopfschmerzen erspart.\n\n\n[Schritt 3] Schule andere Mitarbeiter, um ML-Probleme zu erkennen und eine Daten-Kultur zu etablieren\nWährend Data Scientists die von ihnen verwendeten Algorithmen im Detail verstehen müssen, sollten andere Mitarbeiter (insbesondere Data Strategists und Abteilungsleiter:innen) ein grundlegendes Verständnis davon haben, wozu ML fähig ist und wozu nicht, damit sie mögliche ML-Probleme in ihrem Bereich identifizieren und an das KI-Team weitergeben können.\n\n\n\nIch habe Schulungen auf verschiedenen Niveaus für alle Zielgruppen konzipiert.\n\n\n\n\n[Schritt 4] Entwickle eine schlüssige Strategie mit langfristigen Zielen, die zu einem Wettbewerbsvorteil führt\nDie Entwicklung einer Strategie mag der erste Impuls einer Führungskraft sein, wenn sie mit einem neuen Thema wie KI konfrontiert wird. Da sich KI-Probleme jedoch so stark von anderen Projektarten unterscheiden, lohnt es sich wirklich, zuerst Erfahrung mit diesem Thema zu sammeln (also mit Schritt 1 zu beginnen!). Nachdem einige Pilotprojekte erfolgreich abgeschlossen und die Räder in Gang gesetzt sind, um ein KI-Team zu bilden sowie die anderen Mitarbeiter zu schulen, gibt es hier ein paar Dinge, die man beim Entwickeln einer unternehmensweiten KI-Strategie beachten sollte:\n\nErstelle strategische Datenassets, die für eure Konkurrenz schwer zu replizieren sind:\n\nLangfristige Planung: Welche Daten könnten in Zukunft wertvoll sein? → Fange jetzt an, sie zu sammeln!\nVorabinvestitionen: Welche Infrastruktur und Prozesse sind erforderlich, um die Daten den richtigen Personen zugänglich zu machen?\nWie können Daten aus verschiedenen Abteilungen kombiniert werden, damit das KI-Team “die Punkte verbinden” kann und dadurch einen einzigartigen Wettbewerbsvorteil schaffen kann?\nWelche Möglichkeiten existieren in Bezug auf eine strategische Datenakquise, z.B. in Form von “kostenlosen” Produkten, bei denen Nutzer mit ihren Daten bezahlen (wie bei Google, Facebook, etc.)?\n\nErstelle KI-gestützte Anwendungen, die ein Alleinstellungsmerkmal für eure Produkte sind:\n\nVersuche nicht, eine Standardlösung zu bauen, die man auch leicht von einem externen Anbieter einkaufen könnte. Kombiniere ML stattdessen mit eurem einzigartigen Fachwissen und Daten, um neue Funktionen für bestehenden Produkte zu entwickeln. Somit gewinnen sie für Kunden an Attraktivität und es können neue Marktbereiche erschlossen werden.\nWie könnt ihr einen positiven Kreislauf etablieren, in dem eine KI mehr Nutzer anzieht, welche wiederum mehr Daten generieren, mit denen die KI verbessert werden kann, um so weitere Nutzer anzuziehen?\n\n\n\n\n\n\n\n\n\n[Schritt 5] Kommuniziert euren Erfolg\nNach erfolgreicher Implementierung von KI im Unternehmen solltet ihr euren Erfolg natürlich auch kommunizieren. Dazu gehören neben internen und externen Pressemitteilungen zum Beispiel auch Stellenangebote. Zeigen diese eure KI Kompetenzen, statt eine Reihe von Buzzwords aufzulisten, ziehen sie automatisch qualifiziertere Kandidat:innen an.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fazit</span>"
    ]
  }
]
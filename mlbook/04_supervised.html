<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Supervised Learning Basics – A Practitioner's Guide to Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05_supervised_models.html" rel="next">
<link href="./03_unsupervised.html" rel="prev">
<link href="./favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-37b241f53b32a5ad598e4053e71b073f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04_supervised.html"><span class="chapter-title">Supervised Learning Basics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A Practitioner’s Guide to Machine Learning</a> 
        <div class="sidebar-tools-main">
    <a href="./A-Practitioner-s-Guide-to-Machine-Learning.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01a_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01b_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01c_python.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">ML with Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Data Analysis &amp; Preprocessing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_unsupervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Unsupervised Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Supervised Learning Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_supervised_models.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Supervised Learning Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pitfalls.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Avoiding Common Pitfalls</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_advanced_topics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advanced Topics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Conclusion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#different-types-of-models" id="toc-different-types-of-models" class="nav-link active" data-scroll-target="#different-types-of-models">Different types of models</a>
  <ul class="collapse">
  <li><a href="#problem-type-regression-vs.-classification" id="toc-problem-type-regression-vs.-classification" class="nav-link" data-scroll-target="#problem-type-regression-vs.-classification">Problem type: regression vs.&nbsp;classification</a></li>
  <li><a href="#problem-complexity-linear-or-nonlinear" id="toc-problem-complexity-linear-or-nonlinear" class="nav-link" data-scroll-target="#problem-complexity-linear-or-nonlinear">Problem complexity: linear or nonlinear</a></li>
  <li><a href="#algorithmic-approaches-features-based-vs.-similarity-based-models" id="toc-algorithmic-approaches-features-based-vs.-similarity-based-models" class="nav-link" data-scroll-target="#algorithmic-approaches-features-based-vs.-similarity-based-models">Algorithmic approaches: features-based vs.&nbsp;similarity-based models</a></li>
  </ul></li>
  <li><a href="#sec-model-eval" id="toc-sec-model-eval" class="nav-link" data-scroll-target="#sec-model-eval">Model Evaluation</a>
  <ul class="collapse">
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model Selection</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-supervised-basics" class="quarto-section-identifier"><span class="chapter-title">Supervised Learning Basics</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Now that we’ve surveyed the different unsupervised learning algorithms, let’s move on to supervised learning:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/01b_ml_overview/ml_algorithms_supervised.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<section id="supervised-learning-in-a-nutshell-with-scikit-learn" class="level4">
<h4 class="anchored" data-anchor-id="supervised-learning-in-a-nutshell-with-scikit-learn">Supervised learning in a nutshell (with scikit-learn):</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/supervised_learning_14.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption>First, the available data needs to be split into a training and test part, where we’re assigning the majority of the data points to the training set and the rest to the test set. (Please note that, as an exception, in this graphic, <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are rotated by 90 degrees, i.e., the features are in the rows and the data points are in the columns.) Next, we need to decide on the type of model that we want to use to describe the relationship between the inputs <span class="math inline">\(X\)</span> and the outputs <span class="math inline">\(\mathbf{y}\)</span> and this model also comes with some hyperparameters that we need to set (which are passed as arguments when instantiating the respective sklearn class). Then we can call the <code>.fit(X_train, y_train)</code> method on the model to learn the internal model parameters by minimizing some model-specific objective function on the training data. Now the model is ready to generate predictions for new data points, i.e., by calling <code>.predict(X_test)</code>, we obtain the predicted values <span class="math inline">\(\mathbf{\hat{y}}\)</span> for the test points. Finally, to get an estimate of how useful the model will be in practice, we evaluate it by comparing the predicted target values of the test set to the corresponding true labels.</figcaption>
</figure>
</div>
<p>In the following sections, we introduce the different approaches to supervised learning and explain when to use which kind of model, then discuss how to evaluate and select the hyperparameters of a supervised learning model.</p>
</section>
<section id="different-types-of-models" class="level2">
<h2 class="anchored" data-anchor-id="different-types-of-models">Different types of models</h2>
<p>The most important task of a data scientist is to select an appropriate model (and its hyperparameters) for solving a problem.</p>
<div class="custom-gray-block">
<p><u>Three considerations when choosing a supervised learning model</u></p>
<ol type="1">
<li><p><strong>Problem type:</strong><br>
What kind of problem are you trying to solve: <em>regression</em> or <em>classification</em>?<br>
→ Depends on the type of target variable, i.e., if it has continuous or discrete values.</p></li>
<li><p><strong>Problem complexity:</strong><br>
How complicated is the relationship between the input features and target variable: <em>linear</em> or <em>nonlinear</em>?<br>
→ Depends on the available data, i.e., how easily the target can be predicted from the inputs.</p></li>
<li><p><strong>Algorithmic approach:</strong><br>
Which type of model works best for this dataset size &amp; complexity: <em>features-based</em> or <em>similarity-based</em>?<br>
→ Depends on the model you choose, i.e., it either learns according to the first or second strategy.</p></li>
</ol>
</div>
<section id="problem-type-regression-vs.-classification" class="level3">
<h3 class="anchored" data-anchor-id="problem-type-regression-vs.-classification">Problem type: regression vs.&nbsp;classification</h3>
<p>The type of the target variable that we want to predict determines whether we are dealing with a regression or classification problem.</p>
<p><strong><em>Regression:</em></strong><br>
Prediction of continuous value(s) <em>(e.g., price, number of users, etc.)</em>.</p>
<p><strong><em>Classification:</em></strong><br>
Prediction of discrete values:</p>
<ul>
<li>binary <em>(e.g., product will be faulty: yes/no)</em></li>
<li>multi-class <em>(e.g., picture displays cat/dog/house/car/…)</em></li>
<li>multi-label <em>(e.g., picture may display multiple objects)</em></li>
</ul>
<p>→ Many classification models actually predict probabilities for the different classes, i.e., a score between 0 and 1 for each class. The final class label is then chosen by applying a threshold on this score (typically 0.5 for binary classification problems) or by taking the outcome with the highest score (in multi-class problems).</p>
<p>⇒ Whether we are dealing with a regression or classification problem is important to know and has implications for our overall workflow, e.g., how we define &amp; measure success. However, the actual models that we use to solve these problems are very similar, e.g., almost all <code>sklearn</code> models exist in either a <code>Regressor</code> or <code>Classifier</code> variant to generate the appropriate output for the respective problem type.</p>
<div class="custom-gray-block">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In some cases it is even entirely up to us whether to frame the task as a regression or classification problem.</p>
</div>
</div>
<p><u>Example:</u> A product is deemed faulty, if it breaks within the warranty period of 6 months, where we assume that normally the product would be used at most 300 times during these 6 months.</p>
<p>Depending on the use case, we might either be interested in how long a product lasts in total, or only whether this product will be a warranty case or not, i.e., we could formulate the problem as:</p>
<ul>
<li>A <strong>regression task</strong>: Predict, <em>after how many uses</em> the product will break.</li>
<li>A <strong>classification task</strong>: Predict, <em>if the product will break</em> before it was used 300 times.</li>
</ul>
<p>However, in many cases with a continuous target variable, we can learn a more accurate prediction model with the regression approach, since here the labels carry more information and edge case errors are not penalized as much. For example, in the classification case, predicting that a product is okay when it lasted 299 uses would be just as wrong as predicting ‘okay’ for a product that lasted only 2 uses.</p>
<p>If the workflow, where our model is later embedded in, requires a classification output, we can still transform the regression output of the model into a classification output later by simply setting a threshold, i.e., in this case if the regression model predicts a value lower than 300 we would output ‘faulty’ and otherwise ‘okay’.</p>
</div>
</section>
<section id="problem-complexity-linear-or-nonlinear" class="level3">
<h3 class="anchored" data-anchor-id="problem-complexity-linear-or-nonlinear">Problem complexity: linear or nonlinear</h3>
<p>In accordance with the product warranty example described above, we now illustrate what it means for a problem to be linear or nonlinear on a small toy dataset:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/problem_types2.png" class="img-fluid figure-img" style="width:35.0%"></p>
<figcaption>The first input feature we consider is the production temperature, which is an independent variable, since the operator in control of the production process is free to just choose some value here. The target variable that we want to predict is the lifetime of the produced product and is dependent on this input variable. For the different temperature settings that were tested, we have collected the respective lifetime measurements (and, as you can imagine, getting such labels can be quite expensive, since these products not only needed to be produced, but also go through some stress test to determine after how many uses they break).</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/problem_types8.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>As you might have already guessed, the relationship between the target variable lifetime and the input feature temperature <em>(top left)</em> is a linear one, i.e., we could describe it by a straight line. Since the lifetime is a continuous variable, this is a regression problem. However, by defining some cutoff on the target variable (e.g., at 300 usages for in and out of warranty), this problem can be transformed into a classification problem, where the task here is to distinguish between faulty and good products <em>(bottom left)</em>. Again, the relationship between the target and the input variable temperature is linear, since the problem can be solved by drawing one line parallel to the y-axis, which defines some threshold on the temperature, where all data points with a temperature lower than this threshold would be classified as ‘ok’ and those products produced at a higher temperature would be predicted as ‘broken’. In the center panel, we have the same situation only this time with a different input feature, pressure. In the regression case, the lifetime values now follow some quadratic curve, i.e., we could not adequately describe the relationship between our target variable and the input feature ‘pressure’ with a single straight line, which makes this a nonlinear problem. Similarly, in the classification case below, we would need to define two thresholds on the pressure values to separate the two classes, i.e., again we could not solve the problem with a single straight line. While these four examples were univariate problems, i.e., with a single input variable, in machine learning we usually deal with multivariate problems, often with hundreds or even thousands of input features. For the case of two input variables, this is illustrated on the right, where the data is now plotted in a scatter plot where the axis correspond to the features temperature and pressure, while the color of the dots indicates the target value (either on a continuous scale or the discrete class labels). In the multidimensional case, a problem would be considered linear if it can by solved by a single hyperplane (instead of a line) in the input feature space.</figcaption>
</figure>
</div>
<p>As illustrated in the above examples, whether a problem can be solved by a simple linear model (i.e., a single straight line or hyperplane) or requires a more complex nonlinear model to adequately describe the relationship between the input features and target variable entirely depends on the given data.<br>
This also means that sometimes we can just install an additional sensor to measure some feature that is linearly related to the target variable or do some feature engineering to then be able to get satisfactory results with a linear model, i.e., sometimes, with the right preprocessing, a nonlinear problem can also be transformed into a linear one.</p>
</section>
<section id="algorithmic-approaches-features-based-vs.-similarity-based-models" class="level3">
<h3 class="anchored" data-anchor-id="algorithmic-approaches-features-based-vs.-similarity-based-models">Algorithmic approaches: features-based vs.&nbsp;similarity-based models</h3>
<p>Finally, lets look at how the different models work and arrive at their predictions. This is what really distinguishes the various algorithms, whereas we have already established that there always exists a regression and a classification variant of each model and some models are inherently expressive enough that they can be used to describe nonlinear relationships in the data, while others will only yield satisfactory results if there exists a linear relationship between the available input features and the target variable.</p>
<p>Features-based models learn some parameters or rules that are applied directly to a new data point’s input feature vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>. Similarity-based models, on the other hand, first compute a vector <span class="math inline">\(\mathbf{s} \in \mathbb{R}^n\)</span> with the similarities of the new sample to the training data points and the model then operates on this vector instead of the original input features.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/algo_approach_handout.png" class="img-fluid figure-img" style="width:65.0%"></p>
<figcaption><em>Left:</em> Features-based models are described by a parametrized function that operates on the original input features. For example, in the regression case <em>(top)</em>, the predicted target for a new data point is the value on the line corresponding to its input features, while in the classification case <em>(bottom)</em>, the prediction is made according to on which side of the dividing line the inputs lie. <em>Right:</em> The similarity-based models make the prediction for a new sample based on this data point’s nearest neighbors. Depending on the type of model, this could be as simple as predicting the average target value of the nearest neighbors (regression, <em>top</em>) or their most frequent class (classification, <em>bottom</em>).</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>This distinction between algorithmic approaches is not only interesting from a theoretical point of view, but even more so from a practitioner’s perspective: When using a similarity-based algorithm, we have to be deliberate about which features to include when computing the similarities, make sure that these features are appropriately scaled, and in general think about which similarity measure is appropriate for this data. For example, we could capture domain knowledge by using a custom similarity function specifically tailored to the problem. When using a features-based model, on the other hand, the model itself can learn which features are most predictive by assigning individual weights to each input feature and therefore possibly ignore irrelevant features or account for variations in heterogeneous data. But of course, subject matter expertise is still beneficial here, as it can, for example, guide us when engineering additional, more informative input features.</p>
</div>
</div>
<!-- ../images/04_supervised_intro/comparison_both.png[image,width=300,align="center",caption=""] -->
<p>Okay, now, when should we use which approach?</p>
<p><strong>Features-based models:</strong></p>
<ul>
<li>Number of features should be less than the number of samples!</li>
<li>Good for heterogeneous data due to individual feature weights (although scaling is usually still a good idea).</li>
<li>Easier to interpret (since they describe a direct relationship between input features &amp; target).</li>
</ul>
<p><strong>Similarity-based models:</strong></p>
<ul>
<li>Nonlinear models for small datasets.</li>
<li>Need appropriate similarity function → domain knowledge! (especially for heterogeneous data)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/comparison_all4.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption>This is an overview of the different features-based and similarity-based models we’ll discuss in the following chapters, as well as for which dataset size and problem complexity they are appropriate: While linear models are rather simple and – as the name suggests – only yield good results for linear problems, they are very efficient and can be applied to large datasets. Decision trees are very efficient as well, but can also capture more complex relationships between inputs and output. Similarity-based models like <em>k</em>-nearest neighbors (kNN) or their more sophisticated cousin kernel methods, on the other hand, can be used to solve very complex nonlinear problems, however, as they require the computation of a similarity matrix that scales with the number of training points, they should only be applied to comparatively small datasets (~a few thousand points). Finally, neural networks are the most sophisticated model class and can be used to solve extremely complex problems, but they are also rather data hungry (depending on the size of the network).</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-model-eval" class="level2">
<h2 class="anchored" data-anchor-id="sec-model-eval">Model Evaluation</h2>
<p>Since in supervised learning problems we know the ground truth, we can objectively evaluate different models and benchmark them against each other.</p>
<section id="is-this-a-good-model-for-the-task" class="level4 custom-gray-block">
<h4 class="anchored" data-anchor-id="is-this-a-good-model-for-the-task">Is this a good model for the task?</h4>
<p>I.e., does the model generate <em>reliable predictions</em> for <em>new data points</em>?</p>
<ul>
<li><p>Split the data into training and test sets to be able to get a reliable estimate of how the model will later perform when applied to new data points that it wasn’t trained on.</p></li>
<li><p>Quantify the quality of the model’s predictions on the test set with a suitable evaluation metric (depending on the problem type).<br>
<strong>Evaluation metrics:</strong></p>
<ul>
<li><em>Regression:</em> mean absolute error, mean squared error, <span class="math inline">\(R^2\)</span></li>
<li><em>Classification:</em> (balanced) accuracy</li>
<li><em>Ranking:</em> <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision/recall</a> (→ F1 score), hits@<em>k</em></li>
</ul>
<p>⇒ Are some mistakes worse than others (e.g., consider false positives vs.&nbsp;false negatives in medical tests)?<br>
⇒ Always choose <em>a single metric/KPI</em> to optimize (maybe: additional constraints like runtime).</p></li>
<li><p>Compare the model to a ‘stupid baseline’ that predicts the mean (→ regression) or most frequent class (→ classification).</p></li>
</ul>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<p>We start with three evaluation metrics for regression problems: the mean absolute error, mean squared error, and <span class="math inline">\(R^2\)</span>.</p>
<section id="mean-absolute-error-mae" class="level4">
<h4 class="anchored" data-anchor-id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_mean_absolute.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>This is probably the most straightforward regression error metric and additionally easy to interpret since the error is given in the same units of measurement as the target variable (e.g., if we’re predicting a price in euros, we would know exactly by how many euros the model is off on average).</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="mean-squared-error-mse" class="level4">
<h4 class="anchored" data-anchor-id="mean-squared-error-mse">Mean Squared Error (MSE)</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_mean_squared.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Since this regression error metric is differentiable, it is often used internally when optimizing the parameters of a model (e.g., in linear regression). When reporting the final error of a model, one often takes the square root of the result, i.e., instead reports the root mean squared error (RMSE), since this is again in the same units as the original target variable (but still less intuitive than the MAE).</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="r2" class="level4">
<h4 class="anchored" data-anchor-id="r2"><span class="math inline">\(R^2\)</span></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_r2.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>The <span class="math inline">\(R^2\)</span>, or coefficient of determination, essentially compares the MSE of a regression model against the MSE of the ‘stupid baseline’ for the regression (i.e., predicting the mean), i.e., it normalizes the MSE by the variance of the data. In the best case, the <span class="math inline">\(R^2\)</span> is 1, i.e., when the model explains the data perfectly, and in the worst case, it can even become negative, i.e., when the model performs worse then simply predicting the mean.</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now lets look at evaluation metrics for classification problems.</p>
</section>
<section id="classification-errors-in-detail" class="level4">
<h4 class="anchored" data-anchor-id="classification-errors-in-detail">Classification errors in detail</h4>
<!-- ../images/06_evaluation/error_class_dist2.png[image,width=640,align="center",caption=""] -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_class2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Most (binary) classification models predict the probability that a sample belongs to a certain class, i.e., a score between 0 and 1. By setting a threshold on this probability (typically at 0.5), the prediction score can be transformed into the class label, i.e., either predicting the positive or negative class for this sample. We distinguish between two types of mistakes that a classification model can make in this prediction: <em>false positives (FP)</em>, i.e., incorrectly predicting the positive class for samples belonging to the negative class, and <em>false negatives (FN)</em>, i.e., incorrectly predicting the negative class for samples from the positive class. Depending on the goal of our application, one type of error might be worse than the other, e.g., in a medical test we might rather tell someone that they should come in for a second test to confirm some problematic results than send them home with an undetected disease. By moving the classification threshold, we can control the trade-off between FP and FN.</figcaption>
</figure>
</div>
</section>
<section id="accuracy" class="level4">
<h4 class="anchored" data-anchor-id="accuracy">Accuracy</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_class_accuracy.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption>The accuracy is the most widely used classification evaluation metric, where we simply check, out of all samples, how many were classified correctly (i.e., TP and TN). However, this can be misleading for unequal class distributions and we should always compare the accuracy of the model against the ‘stupid baseline’ for classification, i.e., what the accuracy would be for a “model” that always predicts the most frequent class.</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="unbalanced-class-distributions-accuracy-vs.-balanced-accuracy" class="level4 custom-gray-block">
<h4 class="anchored" data-anchor-id="unbalanced-class-distributions-accuracy-vs.-balanced-accuracy">Unbalanced class distributions: Accuracy vs.&nbsp;Balanced Accuracy</h4>
<p>Below we see the decision boundaries of two models on a toy dataset, where the background color indicates whether the model predicts the blue or red class for a data point in this area. Which model do you think is more useful?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/balanced_acc4a.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>With unbalanced class distributions, e.g., in this case a lot more samples from the blue compared to the red class, the accuracy of a model that simply always predicts the most frequent class can be quite large. But while a 90% accuracy might sound impressive when we report the performance of a model to the project’s stakeholders, this does not necessarily mean that the model is useful, especially since in real world problems the undersampled class is often the one we care about most, e.g., people with a rare disease or products that have a defect.</p>
<p>Instead, the balanced accuracy is often the more informative measure when evaluating classification models and can help us to distinguish between a model that has actually learned something and the ‘stupid baseline’:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/balanced_acc4b.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</section>
<section id="balanced-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="balanced-accuracy">Balanced Accuracy</h4>
<p>To avoid pitfalls of accuracy: consider misclassification rates of both classes separately:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/error_class_balanced_accuracy.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption>For the balanced accuracy, first the fraction of correctly classified points is computed for each class individually and then these values are averaged.</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> balanced_accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="multi-class-problems-micro-vs.-macro-averaging" class="level4">
<h4 class="anchored" data-anchor-id="multi-class-problems-micro-vs.-macro-averaging">Multi-class problems: micro vs.&nbsp;macro averaging</h4>
<p>The accuracy and balanced accuracy scores can be generalized to the multi-class classification case. Here we instead use the terms micro- and macro-averaging to describe the two strategies (which can also be used for other kinds of metrics like the F1-score), where micro-averaging means we compute the score by averaging over all samples, while macro-averaging means we first compute the score for each class separately and then average over the values for the different classes.</p>
<p><strong>Micro-averaged score (→ <code>accuracy_score</code>):</strong></p>
<p><span class="math display">\[
\begin{aligned}
\frac{TP + TN}{TP + FN + TN + FP} = \frac{TP_\text{pos} + TP_\text{neg}}{n_\text{pos} + n_\text{neg}} &amp; \quad \Rightarrow \quad \frac{\sum_c TP_{c}}{\sum_c n_{c}}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(n_{c}\)</span> : number of samples belonging to class <span class="math inline">\(c\)</span><br>
<span class="math inline">\(TP_{c}\)</span> : number of correctly classified samples from class <span class="math inline">\(c\)</span></p>
<p><strong>Macro-averaged score (→ <code>balanced_accuracy_score</code>):</strong></p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{2} \left(\frac{TP}{TP+FN} + \frac{TN}{TN+FP} \right) = \frac{1}{2} \left(\frac{TP_\text{pos}}{n_\text{pos}} + \frac{TP_\text{neg}}{n_\text{neg}} \right) &amp; \quad \Rightarrow \quad \frac{1}{C} \sum_{c=1}^C \frac{TP_{c}}{n_{c}}
\end{aligned}
\]</span></p>
</section>
<section id="multi-class-problems-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="multi-class-problems-confusion-matrix">Multi-class problems: Confusion matrix</h4>
<p>Similarly, the table with the TP/FP/TN/FN entries can be extended for the multi-class classification case:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/confusion_matrix2.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>The heatmap on the left shows the (normalized) confusion matrix for a ten-class classification problem (recognizing handwritten digits), while the plot on the right shows example images for each case. Examining the confusion matrix and some individual examples can give us more faith in the predictions of our model, as we might realize that some misclassifications (highlighted in red) could also happen to a human, e.g., the 4 that was classified as a 1 or even the 4 that was classified as a 7 (which might even be a labeling error from when the dataset was originally created).</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="model-selection" class="level3">
<h3 class="anchored" data-anchor-id="model-selection">Model Selection</h3>
<p>After we’ve chosen an appropriate evaluation metric for our problem, we can use the resulting scores to automatically select the best hyperparameters for a model and ultimately the best model.</p>
<section id="the-case-for-an-additional-validation-set" class="level4">
<h4 class="anchored" data-anchor-id="the-case-for-an-additional-validation-set">The case for an additional validation set</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/validation_set.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>As we’ve established in the beginning, before experimenting with any models, the dataset should be split into a training and test set. However, this isn’t all: Since we are typically experimenting with many different types of models and for each model type with dozens of hyperparameter settings, we should not use this test set to evaluate each of these model candidates, since it might happen that with all these things we try out, we end up choosing a model that just by chance performs well on this test set, but does not generalize to new data later and we would have no way of finding this out before deploying the model in production. Therefore, we introduce a new data split, the <strong>validation set</strong>, that is used to evaluate the different candidate models, while the test set remains locked away until we’re ready to evaluate our final model to get a realistic estimate of how it performs on new data.</figcaption>
</figure>
</div>
<p>If the original dataset is quite big, say, over 100k samples (depending on the diversity of the data, e.g., the number of classes), then it is usually enough to just split the data into training, validation, and test sets at the start, where the validation and test sets contain about 10% of the data each and should be representative of the diversity of the original dataset. However, when the original dataset is smaller, it might not be possible to get such representative splits, which is when a technique called <em>cross-validation</em> (“x-val”) comes in handy.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Especially when working with small datasets, it is important that these splits are well balanced, i.e., that all classes are represented equally in the training, validation, and test sets. This is also called stratified sampling.</p>
</div>
</div>
</section>
<section id="cross-validation" class="level4 custom-gray-block">
<h4 class="anchored" data-anchor-id="cross-validation">Cross-Validation</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/cross_val.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>In a <strong><em>k</em>-fold cross-validation</strong>, the dataset is split into <em>k</em> parts, where each part is once the designated validation set, while the remaining <em>k-1</em> parts are used for training. This way, the model is trained and evaluated <em>k</em> times, each time on different splits. By computing the mean and standard deviation of the error metrics on all folds, we get a reliable estimate of the model’s generalization error and its variation due to the diversity of the dataset. The extreme case of the <em>k</em>-fold cross-validation is the <em>Leave-One-Out</em> cross-validation, where the model is always evaluated on only a single sample.</figcaption>
</figure>
</div>
</section>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Most datasets are collected over longer time periods. Often, the <strong>samples are correlated over time</strong>, i.e., samples collected around the same time are more similar to each other than samples collected weeks or months apart. This is often very apparent in time series data (e.g., seasonality effects in sales data), but it can also true for other types of data (e.g., the topics discussed in newspaper articles change over time; a camera lens might slowly accumulate dust). To get a realistic estimate of how well the model will perform on new data, it is usually best to <strong>use the most recent samples as the test set</strong>. Additionally, it might be necessary to use a <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">time series split</a> for the cross-validation, where the model is always trained on past data and evaluated on newer data. If there is a big difference between the model performance on random vs.&nbsp;chronological train/validation splits, this is a strong indication that the samples are correlated over time!</p>
</div>
</div>
<section id="hyperparameter-tuning" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h4>
<p>Often it is necessary to systematically evaluate a given model with different hyperparameter values to find the best settings. One straightforward approach for doing this is a <strong>grid search</strong>: In a grid search, we define the different values we want to test for each of the model’s hyperparameters and then all combinations of these different values for all hyperparameters are automatically evaluated, similar to how we would do it manually with nested for-loops. This is very useful, as often the different hyperparameter settings influence each other. Conveniently, sklearn furthermore combines this with a cross-validation. However, with many individual settings, this also comes at a computational cost, as the model is trained and evaluated <span class="math inline">\(k \times m_1 \times m_2 \times \dots \times m_i\)</span> times, where <span class="math inline">\(k\)</span> is the number of folds in the cross-validation and <span class="math inline">\(m_1...m_i\)</span> are the number of values that need to be tested for each of the <em>i</em> hyperparameters of the model.</p>
<p>For example, with two hyperparameters, the grid search results could look something like the plot below, which shows a heatmap of the average accuracy achieved with each hyperparameter combination of a model in the cross-validation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/06_evaluation/grid_search.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>While sklearn’s grid search method tells us directly what the best hyperparameter combination is out of the ones it tested (marked with a red star in the plot), it is important to check the complete set of results to verify that we have <strong>covered the whole range of possible hyperparameter values</strong> that could give good results. For example, in the plot above, we see a peak in the middle with the results getting worse to the sides, i.e., we know that better hyperparameter values are unlikely to lie outside of the range we’ve tested.<br>
It is generally a good idea to first start with a large range of values and then zoom in to the area that seems most promising. And of course knowledge about the different algorithms helps a lot in choosing reasonable settings as well.<br>
</p>
<p>Besides the basic grid search, there also exist other, more advanced hyperparameter tuning routines. For example, sklearn additionally implements a <strong>randomized search</strong>, and other dedicated libraries provide even fancier approaches, such as <a href="https://machinelearningmastery.com/what-is-bayesian-optimization/">Bayesian optimization</a>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, RandomizedSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>
</section>
</section>

</main> <!-- /main -->
<script type="text/javascript"> // go once over all images and transform the percentage width into pixels relative to original body size
  const bodyWidth = parseFloat(getComputedStyle(document.documentElement).getPropertyValue('--custom-body-width')); // Get CSS variable
  document.querySelectorAll('.quarto-figure img').forEach((img) => {
    if (img.getAttribute('style')) {
      const styleWidth = img.getAttribute('style').match(/width:\s*([\d.]+)%/); // Extract percentage
      if (styleWidth) {
        const percentage = parseFloat(styleWidth[1]) / 100;
        img.style.width = `${bodyWidth * percentage}px`; // Apply calculated width
      }
    }
  });
</script>

<style type="text/css">
#footer {
  font-size: 80%; /* Makes the font 70% smaller */
  border-top: 1px solid #ccc; /* Adds a horizontal line above the footer */
  padding-top: 10px; /* Adds some spacing above the footer content */
}
</style>

<!-- FOOTER  -->
<div id="footer" class="outer">
  <footer class="inner">
    <p><b>Book/Course Feedback: </b> <a href="https://forms.gle/Ccv5h5zQxwPjWtCS7" target="_blank" rel="nofollow">Full Feedback Survey</a> or <a href="https://forms.gle/qK8T5ALzgpiZaxd49" target="_blank" rel="nofollow">Short Comment</a><br></p>

    <p>Find me on <a href="https://github.com/cod3licious/" target="_blank" rel="nofollow">GitHub</a> and <a href="https://www.linkedin.com/in/franziska-horn/" target="_blank" rel="nofollow">LinkedIn</a><br>
      <a href="https://franziskahorn.de/">Home</a> ~ <a href="mailto:hey@franziskahorn.de?Subject=Book%20Feedback" target="_blank" rel="nofollow">Contact</a> ~ <a href="https://franziskahorn.de/impressum.html">Impressum</a></p>

      <!-- CC license -->
   <p xmlns:cc="http://creativecommons.org/ns#">This work is licensed under <a href="https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0<img style="height:16px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:16px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""></a></p>
  </footer>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03_unsupervised.html" class="pagination-link" aria-label="Unsupervised Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Unsupervised Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05_supervised_models.html" class="pagination-link" aria-label="Supervised Learning Models">
        <span class="nav-page-text"><span class="chapter-title">Supervised Learning Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>
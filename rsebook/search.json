[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clarity-Driven Development of Scientific Software",
    "section": "",
    "text": "Preface\nThis book is meant to empower researchers to code with confidence and clarity.\nIf you studied something other than computer science‚Äîespecially in the natural sciences like physics, chemistry, or biology‚Äîit‚Äôs likely you were never taught how to properly develop software. Yet, you‚Äôre often still expected to write code as part of your daily work. Maybe you‚Äôve taken a programming course like Python for Biologists and can put together functional scripts through trial and error (with a little help from an AI assistant). But chances are, no one ever showed you how to write well-structured, maintainable, and reusable code that could make your life‚Äîand collaborating with your colleagues‚Äîso much easier.\nThis book is for you if you want to:\n\nWrite functional software more quickly\nUse a structured approach to design better programs\nReuse your code in future projects\nFeel confident about what your scripts are doing\nPrepare your research code for production\nShare your work with pride.\n\nWhether you‚Äôre just beginning your scientific journey‚Äîperhaps working on your first major project like a master‚Äôs thesis or your first paper‚Äîor you‚Äôre contemplating a move from academia to industry, the practical advice in this book can guide you along the way. We will approach software design from first principles and tackle research questions with a product mindset. While the book contains some example code in Python to illustrate the concepts, the general ideas are independent of any programming language.\nSoftware development is a craft that‚Äôs best learned with the guidance of a senior colleague‚Äîsomeone who can show you the right tools and provide feedback through code reviews. Unfortunately, mentors with industry experience are rare in academia. While a book can‚Äôt replace an apprenticeship, I hope this one gives you a head start. It‚Äôs the book I wish I could have read at university and the one I always wanted to recommend to the students and junior developers I‚Äôve mentored.\nThis is still a draft version! Please write me an email, if you have any suggestions for how this book could be improved!\nEnjoy! üòä\n\nAcknowledgments\nI would like to thank Marcel Lengert and Sarah Nomura for their thoughtful feedback.\nThe texts in this book were partly edited and refined with the help of ChatGPT, however, all original content is my own.\n\n\nHow to Cite\n@book{horn2025cddsci,\n  author = {Horn, Franziska},\n  title = {Clarity-Driven Development of Scientific Software},\n  year = {2025},\n  url = {https://franziskahorn.de/rsebook/},\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1.html",
    "href": "part1.html",
    "title": "Gaining Clarity",
    "section": "",
    "text": "Before your start writing code, it is important to gain clarity on your concept and approach (Figure¬†1). Specifically, in this first part of the book we‚Äôll examine:\n\nThe outcome we want to achieve, i.e., why we develop software in the first place (1¬† Outcome: Why?).\nWhat outputs create the most value for our users (2¬† Output: What?).\nWhat code generates these outputs and how do we get from working code to good code (3¬† State & Flow: How?).\n\n\n\n\n\n\n\nFigure¬†1: A developer writes code, which is then executed to generate some output that is consumed by a user. Before you start writing this code, it is important to gain clarity on the why, what, and how of your solution.",
    "crumbs": [
      "Gaining Clarity"
    ]
  },
  {
    "objectID": "01_outcome_why.html",
    "href": "01_outcome_why.html",
    "title": "1¬† Outcome: Why?",
    "section": "",
    "text": "Why We Develop Software\nBefore writing your first line of code, it‚Äôs crucial to clearly understand what you‚Äôre trying to achieve‚Äîspecifically, the purpose of your research. This helps you focus on developing an innovative solution that creates real value by improving existing approaches or addressing unmet needs. Furthermore, this clarity will help you choose the most appropriate analysis methods to support your objectives and enable you to communicate your research effectively to ensure your audience understands the benefits of your work.\nWhile programming can be enjoyable in its own right, most people‚Äîand especially companies‚Äîaren‚Äôt willing to invest significant time or resources unless there‚Äôs a clear return. So why do we write code in the first place?\nWhen it comes to serious software development, the motivation usually boils down to profit and/or recognition.\nCompanies are usually looking to make a profit, which can be accomplished in one of two ways:\nAs an individual‚Äîespecially in research‚Äîyour primary goal is probably to get some recognition in your field. For instance:\nWith a bit of luck, that recognition could also lead to profit, such as landing a well-paid job based on a successful side project.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "01_outcome_why.html#why-we-develop-software",
    "href": "01_outcome_why.html#why-we-develop-software",
    "title": "1¬† Outcome: Why?",
    "section": "",
    "text": "Code vs.¬†Software Product\n\n\n\nPlease note that there is a difference between code and a software product:\n\nCode is simply text written in a programming language (like Python).\nA software product is code that actually runs‚Äîeither locally (on your laptop or phone) or remotely (as a web service in the cloud)‚Äîand produces an output that users interact with.\n\nSometimes that interaction with the program is the end goal (e.g., when playing a video game). Other times, the software is just a means to an end‚Äîlike a website you use to order clothes online or the script you run to generate your research results (Figure¬†1.1).\n\n\n\n\n\n\nFigure¬†1.1: To become valuable, code needs to be executed and produce some kind of output for a user. In this chapter we‚Äôll examine why we write this code in the first place, i.e., what outcome we‚Äôre trying to achieve.\n\n\n\n\n\n\n\n\nIncrease revenue: The company builds a software product or feature that users are willing to pay for, like a web application offered as software-as-a-service (SaaS) with a monthly subscription, or that results in customers spending more money, for example, because of better recommendations on an e-commerce platform.\nReduce costs: Alternatively, companies might build internal tools that automate tedious or repetitive tasks. By saving employee time, these tools reduce operational costs and indirectly increase profit.\n\n\n\nYou might write code to generate results that get your paper published in a respected journal and cited by others.\nOr you might create an open-source library that becomes popular (and receives a lot of stars on GitHub).\n\n\n\nYour Users and Their Priorities\nWhatever your motivation, success‚Äîwhether financial or reputational‚Äîonly comes if your software meets a real need. In other words, it must create value for your users.\nBefore developing a product, we therefore first need to understand our users and their priorities [1]. These considerations are not only relevant for software, but apply to all kinds of products‚Äîphysical or digital: a woodworking tool, an e-commerce website, or, in our case, your research results.\nAsk yourself:\n\nWho are your users?\nDepending on the product, your target user group might be broad or highly specific. For example, a general consumer product could be used by anyone over 14, while enterprise solutions may cater to niche audiences, such as professionals in a particular field. Even if your users could theoretically be ‚Äúeveryone,‚Äù picturing a more specific user can help refine your design. Trying to please everyone often results in satisfying no one. Focusing on a distinct user group can also help differentiate your product in the market.\nUser experience (UX) designers often create user personas‚Äîfictional but representative users based on real-world insights. These personas include details like age, profession, hobbies, and specific needs:\n\nWoodworking tool: ‚ÄúMary, the multitasking mom‚Äù‚Äîa part-time teacher who enjoys DIY projects and wants to build a bird house with her daughter.\nE-commerce website: ‚ÄúHarry, the practical shopper‚Äù‚Äîa 55-year-old lawyer who wants to buy a birthday gift for his partner.\nYour research results: ‚ÄúReviewer 2, the relentless critic‚Äù‚Äîa researcher from your field who works on similar research questions.\n\nWhat are their priorities?\nWhat is important to them? Why are they looking for alternatives to existing solutions?\n\nWoodworking tool: Mary is mainly interested in the weight and noise level‚Äîthe tool should be light enough for one-handed use and quiet enough to be used inside an apartment in the city.\nE-commerce website: Harry wants to complete his task quickly, so he values a clean, easy-to-navigate design and the ability to find suitable products with minimal effort.\nYour research results: When Reviewer 2 evaluates a novel model architecture, they mostly check its predictive accuracy and performance on large datasets.\n\n\n\nInnovative Solutions\nLearning about your users and their priorities can give you a clearer sense of where to focus your efforts. If current solutions fall short in the dimensions your users care most about, then you‚Äôve identified a meaningful gap‚Äîa real problem that‚Äôs worth solving.\nThe next step is to explore an innovative idea: a way to address this problem more effectively than existing alternatives, at least for this specific group of users. You may not yet know whether such a solution is technically feasible, but the gap itself at least justifies further exploration.\nIn the world of consumer products, an innovative solution often addresses unmet needs or improves a frustrating and inefficient user experience. In research, we also aim to advance the state of the art. That might mean filling a gap in knowledge, or developing a new method, material, or process with improved properties. In your area of expertise, you‚Äôre probably already aware of something that could be improved‚Äîwhere existing approaches fall short and where your idea might offer a better solution.\nThe next section outlines common research goals and the types of data analysis typically used to achieve them. This will help you sharpen your direction and understand what kinds of evidence you‚Äôll need to support your work.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "01_outcome_why.html#types-of-research-questions",
    "href": "01_outcome_why.html#types-of-research-questions",
    "title": "1¬† Outcome: Why?",
    "section": "Types of Research Questions",
    "text": "Types of Research Questions\nMost research questions can be categorized into four broad groups, each associated with a specific type of analytics approach (Figure¬†1.2).\n\n\n\n\n\n\nFigure¬†1.2: Descriptive, diagnostic, predictive, and prescriptive analytics, with increasing computational complexity and need to write custom code.\n\n\n\n\nDescriptive Analytics\nThis approach focuses on observing and describing phenomena to establish baseline measurements or track changes over time.\nExamples include:\n\nIdentifying animal and plant species in unexplored regions of the deep ocean.\nMeasuring the physical properties of a newly discovered material.\nSurveying the political views of the next generation of teenagers.\n\nMethodology:\n\nCollect a large amount of data (e.g., samples or observations).\nCalculate summary statistics like averages, ranges, or standard deviations.\n\n\n\nDiagnostic Analytics\nHere, the goal is to understand relationships between variables and uncover causal chains to explain why phenomena occur.\nExamples include:\n\nInvestigating how CO2 emissions from burning fossil fuels drive global warming.\nEvaluating whether a new drug reduces symptoms and under what conditions it works best.\nExploring how economic and social factors influence shifts toward right-wing political parties.\n\nMethodology:\n\nPerform exploratory data analysis, such as looking for correlations between variables.\nConduct statistical tests to support or refute hypotheses (e.g., comparing treatment and placebo groups).\nDesign of experiments to control for external factors (e.g., randomized clinical trials).\nBuild predictive models to simulate relationships. If the predictions from these models match new real-world observations, it suggests their assumptions correctly represent causal effects.\n\n\n\nPredictive Analytics\nThis method involves building models to describe and predict relationships between independent variables (inputs) and dependent variables (outputs). These models often rely on insights from diagnostic analytics, such as which variables to include in the model and how they might interact (e.g., linear or nonlinear dependence). Despite its name, this approach is not just about predicting the future, but used to estimate unknown values in general (e.g., variables that are difficult or expensive to measure). It also includes any kind of simulation model to describe a process virtually (i.e., to conduct in silico experiments).\nExamples include:\n\nWeather forecasting models.\nDigital twin of a wind turbine to simulate how much energy is generated under different conditions.\nPredicting protein folding based on amino acid sequences.\n\nMethodology:\nThe key difference lies in how much domain knowledge informs the model:\n\nWhite-box (mechanistic) models: Based entirely on known principles, such as physical laws or experimental findings. These models are often manually designed, with parameters fitted to match observed data.\nBlack-box (data-driven) models: Derived primarily from observational data. Researchers usually test different model types (e.g., neural networks or Gaussian processes) and choose the one with the highest prediction accuracy.\nGray-box (hybrid) models: These combine mechanistic and data-driven approaches. For example, the output of a mechanistic model may serve as an input to a data-driven model, or the data-driven model may predict residuals (i.e., prediction errors) from the mechanistic model, where both outputs combined yield the final prediction.\n\n\n\n\n\n\nResources to learn more about data-driven models\n\n\n\n\n\nIf you want to learn more about how to create data-driven models and the machine learning (ML) algorithms behind them, these two free online books are highly recommended:\n\n[4] Supervised Machine Learning for Science by Christoph Molnar & Timo Freiesleben; a fantastic introduction focused on applying black-box models in scientific research.\n[6] A Practitioner‚Äôs Guide to Machine Learning by me; a broader overview of ML methods for a variety of use cases.\n\n\n\n\n\nProvided the developed model is sufficiently accurate, researchers can then analyze its behavior (e.g., through a sensitivity analysis, which examines how outputs change with varying inputs) to gain further insights about the modeled system itself (to feed back into diagnostic analytics).\n\n\nPrescriptive Analytics\nThis approach focuses on decision-making and optimization, often using predictive models.\nExamples include:\n\nScreening thousands of drug candidates to find those most likely to bind with a target protein.\nOptimizing reactor conditions to maximize yield while minimizing energy consumption.\n\nMethodology:\n\nDecision support: Use models for ‚Äúwhat-if‚Äù analyses to predict outcomes of different scenarios. For example, models can estimate the effects of limiting global warming to 2¬∞C versus exceeding that threshold, thereby informing policy decisions.\nDecision automation: Use models in optimization loops to systematically test input conditions, evaluate outcomes (e.g., resulting predicted material quality), and identify the best conditions automatically.\n\n\n\n\n\n\nModel accuracy is crucial\n\n\n\nThese recommendations are only as good as the underlying models. Models must accurately capture causal relationships and often need to extrapolate beyond the data used to build them (e.g., for disaster simulations). Data-driven models are typically better at interpolation (predicting within known data ranges), so results should ideally be validated through additional experiments, such as testing the recommended new materials in the lab.\n\n\n\nTogether, these four types of analytics form a powerful toolkit for tackling real-world challenges: descriptive analytics provides a foundation for understanding, diagnostic analytics uncovers the causes behind observed phenomena, predictive analytics models future scenarios based on this understanding, and prescriptive analytics turns these insights into actionable solutions. Each step builds on the previous one, creating a systematic approach to answering complex questions and making informed decisions.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "01_outcome_why.html#draw-your-innovation",
    "href": "01_outcome_why.html#draw-your-innovation",
    "title": "1¬† Outcome: Why?",
    "section": "Draw Your Innovation",
    "text": "Draw Your Innovation\nWhether you‚Äôre collaborating with colleagues, presenting at a conference, or writing a paper‚Äîclearly communicating the problem you‚Äôre solving and your proposed solution is essential.\nVisual representations are particularly powerful for conveying complex ideas. One effective approach is creating ‚Äúbefore and after‚Äù visuals that contrast the current state of the field with your proposed improvements (Figure¬†1.3).\nThe ‚Äúbefore‚Äù scenario might show a lack of data, an incomplete understanding of a phenomenon, poor model performance, or an inefficient process or material. The ‚Äúafter‚Äù scenario highlights how your research addresses these issues and improves on the current state, such as refining a predictive model or enhancing the properties of a new material.\n\n\n\n\n\n\nFigure¬†1.3: Exemplary research goals and corresponding ‚Äúbefore and after‚Äù visuals for descriptive, diagnostic, predictive, and prescriptive analytics tasks.\n\n\n\nAt this point, your ‚Äúafter‚Äù scenario might be based on a hypothesis or an educated guess about what your results will look like‚Äîand that‚Äôs totally fine! The purpose of visualizing your solution is to guide your development process. Later, you can update the picture with actual results if you decide to include it in a journal publication, for example.\nOf course, not all ideas are tied directly to analytics. Sometimes the main improvement is more qualitative, for example, focusing on design or functionality (Figure¬†1.4).\n\n\n\n\n\n\nFigure¬†1.4: This example illustrates a task where a robot must reach its target (represented by money) as efficiently as possible. Original approach (left): The robot relied on information encoded in the environment as expected rewards. To determine the shortest path to the target, the robot required a large sensor (shown as the yellow circle) capable of scanning multiple nearby fields to locate the highest reward. New approach (right): Instead of relying on reward values scattered across the environment, the optimal direction is now encoded directly in the current field. This eliminates the need for large sensors, as the robot only needs to read the value of its current position, enabling it to operate with a much smaller sensor and thereby reducing hardware costs. Additional experiments still need to be demonstrate that with the new approach, the robot reaches its target at least as quickly as with the original approach.\n\n\n\nIt is also not always necessary to include a ‚Äúbefore‚Äù scenario, for example, if existing solutions are not directly comparable or so well known that they require no further explanation (Figure¬†1.5).\n\n\n\n\n\n\nFigure¬†1.5: Illustration of the approach used in the paper Automating the search for a patent‚Äôs prior art with a full text similarity search [5] (where the alternative would be an ordinary manual keyword search).\n\n\n\nOf course, the examples shown here are already refined for publication‚Äîyour initial sketches will probably look a bit messier (Figure¬†1.6). Have a look at [3] for some tips on communicating science through visualizations and creating insightful graphics.\n\n\n\n\n\n\nFigure¬†1.6: One of several sketches and the resulting final figure that was included in my PhD thesis [7] (showing the SchNet neural network architecture).\n\n\n\nGive it a try‚Äîdoes the sketch help you explain your research to your family?",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "01_outcome_why.html#evaluation-metrics",
    "href": "01_outcome_why.html#evaluation-metrics",
    "title": "1¬† Outcome: Why?",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nTo demonstrate the impact of your work and compare your solution against existing approaches, it‚Äôs crucial to define what success looks like quantitatively. Consider these common evaluation metrics to measure the outcome of your research and generate compelling results:\n\nNumber of samples: This refers to the amount of data you‚Äôve collected, such as whether you surveyed 100 or 10,000 people. Larger sample sizes can provide more robust and reliable results. It is also important to make sure your sample is representative of the population as a whole, i.e., to avoid sampling bias, which can cause misleading results and incorrect conclusions.\nReliability of measurements: This evaluates the consistency of your data. For example, how much variation occurs if you repeat the same measurement, e.g., run a simulation with different random seeds. This is important as others need to be able to reproduce your results.\nStatistical significance: The outcome of a statistical hypothesis test, such as a p-value that indicates whether the difference in symptom reduction between the treatment and placebo groups is significant.\nModel accuracy: For predictive models, this includes:\n\nStandard metrics like \\(R^2\\) to measure how closely the model‚Äôs predictions align with observational data.\nCross-validation scores to assess performance on new data.\nUncertainty estimates to understand how confident the model is in its predictions.\n\nAlgorithm performance: This includes metrics like memory usage and the time required to fit a model or make predictions, and how these values change as the dataset size increases. Efficient algorithms are crucial when scaling to large datasets or handling complex simulations.\nKey Performance Indicators (KPIs): Any other practical measures that matter in your field. For example:\n\nFor a chemical process: yield, purity, energy efficiency.\nFor a new material: strength, durability, cost.\nFor an optimization task: convergence time, solution quality.\n\n\nYour evaluation typically involves multiple metrics. For example, in prescriptive analytics, you need to demonstrate both the accuracy of your model and that the recommendations generated with it led to a genuinely optimized process or product. Before starting your research, review similar work in your field to understand which metrics are standard in your community (i.e., what Reviewer 2 cares about).\nIdeally, you should already have an idea of how existing solutions perform on your chosen metrics (e.g., based on findings from other publications) to establish the baseline your solution should outperform. You‚Äôll likely need to replicate at least some of these baseline results (e.g., by reimplementing existing models) to ensure your comparisons are not influenced by external factors. But understanding where the ‚Äúcompetition‚Äù stands can also help you identify secondary metrics where your solution could excel. For example, even if there‚Äôs little room to improve model accuracy, existing solutions might be too slow to handle large datasets efficiently (Figure¬†1.7).1\n\n\n\n\n\n\nFigure¬†1.7: ‚ÄúChart your competitive position‚Äù [1]: The metrics we‚Äôre interested in often represent trade-offs. For example, we want a high quality product, but it should also be cheap. Or a good model accuracy, but at the same time not use excessive compute resources. Your approach might not outperform existing baselines on all metrics, but its trade-off could still be preferable.\n\n\n\nThese results are central to your research (and publications), and much of your code will be devoted to generating them, along with the models and simulations behind them. Clearly defining the key metrics needed to demonstrate your research‚Äôs impact will help you focus your programming efforts effectively.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe problem you‚Äôre trying to solve.\nExisting solutions to this problem, i.e., the baseline you‚Äôre competing against.\nWhich metrics should be used to quantify your improvement on the current state.\n\n\n\n\n\n\n\n[1] Aulet B. Disciplined Entrepreneurship: 24 Steps to a Successful Startup, Expanded & Updated. John Wiley & Sons (2024).\n\n\n[2] Callaway E. Chemistry Nobel Goes to Developers of AlphaFold AI That Predicts Protein Structures. Nature 634(8034), 525‚Äì526 (2024).\n\n\n[3] Christiansen J. Building Science Graphics: An Illustrated Guide to Communicating Science Through Diagrams and Visualizations. AK Peters/CRC Press (2022).\n\n\n[4] Freiesleben T, Molnar C. Supervised Machine Learning for Science: How to Stop Worrying and Love Your Black Box. (2024).\n\n\n[5] Helmers L, Horn F, Biegler F, Oppermann T, M√ºller K-R. Automating the Search for a Patent‚Äôs Prior Art with a Full Text Similarity Search. PLoS ONE 14(3), e0212103 (2019).\n\n\n[6] Horn F. A Practitioner‚Äôs Guide to Machine Learning. (2021).\n\n\n[7] Horn F. Similarity Encoder: A Neural Network Architecture for Learning Similarity Preserving Embeddings. Technische Universitaet Berlin (Germany) (2020).",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "01_outcome_why.html#footnotes",
    "href": "01_outcome_why.html#footnotes",
    "title": "1¬† Outcome: Why?",
    "section": "",
    "text": "For example, currently, a lot of research aims to replace traditional mechanistic models with data-driven machine learning models, as these enable significantly faster simulations. A notable example is the AlphaFold model, which predicts protein folding from amino acid sequences‚Äîa breakthrough so impactful it was recognized with a Nobel Prize [2]!‚Ü©Ô∏é",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Outcome: Why?</span>"
    ]
  },
  {
    "objectID": "02_output_what.html",
    "href": "02_output_what.html",
    "title": "2¬† Output: What?",
    "section": "",
    "text": "UX Design\nIn the previous chapter, we‚Äôve gained clarity on the problem you‚Äôre trying to solve and how to quantify the improvements your research generates. Now it‚Äôs time to dive deeper into what these results might actually look like and the data on which they are built. This requires us to understand what kind of output our code should create in order to be useful for our users (Figure¬†2.1), for example, what kind of plots would help your audience to understand the benefits of your approach.\nBased on what you know about your users and their priorities, you can now design the user experience (UX) of the product such that it will (hopefully) be valuable for your users, easily understandable, and enjoyable to use. Use sketches, wireframes, or prototypes to make your idea tangible:\nThis is an iterative process‚Äîyou refine your design until you‚Äôre reasonably confident it satisfies your goal.\nTry to empathize with the user‚Äôs experience: How will they interact with the product? Where and for how long will they use the product? What constraints do these conditions introduce? For example, the e-commerce website might be viewed on a smartphone with a large font size, so the page must be optimized for mobile browsing and still look good with an increased text size. Or if your plots are included in a journal paper, they might be printed out in black and white and you have to ensure that the color-coding is still legible in this case.\nTesting your design with potential users is crucial‚Äîis this what they need? Observe where they struggle: Do your colleagues correctly interpret the message you want to convey with your plots? Your design idea is essentially a bet, and before investing significant resources to implement it, you should validate whether your assumptions are correct and this design is valuable and usable.\nIf possible, you should additionally test your product with real users as you build it. While physical products are hard to modify after manufacturing, software is flexible‚Äîso take advantage of that. Keep iterating to better meet user needs, both now and in the future.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Output: What?</span>"
    ]
  },
  {
    "objectID": "02_output_what.html#ux-design",
    "href": "02_output_what.html#ux-design",
    "title": "2¬† Output: What?",
    "section": "",
    "text": "Woodworking tool: Drawings, material specifications, and physical prototypes of a lightweight, quiet tool that an average woman can use with one hand.\nE-commerce website: Wireframes and mockups of a website with a modern interface that helps users quickly find and purchase products (Figure¬†2.2).\n\n\n\n\n\n\nFigure¬†2.2: A simple mockup illustrating the user experience flow on an e-commerce website, from opening the page and looking at a product to purchasing it.\n\n\n\nYour research results: Sketches of the plots you want to generate to highlight the advantages of your approach (which we‚Äôll discuss in more detail in Section 2.3).",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Output: What?</span>"
    ]
  },
  {
    "objectID": "02_output_what.html#data-analysis-results",
    "href": "02_output_what.html#data-analysis-results",
    "title": "2¬† Output: What?",
    "section": "Data Analysis Results",
    "text": "Data Analysis Results\nNo matter how much programming your project requires, nearly all scientific work involves analyzing your data to create result plots for publications or presentations.\nWhen analyzing data, the process is typically divided into two phases:\n\nExploratory Analysis: This involves generating a variety of plots to gain a deeper understanding of your data, such as identifying correlations between variables. It‚Äôs often a quick and dirty process to help you familiarize yourself with the dataset.\nExplanatory Analysis: This focuses on creating refined, polished plots intended for communicating your findings to others, such as in a publication or presentation. These visuals are designed to clearly convey your results to an audience that may not be familiar with your data.\n\nBut before we dive into how to conduct exploratory and explanatory analyses, let‚Äôs first take a quick look at what data actually is.\n\nData Types\nIn one form or another, you‚Äôre research will rely on data, both collected or generated by yourself and possibly others.\n\nStructured vs.¬†Unstructured Data\nData can take many forms, but one key distinction is between structured and unstructured data (Figure¬†2.3).\n\n\n\n\n\n\nFigure¬†2.3: Structured and unstructured data.\n\n\n\nStructured data is organized in rows and columns, like in Excel spreadsheets, CSV files, or relational databases. Each row represents a sample or observation (a data point), while each column corresponds to a variable or measurement (e.g., temperature, pressure, household income, number of children).\nUnstructured data, in contrast, lacks a predefined structure. Examples include images, text, audio recordings, and videos, typically stored as separate files on a computer or in the cloud. While these files might include structured metadata (e.g., timestamps, camera settings), the data content itself can vary widely‚Äîfor instance, audio recordings can range from seconds to hours in length.\nStructured data is often heterogeneous, meaning it includes variables representing different kinds of information with distinct units or scales (e.g., temperature in ¬∞C and pressure in kPa). Unstructured data tends to be homogeneous; for example, there‚Äôs no inherent difference between one pixel and the next in an image.\n\n\n\n\n\n\nThis book focuses on structured data\n\n\n\nEven though unstructured data is common in science (e.g., microscopy images), for simplicity, this book focuses on structured data. Furthermore, for now we‚Äôll assume that your data is stored in an Excel or CSV file, i.e., a spreadsheet with rows (samples) and columns (variables), on your computer. Later in Chapter 6, we‚Äôll discuss more advanced options for storing and accessing data, such as databases and APIs.\n\n\n\n\nProgramming Data Types\nEach variable in your dataset (i.e., each column in your spreadsheet) is represented as a specific data type, such as:\n\nNumbers (integers for whole numbers or floats for decimals)\nStrings (text)\nBoolean values (true/false)\n\nIn programming, these are so-called primitive data types (as opposed to composite types, like lists or dictionaries containing multiple values, or user-defined objects) and define how information is stored in computer memory.\n\n\n\n\n\n\nData types in Python\n\n\n\n\n\n# integer\ni = 42\n# float\nx = 4.1083\n# string\ns = \"hello world!\"\n# boolean\nb = False\n\n\n\n\n\nStatistical Data Types\nEven more important than how your data is stored, is understanding what your data means. Variables fall into two main categories:\n\nContinuous (numerical) variables represent measurable values (e.g., temperature, height). These are usually stored as floats or integers.\nDiscrete (categorical) variables represent distinct options or groups (e.g., nationality, product type). These are often stored as strings, booleans, or sometimes integers.\n\n\n\n\n\n\n\nMisleading data types\n\n\n\nBe cautious: a variable that looks numerical (e.g., 1, 2, 3) may actually represent categories. For example, a material_type column with values 1, 2, and 3 might correspond to aluminum, copper, and steel, respectively. In this case, the numbers are IDs, not quantities.\n\n\nRecognizing whether a variable is continuous or discrete is crucial for creating meaningful visualizations and using appropriate statistical models.\n\n\nTime Series Data\nAnother consideration is whether your data points are linked by time. Time series data often refers to numerical data collected over time, like temperature readings or sales numbers. These datasets are usually expected to exhibit seasonal patterns or trends over time.\nHowever, nearly all datasets involve some element of time. For example, if your dataset consists of photos, timestamps might seem unimportant, but they could reveal trends‚Äîlike changes in image quality due to new equipment.\n\n\n\n\n\n\nAlways record timestamps\n\n\n\nAlways include timestamps in your data or metadata to help identify potential correlations or unexpected trends over time.\n\n\nSometimes, you may be able to collect truly time-independent data (e.g., sending a survey to 1,000 people simultaneously and they all answer within the next 10 minutes). But usually, your data collection will take longer and external factors‚Äîlike an election during a longer survey period‚Äîmight unintentionally affect your results. By tracking time, you can assess and adjust for such influences.\n\n\n\nExploratory Analysis\nIn this initial analysis, the goal is to get acquainted with the data, check if the trends and relationships you anticipated are present, and uncover any unexpected patterns or insights.\n\nExamine the raw data:\n\nIs the dataset complete, i.e., does it contain all the variables and samples you expected?\n\nExamine summary statistics (e.g., mean, standard deviation (std), min/max values, missing value count, etc.):\n\nWhat does each variable mean? Given your understanding of the variable, are its values in a reasonable range?\nAre missing values encoded as NaN (Not a Number) or as ‚Äòunrealistic‚Äô numeric values (e.g., -1 while normal values are between 0 and 100)?\nAre missing values random or systematic (e.g., specific measurements might only be collected under certain conditions; in a survey rich people are less likely to answer questions about their income)? This can influence how missing values should be handled, e.g., whether it makes sense to impute them with the mean or some other specific value (e.g., zero).\n\nExamine the distributions of individual (continuous) variables:\n\n\n\nHistogram, strip plot, violin plot, box plot, and summary statistics of the same values.\n\n\n\nAre there any outliers? Are these genuine edge cases or can they be ignored (e.g., due to measurement errors or wrongly encoded data)?\nIs the data normally distributed or does the plot show multiple peaks? Is this expected?\n\nExamine trends over time (by plotting variables over time, even if you don‚Äôt think your data has a meaningful time component, e.g., by lining up representative images according to their timestamps to see if there is a pattern):\n\n\n\nWhat caused these trends and what are their implications for the future? This plot shows fictitious data of the pressure in a pipe affected by fouling‚Äîthat is, a buildup of unwanted material on the pipe‚Äôs surface, leading to increased pressure. The pipe is cleaned at regular intervals, causing a drop in pressure. However, because the cleaning process is imperfect, the baseline pressure gradually shifts upward over time.\n\n\n\nAre there time periods where the data was sampled irregularly or samples are missing? Why?\nAre there any (gradual or sudden) data drifts over time? Are these genuine changes (e.g., due to changes in the raw materials used in the process) or artifacts (e.g., due to a malfunctioning sensor recording wrong values)?\n\nExamine relationships between two variables:\n\n\n\nDepending on the variables‚Äô types (continuous or discrete), relationships can be shown in scatter plots, box plots, or a table. Please note that not all interesting relations between the two variables can be detected through a high correlation coefficient, so you should always check the scatter plot for details.\n\n\n\nAre the observed correlations between variables expected?\n\nExamine patterns in multidimensional data (using a parallel coordinate plot):\n\n\n\nEach line in a parallel coordinate plot represents one data point, with the corresponding values for the different variables marked at the respective y-axis. The screenshot here shows an interactive plot created using the Python plotly library. By selecting value ranges for the different dimensions (indicated by the pink stripes), it is possible to spot interesting patterns resulting from a combination of values across multiple variables.\n\n\n\nDo the observed patterns in the data match your understanding of the problem and dataset?\n\n\n\n\nExplanatory Analysis\nMost of the plots you create during an exploratory analysis are likely for your eyes only. Any plots you do choose to share with a broader audience‚Äîsuch as in a paper or presentation‚Äîshould be refined to clearly communicate your findings. Since your audience is much less familiar with the data and likely lacks the time or interest to explore it in depth, it‚Äôs essential to make your results more accessible. This process is often referred to as explanatory analysis [1].\n\n\n\n\n\n\nDon‚Äôt force an exploratory analysis onto your audience\n\n\n\nDon‚Äôt ‚Äújust show all the data‚Äù and hope that your audience will make something of it‚Äîunderstand what they need to answer the questions they have.\n\n\nWhen choosing and designing your plot, keep the user experience in mind. Ask yourself: Does this visualization clearly convey the results? Is it easy to understand and interpret? Use the following steps to help you evaluate and improve your plot design.\n\nStep 1: Choose the right plot type\n\nGet inspired by visualization libraries (e.g., here or here), but avoid the urge to create fancy graphics; sticking with common visualizations makes it easier for the audience to correctly decode the presented information.\nDon‚Äôt use 3D effects!\nAvoid pie or donut charts (angles are hard to interpret).\nUse line plots for time series data.\nUse horizontal instead of vertical bar charts for audiences that read left to right.\nStart the y-axis at 0 for area & bar charts.\nConsider using small multiples or sparklines instead of cramming too much into a single chart.\n\n\n\n\nLeft: Bar charts (especially in 3D) make it hard to compare numbers over a longer period of time. Right: Trends over time can be more easily detected in line charts. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 2: Cut clutter / maximize data-to-ink ratio\n\nRemove border.\nRemove gridlines.\nRemove data markers.\nClean up axis labels.\nLabel data directly.\n\n\n\n\nCut clutter! [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 3: Focus attention\n\nStart with gray, i.e., push everything in the background.\nUse pre-attentive attributes like color strategically to highlight what‚Äôs most important.\nUse data labels sparingly.\n\n\n\n\nStart with gray and use pre-attentive attributes strategically to focus the audience‚Äôs attention. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 4: Make data accessible\n\nAdd context: Which values are good (goal state), which are bad (alert threshold)? Should the value be compared to another variable (e.g., actual vs.¬†forecast)?\nLeverage consistent colors when information is spread across multiple plots (e.g., data from a certain country is always shown in the same color).\nAnnotate the plot with text explaining the main takeaways. If this is not possible, e.g., in interactive dashboards where the data keeps changing, the title can instead include the question that the plot should answer (e.g., ‚ÄúIs the material quality on target?‚Äù).\n\n\n\n\nTell a story. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Output: What?</span>"
    ]
  },
  {
    "objectID": "02_output_what.html#sec-draw-results",
    "href": "02_output_what.html#sec-draw-results",
    "title": "2¬† Output: What?",
    "section": "Draw Your Results",
    "text": "Draw Your Results\nYou may not have looked at your data yet‚Äîor maybe you haven‚Äôt even collected it‚Äîbut it‚Äôs important to start with the end in mind.\nIn software development, a UX designer typically creates mockups of a user interface (like the screens of a mobile app) before developers begin coding so that everyone has a clear understanding of the desired end result. Similarly, in our case, we want to start with a clear picture of what the output of our program should look like.1 The difference is that, instead of users interacting with the software themselves, they‚Äôll only see the plots or tables that your program generated, maybe in a journal article.\nBased on your takeaways from the previous chapter‚Äîabout the problem you‚Äôre solving and the metrics you should use to evaluate your solution‚Äîtry sketching what your final results might look like. Put yourself in your audience‚Äôs shoes and consider what they need to address their questions and concerns. Ask yourself: What figures or tables would best communicate the advantages of my approach?2\nDepending on your research goal, your results might be as simple as a single number, such as a p-value or the total number of people surveyed. However, if you‚Äôre reading this, you‚Äôre likely tackling something that requires a more complex analysis. For example, you might compare your model‚Äôs overall performance to several baseline approaches or illustrate how your solution converges over time (Figure¬†2.4).\n\n\n\n\n\n\nFigure¬†2.4: Exemplary envisioned results: The plots show the outcome of a multi-agent simulation, where ‚Äòmy approach‚Äô clearly outperforms two baseline methods. In this simulation, a group of agents is tasked with locating a food source in the environment and transporting the food back to their home base piece by piece. The ideal algorithm identifies the shortest path to the food source quickly to maximize food collection. Each algorithmic approach is tested 10 times using different random seeds to evaluate reliability. The plots display the mean and standard deviations across these runs. Left: How quickly each algorithm converges to the shortest path (resulting in the highest number of agents delivering food back to the home base per step). Right: Cumulative food collected by the end of the simulation.3\n\n\n\nIt‚Äôs important to remember that your actual results might look very different from your initial sketches‚Äîthey might even show that your solution performs worse than the baseline. This is completely normal. The scientific method is inherently iterative, and unexpected results are often a stepping stone to deeper understanding. By starting with a clear plan, you can generate results more efficiently and quickly pivot to a new hypothesis if needed. When your results deviate from your expectations, analyzing those differences can sharpen your intuition about the data and help you form better hypotheses in the future.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe specific results (tables and figures) you want to create to show how your solution outperforms existing approaches (e.g., in terms of accuracy, speed, etc.). \n\n\n\n\n\n\n\n[1] Knaflic CN. Storytelling with Data: A Data Visualization Guide for Business Professionals. John Wiley & Sons (2015).",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Output: What?</span>"
    ]
  },
  {
    "objectID": "02_output_what.html#footnotes",
    "href": "02_output_what.html#footnotes",
    "title": "2¬† Output: What?",
    "section": "",
    "text": "A former master‚Äôs student that I mentored humorously called this approach ‚Äúplot-driven development,‚Äù a nod to test-driven development (TDD) in software engineering, where you write a test for your function first and then implement the function to pass the test. Plot-driven development later turned into clarity-driven development, but the idea behind it stayed the same: understand what output your software should create before writing the code to make it happen.‚Ü©Ô∏é\nIf you‚Äôre already drafting a paper or presentation, you could even use these sketches of your results as placeholders to make sure they are advancing the story you‚Äôre trying to tell.‚Ü©Ô∏é\nThese plots were generated with Python using matplotlib‚Äôs plt.xkcd() setting and the xkcd script font. A pen and paper sketch will be sufficient for your case.‚Ü©Ô∏é",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Output: What?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html",
    "href": "03_state_flow_how.html",
    "title": "3¬† State & Flow: How?",
    "section": "",
    "text": "Step by Step: From Output to Input\nNow that you know what outputs you want to create, are you itching to start programming? Hold on for a moment!\nOne of the most common missteps I‚Äôve seen junior developers take is jumping straight into coding without first thinking through what they actually want to build. Imagine trying to construct a house by just laying bricks without consulting an architect first‚Äîhalfway through you‚Äôd probably realize the walls don‚Äôt align, and you forgot the plumbing for the kitchen. You‚Äôd have to tear it down and start over! To avoid this fate for your software, it‚Äôs essential to make a plan and sketch out the final design first (Figure¬†3.1).\nYour software design doesn‚Äôt have to be perfect‚Äîwe don‚Äôt want to overengineer our solution, especially since many details only become clear once we start coding and see how users interact with the software. But the more thought you put into planning, the smoother and faster execution will be.\nUnlike a house, where your design will be quite literally set in stone, code should be designed with flexibility in mind. While expanding a house to add extra rooms for a growing family‚Äîand then removing them again when downsizing for retirement‚Äîwould be costly and difficult, this kind of adaptability is exactly what we strive for in software. Our goal is to create code that can evolve with changing requirements.\nTo make sure your designs will be worthy of implementation, this chapter also introduces key paradigms and best practices that will help you create clean, maintainable code that‚Äôs easy to extend and reuse in future projects.\nOnce we know the outputs we want (e.g., result plots), the next step is identifying the sequence of instructions (flow) and the data (state) required to generate them.\nAt its core, programming is about transforming inputs into outputs. To determine the process for obtaining the desired results, we can work backward to figure out what data we need to create them (Figure¬†3.2). This is especially important when generating data yourself, such as through simulations. For example, if you want to plot how values change over time, you‚Äôll need to record variables at every time step‚Äînot just the final outcome of a simulation (duh!).",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html#step-by-step-from-output-to-input",
    "href": "03_state_flow_how.html#step-by-step-from-output-to-input",
    "title": "3¬† State & Flow: How?",
    "section": "",
    "text": "Figure¬†3.2: Work backward from the desired results to determine what data is needed to create them.\n\n\n\n\nBreaking Down the Steps\nLet‚Äôs map out the steps required to create the above scatter plot displaying actual values \\(y\\) (y), model predictions \\(\\hat{y}\\) (y_pred), and the \\(R^2\\) value (R2) in the title to indicate the model‚Äôs goodness of fit:\n\nTo create the plot, we need R2, y, and y_pred.\nplot_results(R2, y, y_pred)\nR2 can be computed from the values stored in y and y_pred.\nR2 = compute_r2(y, y_pred)\ny can be loaded from a file containing test data.\ny = ...\ny_pred must be estimated using our model, which requires:\n\nThe corresponding input values (x1 ‚Ä¶ x5) from the test data file.\nA trained model that can make predictions.\n\nX_test = ...\ny_pred = model.predict(X_test)\nTo obtain a trained model, we need to:\n\nCreate an instance of the model with the correct configuration.\nLoad the training data.\nTrain the model on the training data.\n\nmodel = MyModel(config)\nX_train, y_train = ...\nmodel.fit(X_train, y_train)\nThe model configuration needs to be provided by the user when running the script.\nconfig = ...\n\nOf course, in the actual implementation, each step will require further details (e.g., the formula for computing \\(R^2\\) or how to load the training and test datasets). But since we know that following this sequence in reverse order will produce the desired results, this already provides us with the rough outline of our code (see Section 3.4 for how these steps could come together in the final script).\n\n\n\n\n\n\nOptimize the ordering of steps\n\n\n\nSome steps depend on others (e.g., you must fit your model before making predictions), but others can be performed in any order. Optimizing the sequence can improve performance and efficiency.\nFor example, if you‚Äôre baking bread, you wouldn‚Äôt preheat the oven hours before the dough has risen‚Äîthat would waste energy. Similarly, when processing a large dataset, where you need to perform an expensive computation on each item but only a specific subset of these items is included in the final results, then it may be more efficient to filter the items first so you only compute values for the necessary items:\n\n\n\n\n\n\n\n\n\nDefine Intermediate Data Structures\nWhen transforming input data into desired outputs, it‚Äôs often necessary to persist (i.e., save) intermediate results (Figure¬†3.3).\n\n\n\n\n\n\nFigure¬†3.3: The script to create the result plot in a nutshell: At the top you see the output that is presented to the user in the user interface (UI) as the code is being executed. For the most part, this just consists of some log messages displayed in the terminal to show the progress of the script and then the final plot. Below this is the high level control flow, i.e., the sequence of instructions that we outlined in the previous section as the steps to get from input to output. While these instructions are executed, different variables (like model and y_pred) hold the current state of the program in memory. Some of the values stored in these variables are read from external files, possibly created by a different program. In case your script crashes in between, it is helpful to write intermediate results to disk (i.e., persist the current state of your variables). You could then load these files again to recreate the program‚Äôs previous state (indicated by the light gray arrows) and execute the remaining steps.\n\n\n\nEspecially when different scripts or processes create and consume these intermediate result files, they should be stored in a format that makes sense for both sides. For example:\n\nA user fills out a form on a website. Their information is stored in a database so it can later be retrieved and displayed on their account page.\nYou run simulations that output data to CSV files. These files are then used by a separate script to generate plots.\nYou define model settings in a config.yml file, which your script reads to initialize the model with the correct configuration.\n\nIt‚Äôs important to carefully design the structure of this intermediate data‚Äîwhat fields it includes, what they‚Äôre called, and how they‚Äôre organized. Because if you later need to change the format, you‚Äôll have to update both the producer and consumer processes, and potentially migrate existing data.\nTo design an effective structure, start by identifying the fields required by the downstream process. Then consider whether other processes might use this data and if they might need additional fields. Don‚Äôt overengineer it‚Äîwe‚Äôre not trying to future-proof against every possible scenario. But it‚Äôs worth considering whether to store data at a finer level of detail than what‚Äôs currently needed. It‚Äôs much easier to extract or summarize detailed data later than to reverse-engineer missing details from aggregate values.\n\n\n\n\n\n\nPlan your experiments\n\n\n\nIf your code does more than generate plots‚Äîfor example, if you‚Äôre running a simulation‚Äîyou also need to plan your experiment runs. Ideally, your code should allow you to run the same script with different parameter configurations, making it easy to test different setups.\nTo determine all possible configuration variants for your experiments, consider:\n\nThe models or algorithms you want to compare (your approach and relevant baselines).\nThe hyperparameter settings you want to test for each model.\nThe datasets or simulation environments on which you‚Äôll evaluate your models.\nIf your setup includes randomness (e.g., model initialization or simulation stochasticity), how many different random seeds you‚Äôll use to estimate variability.\n\nThink about the data each experiment run will generate and how you‚Äôll process it later. For instance, you might need a separate script that loads data from multiple runs to create additional plots comparing the performance of multiple models. This is simplified by using a clear naming convention for all output files, such as:\n{dataset}_{model}_{model_settings}_{seed}.csv\nIf your experiments will take several days, check whether you can run them on a compute cluster, submitting each experiment as a separate job to run in parallel.\n\n\n\n\nDesigning an Algorithm\nSome problems require more creativity to come up with an efficient algorithm that produces the correct output from a given input. Take the traveling salesman problem, where the goal is to find the shortest possible route through a given set of cities. Designing an efficient solution often involves choosing the right data structures (e.g., representing cities as nodes in a graph), using heuristics, and breaking the problem down into simpler subproblems (divide & conquer strategy). If you‚Äôre working on a novel problem, studying algorithm design can be invaluable (one of my favorite books on the topic is The Algorithm Design Manual by Steven Skiena [6]).\nHowever, for many common tasks, you can leverage existing algorithms instead of reinventing the wheel. For the rest of this book, we‚Äôll assume you already have a general idea of the steps your code needs to perform and focus on how to implement them effectively.\nWhile our list of steps enables us to create working code, unfortunately, this is not the same as good code. A script with a long sequence of instructions can be difficult to read, understand, and maintain. Since you‚Äôll likely be working with the same code for a while‚Äîand may even want to reuse parts of it in future projects‚Äîit‚Äôs worth investing in better design. Let‚Äôs explore how to make that happen.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html#analogy-the-cupcake-recipe",
    "href": "03_state_flow_how.html#analogy-the-cupcake-recipe",
    "title": "3¬† State & Flow: How?",
    "section": "Analogy: The Cupcake Recipe",
    "text": "Analogy: The Cupcake Recipe\nBefore diving into how to write good code, let‚Äôs explore the basic principles using a more relatable example: a cupcake recipe (Figure¬†3.4). Because who wants result plots when you could have cupcakes?! üßÅ\n\n\n\n\n\n\nFigure¬†3.4: A chef writes a recipe, which then needs to be executed (baked) to generate the output (cupcakes) that the consumers enjoy.\n\n\n\nLike code, a recipe is just text that describes a sequence of steps to achieve a goal. What you ultimately want are delicious cupcakes (your result plots). The recipe (your script) details how to transform raw ingredients (input) into the baked goods (output). But the steps don‚Äôt mean anything unless you actually execute them‚Äîyou have to bake the cupcakes (run python script.py) to get results.\nSo, how can we write a good recipe?\n\nBreaking Down the Steps\nTo start, we brainstorm all the necessary steps for making cupcakes (Figure¬†3.5). For this, we can again work backward from the final result (cupcakes) through the intermediate steps (cake batter and frosting) until we reach the raw ingredients.\n\n\n\n\n\n\nFigure¬†3.5: Everything you need to make cupcakes. The unstructured brain dump we‚Äôll transform into a proper recipe.\n\n\n\nYou‚Äôll notice that your list includes both ingredients (in code: variables containing data) and instructions for transforming those ingredients, like melting butter or mixing multiple ingredients (in code: the control flow, i.e., statements, including conditionals (if/else) and loops (for, while), that create intermediate variables). These steps also need to follow a specific order‚Äîyou wouldn‚Äôt frost a cupcake before baking it! Often, steps depend on one another, requiring a structured sequence (Figure¬†3.6).\n\n\n\n\n\n\nFigure¬†3.6: A recipe usually includes a list of ingredients followed by instructions on what to do with them. The order matters, especially when one step depends on the completion of another.\n\n\n\n\n\nAvoiding Repetition: Reusable Steps\nIf your recipe is part of a cookbook with multiple related recipes, some steps might apply to several of them. Instead of repeating those steps in every recipe, you can group and document them in a separate section (in code: define a function) and reference them when needed (Figure¬†3.7).\nThis follows the Don‚Äôt Repeat Yourself (DRY) principle [7]. Not only does this reduce redundancy, but it also makes updates easier‚Äîif a general step changes (e.g., you refine a technique or fix a typo), you only need to update it in one place (a single source of truth).\n\n\n\n\n\n\nFigure¬†3.7: Some instructions might be relevant in multiple recipes (e.g., general baking tips); instead of repeating them in every recipe, you can just refer to the page where they are described.\n\n\n\n\n\nOrganizing by Components\nLooking at your recipe, you might notice that some ingredients and instructions naturally group into self-contained components (in code: classes). Structuring the recipe this way makes it clearer and easier to follow (Figure¬†3.8). Instead of juggling all the steps at once, you can focus on creating one component at a time‚Äîfirst the plain cupcake, then the frosting, then assembling everything.\nThis modular approach also allows delegation‚Äîfor example, you could buy pre-made cupcakes and just focus on making the frosting. In programming, this means that the final code doesn‚Äôt need to know how each component was made‚Äîit only cares that the components exist.\n\n\n\n\n\n\nFigure¬†3.8: For more complex recipes where the final dish consists of multiple components (like cupcakes, which have cake part and frosting), grouping by component improves clarity.\n\n\n\n\n\nMaking Variants with Minimal Effort\nOrganizing a recipe into components also makes it easier to create variations (Figure¬†3.9).\nTwo key concepts make this possible:\n\nPolymorphism‚Äîeach variation should behave like the original so it can be used interchangeably (in code: implement the same interface).\nCode reuse‚Äîinstead of rewriting everything from scratch, extend the original recipe by specifying only the changes (in code: use inheritance or mixins).\n\nFor example, a chocolate frosting variant extends the plain frosting recipe by adding cocoa powder. The rest of the instructions remain unchanged.\n\n\n\n\n\n\nFigure¬†3.9: Some components have variants that can be used as drop-in replacements. Since these variants extend the original (e.g., chocolate frosting builds on plain frosting), we reuse existing instructions and only describe what‚Äôs new.\n\n\n\nBy applying these strategies, we turn a random assortment of ingredients and instructions into a well-structured, easy-to-follow recipe. This makes the cookbook not only clearer but the recipes also easier to maintain and extend‚Äîchanges to general instructions only need to be made once, and readers can effortlessly create variations by reusing existing steps.\nNow, let‚Äôs take the same approach with code.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html#good-code",
    "href": "03_state_flow_how.html#good-code",
    "title": "3¬† State & Flow: How?",
    "section": "Good Code",
    "text": "Good Code\nWhen we talk about ‚Äúgood code‚Äù in this book, we focus on three key properties, each building upon and influencing the others:\n\nEasy to understand: To work with code, you need to be able to understand it. When using a common library, it may be enough to know what a function does and how to use it, trusting that it works correctly under the hood. But with your own code, you also need to understand the implementation details. Otherwise, you‚Äôll hesitate to make changes and won‚Äôt be able to confidently say that it behaves as expected. Since code is read far more often than it is written, making it easy to understand ultimately saves time in the long run.\nEasy to change: Code is never truly finished. It requires ongoing maintenance‚Äîfixing bugs, updating dependencies (e.g., when a library releases a security patch with breaking changes), and adapting to evolving requirements, such as adding new features to stay competitive. Code that is easy to modify makes all of this much smoother.\nEasy to build upon and reuse: Ideally, adding new features shouldn‚Äôt mean proportional growth in your codebase. Instead, you should be able to reuse existing functionality. Following the DRY (Don‚Äôt Repeat Yourself) principle‚Äîavoiding redundant logic‚Äîmakes code easier to maintain because updates only need to be made in one place.\n\nThese qualities (along with others, like being easy to test) can be achieved by breaking code into smaller, independent units and organizing them into clear layers of abstraction. This approach has multiple benefits:\n\nManageable cognitive load: Smaller units fit comfortably into your working memory, making them easier to understand and reason about.\nEase of modification: Since units are independent, you can change one without unintended side effects elsewhere.\nReusability: Well-designed, general-purpose building blocks allow you to quickly assemble more complex functionality‚Äîlike constructing something out of LEGO bricks.\n\n\nFrom Complex to Complicated\nA well-known software engineering principle is KISS‚Äî‚ÄúKeep It Simple, Stupid!‚Äù While this is good advice at the level of individual functions or classes, it‚Äôs unrealistic to expect an entire software system to be simple. Most real-world software consists of multiple interacting components, making it inherently complicated rather than simple. The goal is to avoid unnecessary complexity (Figure¬†3.10).\n\n\n\n\n\n\nFigure¬†3.10: In accordance with the Cynefin framework, (software) systems can have different levels of complexity [2]. A script that sequentially executes a few steps might be simple. Most software, however, is at least complicated‚Äîit consists of multiple interacting components but can still be broken down into understandable subsystems. A complex system, often referred to as ‚Äúspaghetti code‚Äù or a ‚Äúbig ball of mud‚Äù [1], has so many interdependencies that its behavior is very difficult to predict‚Äîchanges in one part can have unintended consequences elsewhere.1 (Figure adapted from [3])\n\n\n\nA complex system has many interconnected elements with dependencies that are difficult to trace. Making a small change in one place can unexpectedly break something elsewhere. This makes debugging and maintenance a nightmare.\nInstead, we aim for a complicated system‚Äîone that may have many components, but is structured into independent, well-organized subsystems [2]. This way, we can understand and modify individual parts without having to fully grasp the entire system. To achieve this, components should be:\n\nDecoupled ‚Äì minimizing dependencies between them.\nCohesive ‚Äì ensuring they are ‚Äúcomplete‚Äù and contain all the code needed to fulfill their purpose.\n\nDecomposing a system this way will always constitute a trade-off, requiring us to balance local vs.¬†global complexity [2]. Consider two extremes:\n\nA system with two massive 500-line functions has low global complexity (only two things to keep track of) but high local complexity (each function is overwhelming to understand).\nA system with 500 tiny 2-line functions has low local complexity (each function is simple) but high global complexity (understanding how they interact becomes difficult).\n\nWhat constitutes a ‚Äúright-sized‚Äù unit‚Äîsmall enough to be understandable, yet large enough to avoid excessive fragmentation‚Äîwill depend on the context and your personal preferences (I usually aim for functions with around 5-15 lines of code).\nIdeally, these units should then form a hierarchy with different levels of abstraction (Figure¬†3.11), where lower-level units can be used as building blocks to create more advanced functionality.\n\n\n\n\n\n\nFigure¬†3.11: Complicated systems in software design often follow a hierarchy of abstraction. For example, to plot the results of a predictive model you call a function to create a scatter plot of predicted vs.¬†actual values, which internally calls another, more specific function to compute the \\(R^2\\) value displayed in the plot‚Äôs title.\n\n\n\nAt each level, you only need to understand how to use the functions at lower levels but not their internals. This reduces cognitive load, making it easier to navigate and extend the codebase with confidence.\nHowever, it‚Äôs important to note that low-level functions should ideally be kept stable, since everything built on top of them depends on their behavior. If these functions change, any code that relies on them will also need to be updated. This applies not only to your own functions but also to external libraries your code depends on.\nStandard libraries (i.e., those included with a programming language, not installed separately) are generally safe to build upon, as their functionality tends to be stable. In contrast, relying heavily on experimental or rapidly evolving libraries can lead to constant breakage as new versions introduce changes. To mitigate this, consider implementing an anti-corruption layer‚Äîa wrapper that provides a stable interface and handles necessary transformations. This centralizes adaptation and isolates the rest of your code from volatile dependencies.\n\n\nCohesive & Decoupled Units\nCode consists of variables that store data (state) and statements such as conditionals (if/else), loops (for/while), and transformations of this data to generate the desired output (control flow). To make our code easier to understand, modify, and reuse, we group related lines of code into individual units: functions and classes.\nEach unit has an interface (what is visible from the outside) and an implementation (the actual code‚Äîdetails that users of the unit shouldn‚Äôt need to worry about). By designing simple interfaces that hide complex implementations, we create an abstraction that reduces cognitive load.\nA good unit is both cohesive and decoupled:\n\nCohesion: Each unit should do a single, well-defined task, and all its code should relate to that purpose.\nDecoupling: Units should have minimal dependencies on other parts of the code or external resources. This reduces the risk that changes in one part of the system trigger more changes in multiple other parts.\n\nIn the following sections, we‚Äôll explore how to create such units in practice.\n\nInterface & Implementation\nTo avoid creating an unmanageable tangle of code, it‚Äôs crucial that we establish clear boundaries for each subsystem‚Äîwhat is inside a unit‚Äôs scope and what isn‚Äôt. These boundaries form the unit‚Äôs interface: the part that is accessible from the outside and works like a contract, guaranteeing stable usage over time.\nThe unit‚Äôs implementation, on the other hand, consists of the internal details of how the code works. This part is private and subject to change, meaning others shouldn‚Äôt rely on it.\nLet‚Äôs look at a simple example:\ndef n_times_x(x, n):\n    result = 0\n    for i in range(n):\n        result += x\n    return result\n\nif __name__ == '__main__':\n    # call our function with some values\n    my_result = n_times_x(3, 5)\n    print(my_result)  # should output 15\nThe interface (also called signature) of the n_times_x function consists of:\n\nThe function name (n_times_x)\nThe input arguments (x and n)\nThe return value (result)\n\n\n\n\n\n\n\nMake interfaces explicit\n\n\n\nInterfaces should be explicit and foolproof, especially since not all users read documentation carefully.\nFor example, if a function relies on positional arguments, users might accidentally swap values, leading to hard-to-find bugs. Using keyword arguments (i.e., forcing users to specify argument names) makes the interface clearer and reduces errors.\n\n\nAs long as the interface remains unchanged, we can freely modify the implementation. For instance, we can replace the inefficient loop with a proper multiplication:\ndef n_times_x(x, n):\n    return n * x\nThis change improves efficiency, but since the function still works the same way externally (it‚Äôs called with the same arguments and returns the same results), no updates are required in other parts of the code. This is the power of clear boundaries.\nHowever, changing a function‚Äôs name, for example, is another story. If we rename n_times_x, every reference to it must also be updated. Modern IDEs can automate this within a project, but if the function is used in external code (e.g., if it is part of an open source library), renaming requires a deprecation process to transition users gradually.\nThis is why choosing stable, well-thought-out interfaces upfront saves effort in the long run.\n\n\n\n\n\n\nGo deep\n\n\n\nPowerful units have narrow interfaces with deep implementations‚Äîthey expose only what‚Äôs necessary while handling meaningful computations internally [5]. For example:\n\nA function might take just two inputs (narrow interface) but span ten lines of logic (deep implementation).\nOr a one-liner function might use a complex formula that users shouldn‚Äôt need to understand.\n\nBut if your function is called n_times_x, and the implementation is just a one-liner doing exactly that, the abstraction does not help to reduce cognitive load. üòâ As a general rule, a unit‚Äôs interface should be significantly easier to understand than its implementation.\n\n\n\n\nCohesion\nWhen breaking code into functions, everything inside a unit should be related to its single responsibility. The unit should:\n\nInclude all relevant code needed to perform its task.\nExclude unrelated logic that belongs elsewhere.\n\nThis principle extends to classes, which group related data (attributes) and behaviors (methods).\n\nData Types Revisited: Classes\nVariables store values, like the ingredients from our cupcake recipe. The data referenced by a variable is always of a certain type:\n\nPrimitive types: Simple values like integers or strings (as discussed in Section 2.2.1).\nComposite types: Structures like lists and dictionaries that hold multiple values.\nUser-defined objects: Special-purpose structures defined in classes.\n\n# primitive data type: float\nx = 4.1083\n\n# composite data type: list\nmy_list = [\"hello\", 42, x]\n\n# composite data type: dict\nmy_dict = {\n    \"key1\": \"hello\",\n    \"key2\": 42,\n    \"key3\": my_list,\n}\nDictionaries are flexible but lack structure‚Äîfields can be added, removed, or modified unpredictably. Classes provide a more controlled way to define related values as part of a single entity. For example, a Person class ensures that every object has a first name, last name, and date of birth:\nfrom datetime import date\n\nclass Person:\n    def __init__(self, first_name: str, last_name: str, date_of_birth: date):\n        self.first_name = first_name\n        self.last_name = last_name\n        if date_of_birth &gt; date.today():\n            raise ValueError(\"Date of birth cannot be in the future.\")\n        # this attribute is private (by prefixing _) so that it is not accessed outside of the class\n        self._date_of_birth = date_of_birth\n\n    def get_age(self) -&gt; int:\n        # code outside the class only gets access to the person's age\n        today = date.today()\n        age = today.year - self._date_of_birth.year\n        if (today.month, today.day) &lt; (self._date_of_birth.month, self._date_of_birth.day):\n            age -= 1\n        return age\n\nif __name__ == '__main__':\n   new_person = Person(\"Jane\", \"Doe\", date(1988, 5, 20))\nAdditionally, the methods of a class can control how attributes can be accessed or modified.\nA cohesive class is designed to represent one specific concept completely‚Äîand only that concept. If a class contains many attributes, and one group of methods uses one subset of those attributes while another group relies on a different, non-overlapping subset, this suggests the class may be doing too much. In such cases, it likely no longer adheres to the single responsibility principle and should be broken into smaller, more focused classes.\n\n\nPublic vs.¬†Private in Classes\nFor well-structured classes, we should carefully control what is public and what is private:\n\nPublic attributes & methods form the external interface‚Äîchanging them may break code elsewhere.\nPrivate attributes & methods (meant for internal use) may be modified anytime.\n\n\n\n\n\n\n\nPublic and private in different languages\n\n\n\nAccess levels vary by programming language, for example:\n\nJava has multiple levels (public, protected, package, private).\nPython relies on convention rather than enforcement. Prefixing names with _ or __ signals they are ‚Äúprivate‚Äù, though they remain accessible.\n\nWhile Python won‚Äôt stop users from accessing private attributes, they do so at their own risk, knowing these details might change without notice.\n\n\n\n\n\nDecoupled\nIdeally, we want our units to be independent, meaning they should have as few dependencies as possible on their surroundings. This makes the code easier to understand since each unit can be comprehended in isolation. But even more importantly, it simplifies modifications: when requirements change or new features are needed, we want to minimize the number of places that require updates. In contrast, if code is tightly coupled, a single change in one unit can ripple through multiple parts of the system, increasing the likelihood of errors.\n\nRely Only on Public Interfaces\nA key principle for writing decoupled code is to only depend on the public interfaces of functions and classes. This means:\n\nAvoid accessing private variables.\nDon‚Äôt rely on undocumented quirks or unintended behaviors (e.g., a function returning an odd result when passed values outside its expected range).\n\nIf a function‚Äôs official interface changes, you‚Äôll likely receive a warning. But if you rely on internal implementation details, your code becomes fragile‚Äîseemingly minor updates in other parts of the code could break it.\n\n\nPure vs.¬†Impure Functions\nIn addition to dependencies on code defined elsewhere, another form of coupling comes from relying on external resources (like files, databases, or APIs). These kinds of external dependencies are eliminated when your units are pure functions [4].\nA pure function behaves like a mathematical function \\(f: x \\to y\\), taking inputs \\(x\\) and returning an output \\(y\\)‚Äîand given the same inputs, the results will be the same every time. Impure functions, on the other hand, have side effects‚Äîthey interact with the outside world, such as reading from or writing to a file or modifying global state.\ndef add(a, b):\n    # pure: returns a computed result without side effects\n    return a + b\n\ndef write_file(file_path, result):\n    # impure: writes data to a file (side effect)\n    with open(file_path, \"w\") as f:\n        f.write(result)\nWhy favor pure functions?\n\nThey are predictable‚Äîcalling them with the same inputs always produces the same output.\nThey are easy to test since they don‚Äôt rely on external state.\nThey reduce unexpected behavior, unlike impure functions, which may depend on changing external factors (e.g., the function may run fine the first time but then crash when trying to create a file that already exists or the computed results are suddenly different because in the meantime other code updated values stored in a database).\n\nThat said, impure functions are often necessary‚Äîcode needs to have an effect on the outside world, otherwise why should we execute it in the first place? The trick is to encapsulate as much critical logic as possible in pure functions, keeping side effects separate:\ndef pure_function(data):\n    # process data (pure logic)\n    result = ...\n    return result\n\ndef impure_function():\n    with open(\"some_input.txt\") as f:\n        data = f.read()\n    result = pure_function(data)\n    with open(\"some_output.txt\", \"w\") as f:\n        f.write(result)\nThis structure ensures the core logic is pure and testable:\n# tests.py\ndef test_pure_function():\n    data = \"some test data\"\n    result = pure_function(data)\n    assert result == \"some test result\"\n\n\nDependency Injection for Flexibility\nTo further decouple your code, use dependency injection: instead of hardcoding dependencies (e.g., file operations, database access), pass them as arguments. This allows functions to remain independent of specific implementations.\nWithout dependency injection:\nclass DataSource:\n    def read(self):\n        with open(\"some_input.txt\") as f:\n            return f.read()\n    def write(self, result):\n        with open(\"some_output.txt\", \"w\") as f:\n            f.write(result)\n\ndef fun_without_di():\n    # create external dependency inside the function\n    ds = DataSource()\n    data = ds.read()\n    result = ...\n    ds.write(result)\nWith dependency injection:\ndef fun_with_di(ds):\n    # dependency is passed as an argument\n    data = ds.read()\n    result = ...\n    ds.write(result)\n\nif __name__ == '__main__':\n    ds = DataSource()\n    fun_with_di(ds)\nNow, fun_with_di only requires an object implementing read and write, but it doesn‚Äôt care if it‚Äôs a DataSource or something else that implements the same interface. This makes testing much easier:\n# tests.py\nclass MockDataSource:\n    def read(self):\n        return \"Mocked test data\"\n    def write(self, result):\n        self.success = True\n\ndef test_fun_with_di():\n    mock_ds = MockDataSource()\n    fun_with_di(mock_ds)\n    assert mock_ds.success\nDependency injection can also be combined with pure functions to make your code testable at different levels of abstraction.\n\n\nAnti-Pattern: Global Variables\nBesides external resources like files and databases, another source of tight coupling and error-prone behavior is relying on global variables. These are defined at the script level (often below import statements) and can be accessed or modified anywhere in the code.\nGlobal variables can introduce temporal coupling, meaning the order of function execution suddenly matters. This can lead to subtle and hard-to-debug issues:\nPI = 3.14159\n\ndef calc_area_impure(r):\n    # since PI is not defined inside the function, the fallback is to use the global variable\n    return PI * r**2\n\ndef change_pi(new_pi=3):\n    # `global` is needed; otherwise, this would create a new local variable\n    global PI\n    PI = new_pi\n\nif __name__ == '__main__':\n    r = 5\n    area_org = calc_area_impure(r)\n    change_pi()\n    area_new = calc_area_impure(r)\n    assert area_org == area_new, \"Unexpectedly different results!\"\nThe safer approach is to pass values explicitly:\ndef calc_area_pure(r, pi):\n    return pi * r**2\nThis avoids hidden dependencies and ensures reproducibility.\nTo prevent unintended global variables, wrap your script logic inside a function:\ndef main():\n   # main script code incl. variable definitions\n   ...\n\nif __name__ == '__main__':\n    main()\nThis ensures that variables defined inside main() don‚Äôt leak into the global namespace.\n\n\n\n\n\n\nThink you need global variables? Wrap your code inside a class instead!\n\n\n\nIf multiple functions need to update the same set of variables, consider grouping them into a class instead of passing the same arguments repeatedly or relying on globals. A class can store shared state in attributes, accessed via self.\n\n\n\n\nüö® Call-By-Value vs.¬†Call-By-Reference\nAnother common pitfall is accidentally modifying function arguments.\nDepending on the programming language you use, input arguments are passed:\n\nBy value: A copy of the data is passed.\nBy reference: The function gets access to the original memory location and can modify the data directly.\n\nPython uses call-by-reference for mutable objects (e.g., lists, dictionaries), which can lead to unexpected behavior:\ndef change_list(a_list):\n    a_list[0] = 42  # modifies the original list\n    return a_list\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    print(my_list)  # [1, 2, 3]\n    new_list = change_list(my_list)\n    print(new_list) # [42, 2, 3]\n    print(my_list)  # [42, 2, 3] üò±\nTo prevent such side effects, create a copy of the variable before modifying it. This ensures the original remains unchanged:\nfrom copy import deepcopy\n\ndef change_list(a_list):\n    a_list = deepcopy(a_list)\n    a_list[0] = 42\n    return a_list\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    new_list = change_list(my_list)\n    print(my_list)  # [1, 2, 3] üòä\nTo avoid sneaky bugs, test your code to verify:\n\nInputs remain unchanged after execution.\nThe function produces the same output when called twice with identical inputs.\n\n\n\n\n\nBuilding with Blocks: Units in Context\nBy following the strategies outlined in the previous section, your code units should now be well-structured: each performs a meaningful task, encapsulated behind a simple interface, and avoids unnecessary dependencies on external resources or volatile implementation details. But does this mean that when combined, these units automatically form good code? Unfortunately, not necessarily.\n\nDon‚Äôt Repeat Yourself (DRY)\nWhen considering multiple code units in context, a common pitfall is violating the DRY (Don‚Äôt Repeat Yourself) principle. For example, critical business logic‚Äîsuch as computing an evaluation metric, applying a discount in an e-commerce system, or validating data, like the rules that define an acceptable password‚Äîmight be duplicated across several locations. If this logic needs to change (due to an error or evolving business requirements), you‚Äôd need to update multiple places, making your code harder to maintain and more prone to bugs. Make sure that important values and business rules are defined in a central place, the single source of truth, to avoid inconsistencies and duplication.\nThe most obvious DRY violation occurs when you catch yourself copy-pasting code‚Äîthis is a clear signal to refactor it into a reusable function. However, DRY isn‚Äôt just about code; it applies to anything where a single change in requirements forces multiple updates, including tests, documentation, or data stored across different databases [7]. To maintain your code effectively, aim to make each change in one place only. If that‚Äôs not feasible, at least keep related elements close together‚Äîfor example, documenting interfaces using docstrings in the code rather than in separate files.\n\n\nFacilitate Reuse\nWhile we want to keep interfaces narrow and simple, we also want our code units to be broadly applicable. This often means replacing hardcoded values with function arguments. Consider these two functions:\ndef a_plus_1(a):\n    return a + 1\n\ndef a_plus_b(a, b=1):\n    return a + b\nThe first function is highly specific, while the second is more general and adaptable. By providing sensible default values (b=1), we keep the function easy to use while allowing flexibility for more advanced cases.\n\n\n\n\n\n\nReusable code through refactoring, not overengineering\n\n\n\nReusable code isn‚Äôt typically created on the first try because future needs are unpredictable. Instead of overengineering, focus on writing simple, clear code and adapt it as new opportunities for reuse emerge. This process, called refactoring, is covered in more detail in Section 5.6.\n\n\nThat said, if your function ends up with ten arguments, reconsider whether it has a single responsibility. If it‚Äôs doing too much, breaking it into multiple functions is likely a better approach.\n\nPolymorphism\nWhen working with classes, reuse can be achieved through polymorphism, where multiple classes implement the same interface and can be used interchangeably. We‚Äôve already applied this principle when we used MockDataStorage instead of DataStorage for testing. This approach can be useful in research code as well, for example, you could define a Model interface with fit and predict methods so multiple models can share the same experiment logic:\nclass Model:\n    def fit(self, x, y):\n        raise NotImplementedError\n    def predict(self, x):\n        raise NotImplementedError\n\nclass MyModel(Model):\n    def fit(self, x, y):\n        ...  # implementation\n    def predict(self, x):\n        y_pred = ...\n        return y_pred\n\ndef run_experiment(model: Model):\n    # this code works with any model that implements\n    # appropriate fit and predict functions\n    x_train, y_train = ...\n    model.fit(x_train, y_train)\n    x_test = ...\n    y_test_pred = model.predict(x_test)\n\nif __name__ == '__main__':\n    # create a specific model (possibly based on arguments passed by the user)\n    model = MyModel()\n    run_experiment(model)\nThis way you can reuse your analysis code with all your models, which not only avoids redundancy, but ensure that all models are evaluated consistently.\n\n\nMixins\nAnother approach to reuse is using mixins, small reusable classes that provide additional functionality:\nimport numpy as np\n\nclass ScoredModelMixin:\n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.mean((y - y_pred)**2)\n\nclass MyModel(Model, ScoredModelMixin):\n    ...\n\nif __name__ == '__main__':\n    model = MyModel()\n    # this uses the function implemented in ScoredModelMixin\n    print(model.score(np.random.randn(5, 3), np.random.randn(5)))\nHistorically, deep class hierarchies with multiple levels of inheritance were common, but they often led to unnecessary complexity. Instead of forcing all functionality into a single class hierarchy, it‚Äôs better to keep interfaces narrow and compose functionality from multiple small, focused classes.\n\n\n\n\nSummary: From Working to Good Code\nWith these best practices in mind, revisit the steps you outlined when working backward from your desired output (in our case a plot) to the necessary inputs (data):\n\nGroup related steps into reusable functions. Functions like load_data, fit_model, predict_with_model, and compute_r2 help structure your code and prevent redundancy (DRY principle).\nIdentify explicit and implicit inputs and outputs:\n\nInputs: Passed as function arguments or read from external sources (files, databases, APIs‚Äîbut avoid global variables!).\nOutputs: Return values or data written to an external source (and other side effects, like modifying input arguments).\n\nExtract pure functions, which use only explicit inputs (arguments) and outputs (return values), from functions that rely on external resources or have other side effects. For example, if load_data includes both file I/O and preprocessing, separate out preprocess as a pure function to improve testability. Additionally, consider opportunities for dependency injection.\nEncapsulate related variables and functions into classes:\n\nLook for multiple variables describing the same object (e.g., parameters describing a Model instance).\nIdentify functions that need access to private attributes or should update attributes in-place (e.g., fit and predict should be methods of Model).\n\nGeneralize where possible:\n\nShould hardcoded values be passed as function arguments?\nCould multiple classes implement a unified interface? For example, different model classes should all implement the same fit and predict methods so they can be used with the same analysis code.\n\nOrganize your code into modules when it grows too large:\n\nKeep closely related functions together (e.g., load_data and preprocess, which can be expected to change together).\nPlace logically grouped files into separate directories (e.g., models/ for different model implementations).\n\n\nBy following these steps, you‚Äôll create code that is not only functional but also maintainable and extensible. However, avoid overengineering by trying to predict every possible future requirement. Instead, keep your code as simple as possible and refactor it as actual needs evolve.",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html#sec-draw-code",
    "href": "03_state_flow_how.html#sec-draw-code",
    "title": "3¬† State & Flow: How?",
    "section": "Draw Your Code",
    "text": "Draw Your Code\nTo summarize the overall design, we can create sketches that serve as both a high-level overview and an implementation guide. While formal modeling languages like UML exist for this purpose, don‚Äôt feel pressured to use them‚Äîthese diagrams are for you and your collaborators, so prioritize clarity over formality. Unless they‚Äôre part of official documentation that must meet specific standards, a quick whiteboard sketch is often a better use of your time.\nWe distinguish between two types of diagrams:\n\nStructural Diagrams ‚Äì These show the organization of your code: which functions and classes are in which modules, and how they depend on one another.\nBehavioral Diagrams ‚Äì These describe how your program runs: how inputs flow through the system and are transformed into outputs.\n\n\nStructure: Your Personal Code Library\nOur first sketch provides an overview of the reusable functions and classes we assembled in the previous section (Figure¬†3.12). This collection forms your personal code library, which you can reuse not just for this project but for future ones as well.\n\n\n\n\n\n\nFigure¬†3.12: The different functions and classes in our personal library, a collection of reusable units that can serve us in multiple projects. They are organized in different files and folders to keep related code close together. Green boxes represent pure functions, purple boxes impure functions with side effects (like reading or writing to external files), and blue boxes class methods. For classes (like MyModel) that extend another class (Model), only the additional attributes and methods are listed for that class. The arrows indicate dependencies.\n\n\n\n\n\nBehavior: Mapping Out Your Script\nOur second sketch outlines the script you‚Äôll execute to create the results you want, which builds upon these components (Figure¬†3.13). When designing the flow of your script, consider:\n\nHow the script is triggered ‚Äì Does it take command-line arguments? What inputs does it require?\nWhat the final output should be ‚Äì A results plot? A summary table? Something else?\nWhich intermediate results should be saved ‚Äì If your script crashes, you don‚Äôt want to start over. Since variables in memory disappear when the script terminates, consider saving key outputs to disk at logical checkpoints.\nWhich functions and classes from your library are used at each step ‚Äì Also, note any external resources (e.g., files, databases, or APIs) they interact with.\n\n\n\n\n\n\n\nFigure¬†3.13: The full flow of your script, including external resources and reusable functions. If you want, you can omit the lower levels as these internal function calls are already covered in our library diagram. The code to create a new model should create models of different types depending on the configuration parameters passed when calling the script so you can use the same script for multiple experiments.\n\n\n\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe inputs and steps required to produce your desired outputs‚Äîwhat constitutes working code.\nHow to structure this code into good code by creating cohesive, decoupled, and reusable units that use simple interfaces to abstract away complicated implementation details.\n\n\n\n\n\n\n\n[1] Foote B, Yoder J. Big Ball of Mud. Pattern languages of program design 4, 654‚Äì692 (1997).\n\n\n[2] Khononov V. Balancing Coupling in Software Design: Universal Design Principles for Architecting Modular Software Systems. Addison-Wesley Professional (2024).\n\n\n[3] McChrystal GS, Collins T, Silverman D, Fussell C. Team of Teams: New Rules of Engagement for a Complex World. Penguin (2015).\n\n\n[4] Normand E. Grokking Simplicity: Taming Complex Software with Functional Thinking. Manning Publications (2021).\n\n\n[5] Ousterhout JK. A Philosophy of Software Design. Yaknyam Press Palo Alto, CA, USA (2018).\n\n\n[6] Skiena SS. The Algorithm Design Manual. Springer (2020).\n\n\n[7] Thomas D, Hunt A. The Pragmatic Programmer: Your Journey to Mastery, 20th Anniversary Edition. Addison-Wesley Professional (2019).",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "03_state_flow_how.html#footnotes",
    "href": "03_state_flow_how.html#footnotes",
    "title": "3¬† State & Flow: How?",
    "section": "",
    "text": "The fourth category is chaotic, where cause and effect are unknowable. In software, this could be compared to rare cosmic-ray-induced bit flips that cause random, unpredictable behavior.‚Ü©Ô∏é",
    "crumbs": [
      "Gaining Clarity",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>State & Flow: How?</span>"
    ]
  },
  {
    "objectID": "part2.html",
    "href": "part2.html",
    "title": "Writing Code",
    "section": "",
    "text": "Now that you‚Äôve gained clarity on your concept, it‚Äôs finally time to write code (Figure¬†1).\n\n\n\n\n\n\nFigure¬†1: It‚Äôs finally time to implement your solution‚Äîwhile following some best practices.\n\n\n\nThis part starts with an introduction to some tools that will help you develop software more efficiently (4¬† Tools). Next, you‚Äôll learn about the best practices you should follow during the actual implementation (5¬† Implementation). We‚Äôll close with an outlook on what it takes to go from research to production-grade software (6¬† From Research to Production).\nAlthough the code examples in this book use Python, the general principles discussed here apply to most programming languages.",
    "crumbs": [
      "Writing Code"
    ]
  },
  {
    "objectID": "04_tools.html",
    "href": "04_tools.html",
    "title": "4¬† Tools",
    "section": "",
    "text": "Programming Languages\nLet‚Äôs start with a quick tour of some tools that can make your software engineering journey smoother.\nDifferent programming languages suit different needs. Here‚Äôs a quick overview of some popular ones used in science and engineering:\nDue to its broad applicability and popularity in industry, Python is used for the examples in this book. However, you should choose the programming language that is most popular in your field as this will make it easier for you to find relevant resources (e.g., tailored libraries) and collaborate with colleagues.\nThere are plenty of great books and other resources available to teach you programming fundamentals, which is why this book focuses on higher level concepts. Going forward we‚Äôll assume that you‚Äôre familiar with the basic syntax and functionality of your programming language of choice (incl.¬†key scientific libraries). For example, to learn Python essentials, you can work through this tutorial.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#programming-languages",
    "href": "04_tools.html#programming-languages",
    "title": "4¬† Tools",
    "section": "",
    "text": "R: Commonly used for statistics, with rich functionality to create data visualizations, fit statistical models (like different types of regression), and conduct advanced statistical tests (like ANOVA). The poplar Shiny framework also makes it possible to create interactive dashboards that run as web applications.\nMATLAB: Once dominant in engineering, used for simulations. But due to its high licensing costs, MATLAB is being replaced more and more by Python and Julia.\nJulia: Gaining traction in scientific computing for its speed and modern syntax.\nPython: A versatile language with strong support for data science, AI, web development, and more. Its active open source community has created many popular libraries for scientific computing (numpy, scipy), machine learning (scikit-learn, TensorFlow, PyTorch), and web development (FastAPI, streamlit).",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#version-control",
    "href": "04_tools.html#version-control",
    "title": "4¬† Tools",
    "section": "Version Control",
    "text": "Version Control\nVersion control is a system used to track and record changes to files over time, like a time machine that lets you revert to any version of your code or examine how it evolved. Version control is essential in software development to keep track of code changes and collaborate effectively.\n\nWhy Use Version Control?\n\nTrack changes: See what you‚Äôve modified and when, with the ability to revert if necessary.\nReview collaborators‚Äô changes: When working with others, reviewing their changes before they are merged with the main version of the code (in so-called pull or merge requests) ensures quality and provides opportunities to teach each other better ways of doing things.\nNot just for code: Version control can be used for any kind of file. While it‚Äôs less effective for binary formats like images or Microsoft Word documents where you can‚Äôt create a clean ‚Äúdiff‚Äù between two versions, you should definitely give it a try when writing your next paper in a text-based format like LaTeX.\n\n\n\nGit\nThe go-to tool for version control is Git. While desktop clients exist, you can also use git directly in the terminal as a command line tool.\nIf you‚Äôre new to Git, this beginner‚Äôs guide is a great place to start.\n\n\n\n\n\n\nEssential git commands\n\n\n\n\n\n\ngit init: Start a new repository in the current folder.\ngit status: View changes.\ngit diff: View differences between file versions before committing.\ngit add [file]: Stage files for a commit.\ngit commit -m \"message\": Save staged changes.\ngit push: Upload changes to a remote repository (e.g., on GitHub).\ngit pull: Download changes from a remote repository.\ngit branch: Create or list branches.\ngit checkout [branch]: Switch branches.\ngit merge [branch]: Combine branches.\n\n\n\n\nBy default, your repository‚Äôs files are on the main branch. Creating a new branch is like stepping into an alternate universe where you can experiment without affecting the main timeline. When making a major change or adding a new feature, it‚Äôs good practice to create a new branch, like new-feature, and implement your changes there. Once you‚Äôre satisfied with the result, you can merge the changes back into the main branch.\nThis approach keeps the main branch stable and ensures you always have a working version of your code. If you decide against your new feature, you can simply abandon the branch and start fresh from main. By creating a merge request (MR) once your new-feature branch is ready, you or a collaborator can review the changes thoroughly before merging them into main.\nTo publish your code or collaborate with others, your repository (i.e., the folder under version control) can be hosted on a platform like:\n\nGitHub: Great for open-source projects and public personal repositories to show off your skills.\nGitLab: Supports self-hosting, making it ideal for organizational needs.\n\nWe strongly encourage you to publish any code related to your publications on one of these platforms to promote reproducibility of your results! üë©‚Äçüî¨\n\n\n\n\n\n\nData versioning\n\n\n\nIn addition to the changes made to your code, you should also keep track of how your data is generated and transformed over time (data lineage). While small datasets can be included in your repository (e.g., in a separate data/ folder), there are also more tailored tools available specifically to version your data, like DVC.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#development-environment",
    "href": "04_tools.html#development-environment",
    "title": "4¬† Tools",
    "section": "Development Environment",
    "text": "Development Environment\nThe program you choose for writing code directly impacts your productivity. While you can technically write code using a plain text editor (like Notepad on Windows or TextEdit on macOS), special-purpose text editors and integrated development environments (IDEs) provide a tailored experience that boosts productivity.\n\nText Editors\nDeveloper-focused text editors are lightweight tools with features like syntax highlighting and extensions for basic programming tasks.\nExamples include:\n\nSublime Text: Lightweight and fast, with excellent customization through lots of plugins.\nAtom: Open-source and backed by GitHub (though less popular than other tools).\nVim and Emacs: Some of the first code editors, often used as command line tools and beloved by keyboard shortcuts enthusiasts.\n\n\n\nTerminal\nWhen you write code in a text editor, you need a way to execute it. This is where the terminal comes in. A terminal, or console, lets you interact with your computer through the command line, using text-based commands. Think of it like stepping back to the 1970s‚Äîor like being one of those cool hackers you see on TV.\nOn macOS and Linux, a terminal app is already preinstalled. On Windows, different options exist to install a Unix-like terminal, like the Windows Terminal. Inside the terminal, there‚Äôs a shell: the actual program that processes the commands you type. The most common shells on Unix systems are bash and zsh, which are quite similar. For this book, we‚Äôll assume you‚Äôre using one of these.\nWith the shell, you can navigate your computer‚Äôs file system and run programs through their command-line interface (CLI). Try it out!\n\n\n\n\n\n\nBasic shell commands\n\n\n\nFollow along by typing these commands into your terminal. In parallel, you can watch your normal file browser to see files and folders appear or disappear as you go.\n\npwd: Print the current working directory‚Äîthis shows the path to where you opened the terminal.\nls: List files and directories in the current location. Use ls -la for more details, including hidden files (like .gitignore).\ncd path/to/folder: Change directory to the specified path. Tips: Use tab to autocomplete names. If the path starts with /, it‚Äôs absolute (from the file system‚Äôs root). If it starts with ~/, it‚Äôs relative to your home directory. Use .. to move up one folder.\nmkdir new_folder: Create a new directory named new_folder.\ntouch new_file.txt: Create an empty file named new_file.txt.\ncp new_file.txt copied_file.txt: Copy new_file.txt to copied_file.txt. Use mv instead of cp to move or rename files.\nrm new_file.txt: Delete new_file.txt. Add -r to delete directories. But be careful: files deleted this way bypass the trash and are gone for good, so double-check before hitting enter!\n\nYou can also run other CLI programs in the terminal, like using the git commands described earlier.\nA Python script can be executed with python script.py (assuming the script is in your current directory).\n\n\nNot all CLI programs mentioned in this book will be preinstalled on your machine. Linux systems already come with a command-line package manager (like apt on Ubuntu), which can be used to install other tools. A popular package manager for macOS is brew, while for Windows you can use winget.\nOnce you get comfortable with your shell, you can also create shell scripts (files with a .sh extension) to automate tasks and handle more complex workflows. These scripts can include conditionals, loops, and other programming constructs. For more information on bash scripting, check out this resource and read the first few chapters of the book Research Software Engineering with Python [1].\n\n\nIntegrated Development Environments (IDEs)\nFull IDEs combine all the tools you need in one place‚Äîfile browser, editor, terminal, Git support, debugger, and more. They are ideal for larger projects and provide support for more complex tasks, like renaming variables across multiple files when you‚Äôre refactoring code.\nExamples include:\n\nVS Code: Minimalist by default but highly customizable with plugins, making it suitable for everything from basic editing to full-scale development.\nJetBrains IDEs (e.g., PyCharm): IDEs tailored to the needs of specific programming languages with very advanced features. You need to purchase a license to use the full version, but for many IDEs there is also a free community edition available.\nJupyterLab: An extension of Jupyter notebooks (see below), popular for data science and exploratory coding.\nRStudio: Tailored for R programming, with excellent support for data visualization, markdown reporting, and reproducible research workflows.\nMATLAB: The MATLAB programming language and IDE are virtually synonymous. However, its rich feature set comes with steep licensing fees.\n\n\n\nJupyter Notebooks\nJupyter notebooks are a unique format that lets you mix code, output (like plots), and explanatory text in one document. The name Jupyter is derived from Julia, Python, and R, the programming languages for which the notebook format, and later the JupyterLab IDE, were created. The IDE itself runs inside your web browser.\nNotebooks are great for exploratory data analysis and to create reproducible reports. However, since the notebooks themselves are composed of individual interactive cells that can be executed in any order, developing in notebooks often becomes messy quickly. We recommend that you keep the main logic and reusable functions in separate scrips or libraries and primarily use notebooks to create plots and other results. It is also good practice once you‚Äôre finished to restart the kernel and run your notebook again from top to bottom to make sure everything still works and you‚Äôre not relying on variables that were defined in now-deleted cells, for example.\n\n\n\n\n\n\nNotebooks as text files\n\n\n\nJupyter notebooks, stored as files ending in .ipynb, are internally represented as JSON documents. If you have your notebooks under version control (which you should üòâ), you‚Äôll notice that the diffs between versions look quite bloated. But do not despair! Tools like Jupytext can convert notebooks into plain text without loss of functionality.\n\n\n\n\n\n\n\n\nParameterize notebooks\n\n\n\nIf you want to execute the same notebook with multiple different parameter settings (e.g., create the same plots for different model configurations), have a look at papermill.\n\n\nIn addition to the original JupyterLab IDE and notebooks that you install on your computer, there are also free cloud-based options available, such as Google Colab, which even gives you free compute time on GPUs.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#reproducible-setups",
    "href": "04_tools.html#reproducible-setups",
    "title": "4¬† Tools",
    "section": "Reproducible Setups",
    "text": "Reproducible Setups\n‚ÄúIt works on my machine‚Äù isn‚Äôt good enough for science. Reproducibility means your results can be replicated by others (and by you a few months later when the reviewers of your paper request changes to your experiments). The first step to achieve this is to manage your dependencies (i.e., external libraries used by your code) to ensure the environment in which your code is executed is identical for everyone that runs your code, every time. This can be done using virtual environments, or, if you want to go even further, containers like Docker, which will be discussed in Chapter 6.\n\n\n\n\n\n\nVirtual environments in Python with uv\n\n\n\n\n\nVirtual environments isolate your project‚Äôs dependencies, thereby ensuring consistency. For Python, a common tool to do this is uv. It tracks the libraries and their versions in a pyproject.toml file like this:\n[project]\nname = \"example-project\"\nversion = \"0.1.0\"\ndescription = \"A sample Python project\"\nauthors = [{name=\"Your Name\", email=\"youremail@example.com\"}\"]\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    \"matplotlib &gt;=3.7.2\",\n    \"numpy &gt;=1.22.3,&lt;2\",\n]\nBasic commands:\n\nuv init example-project: Create a new project (folder incl.¬†pyproject.toml file).\nuv add {package}: Add a dependency (can also be done directly in the file).\nuv sync: Install all dependencies.\nuv run python script.py: Run a Python script inside the virtual environment.\n\n\n\n\n\nHandling Randomness\nYour program will often depend on randomly sampled values, for example, when defining the initial conditions for a simulation or initializing a model before it is fitted to data (like a neural network). To ensure that your experiments can be reproduced, it is important that you always set a random seed at the beginning of your program so the random number generator starts from a consistent state.\n\n\n\n\n\n\nSetting random seeds in Python\n\n\n\n\n\nAt the beginning of your script, set a random seed (depending on the library that you‚Äôre using this can vary):\nimport random\nimport numpy as np\n\nrandom.seed(42)\nnp.random.seed(42)\n\n\n\nTo get a better idea of how much your results depend on the random initialization and therefore how robust they are, it is advisable to always run your code with multiple random seeds and compare the results (e.g., compute the mean and standard deviation of the outcomes of different runs like in Figure¬†2.4).\n\n\n\n\n\n\nRandom state at startup\n\n\n\nDepending on the programming language that you‚Äôre using, if you run a script without executing any other code before, the random number generator may or may not always start in the same state. This means, if you don‚Äôt set a random seed and, for example, run your script ten times from scratch, you may always receive the same result even though the results would differ if the code was run under different circumstances. To avoid surprises, you should always explicitly set the random seed to have more control over the results.\n\n\n\n\n\n\n\n\nHardware differences\n\n\n\nIf your code is run on very different hardware, e.g., a CPU vs.¬†a GPU (graphics card, used to train neural network models, for example), despite setting a random seed, your results might still differ slightly. This is due to how the different architectures internally represent float values, i.e., with what precision the numbers are stored in memory.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#clean-and-consistent-code",
    "href": "04_tools.html#clean-and-consistent-code",
    "title": "4¬† Tools",
    "section": "Clean and Consistent Code",
    "text": "Clean and Consistent Code\nEspecially when working together with others, it can be helpful to follow to a style guide to produce clean and consistent code. Google published their style guides for multiple programming languages, which is a great resource and adhering to these rules will also help you to avoid common sources of bugs.\n\nFormatters & Linters\nSince programmers are often rather lazy, they developed tools that automatically fix your code to implement these rules where possible:\n\nFormatters rewrite code to follow a consistent style (e.g., add whitespace after commas).\nLinters analyze code for errors, inefficiencies, and deviations from best practices.\n\n\n\n\n\n\n\nFormatter & Linter in Python: ruff\n\n\n\n\n\nruff is a (super fast) formatter and linter for Python, written in Rust. You can install it via pip and configure it in the same pyproject.toml file that we also used to manage the dependencies of our project. Then run it over you code like this:\nruff check        # see which errors the linter finds\nruff check --fix  # automatically fix errors where possible\nruff format       # automatically format the code\nYou‚Äôll probably want to add exceptions for some of the errors that the linter checks for in your pyproject.toml file as ruff is quite strict. üòâ\n\n\n\nIt is important to have the configuration for your formatter and linter under version control as well, so that all collaborators use the same settings and you avoid unnecessary changes (and bloated diffs in merge requests) when different people format the code.\n\n\nPre-commit Hooks\nIn the heat of the moment, you might forget to run the formatter and linter over your code before committing your changes. To avoid accidentally checking messy code into your repository, you can configure so-called ‚Äúpre-commit hooks‚Äù. Pre-commit hooks catch issues automatically by enforcing coding standards before committing or pushing code with git.\n\n\n\n\n\n\nSetting up pre-commit hooks\n\n\n\n\n\nFirst, you need to install pre-commit hooks, e.g., through Python‚Äôs package manger pip:\npip install pre-commit\nThen configure it in a file named .pre-commit-config.yaml (here done for ruff):\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n    - id: check-yaml\n    - id: end-of-file-fixer\n    - id: trailing-whitespace\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.8.3\n  hooks:\n    # Run the linter.\n    - id: ruff\n      args: [ --fix ]\n    # Run the formatter.\n    - id: ruff-format\nThen install the git hook scripts from the config file:\npre-commit install\nNow the configured hooks will be run on all changed files when you try to commit them and you can only proceed if all checks pass.\n\n\n\nTo catch any style inconsistencies after the code was pushed to your remote repository (e.g., in case one of your collaborators has not installed the pre-commit hooks), you can also add these checks to your CI/CD pipeline (see Chapter 6).",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_tools.html#putting-it-all-together",
    "href": "04_tools.html#putting-it-all-together",
    "title": "4¬† Tools",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nWhen you set up all these tools, your repository should now look something like this (see here for more details; setup for programming languages other than Python will differ slightly):\nproject-name/\n‚îú‚îÄ‚îÄ .gitignore              # Exclude unnecessary files from version control\n‚îú‚îÄ‚îÄ README.md               # Describe the project purpose and usage\n‚îú‚îÄ‚îÄ pre-commit-config.yaml  # Pre-commit hook setup\n‚îú‚îÄ‚îÄ pyproject.toml          # Python dependencies and configs\n‚îú‚îÄ‚îÄ data/                   # Store (small) datasets\n‚îú‚îÄ‚îÄ notebooks/              # For exploratory analysis\n‚îú‚îÄ‚îÄ src/                    # Core source code\n‚îî‚îÄ‚îÄ tests/                  # Unit tests\nA clean project structure makes it easier to maintain your code.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to set up your development environment to code efficiently.\nHow to host your version-controlled repository on a platform like GitHub or GitLab, complete with pre-commit hooks to ensure well-formatted code.\nThe fundamental syntax of your programming language of choice (incl.¬†key scientific libraries) to get started.\n\n\n\nIf you want to gain a deeper understanding of many of the tools mentioned here, along with additional techniques, have a look at the book Research Software Engineering with Python [1].\n\n\n\n\n[1] Irving D, Hertweck K, Johnston L, Ostblom J, Wickham C, Wilson G. Research Software Engineering with Python. (2021).",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "05_implementation.html",
    "href": "05_implementation.html",
    "title": "5¬† Implementation",
    "section": "",
    "text": "Make a Plan\nArmed with the right tools and a solid concept, it‚Äôs time to start coding. In this chapter we‚Äôll discuss some best practices to ensure your idea does not only look good on paper.\nSeeing your software design mapped out in full can feel overwhelming‚Äîthere‚Äôs so much to do! That‚Äôs why it‚Äôs important to break the implementation process into small, manageable tasks. Rather than getting stuck in the details, start by implementing the full end-to-end flow. Once that‚Äôs in place, you can refine individual models, tweak data processing steps, or make plots prettier.\nTo organize implementation steps and track progress, consider using a Kanban board, a tool commonly used in project management. Software like Trello, Notion, or Linear can help you create Kanban boards, where you can describe tasks in more detail (e.g., adding sketches of the plots you want to generate) compared to a simple to-do list.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#make-a-plan",
    "href": "05_implementation.html#make-a-plan",
    "title": "5¬† Implementation",
    "section": "",
    "text": "Minimum Viable Results\n\n\n\nIn product development, there‚Äôs a concept called the Minimum Viable Product (MVP). This refers to the simplest version of a product that still provides value to users. The MVP serves as a prototype to gather feedback on whether the product meets user needs and to identify which features are truly essential. By iterating quickly and testing hypotheses, teams can increase the odds of creating a successful product that people will actually pay for.\nThis approach also has motivational benefits. Seeing something functional‚Äîeven if basic‚Äîearly on makes it easier to stay engaged. It‚Äôs far better than toiling for months without tangible results. You should apply the same mindset to your research software development by starting with a script that generates ‚ÄúMinimum Viable Results.‚Äù\nThis means creating a program that produces outputs resembling your final results, like plots or tables, but using placeholder data instead of actual values. For instance:\n\nIf your goal is to build a prediction model, start with one that simply predicts the mean of the observed data.\nIf you‚Äôre developing a simulation, begin with random outputs, such as a random walk.\n\nBy starting with Minimum Viable Results, you can test your code end-to-end early on, see tangible progress, and iteratively improve from there.\nThis approach also serves as a ‚Äústupid baseline‚Äù‚Äîa simple, easy-to-beat reference point for your final method. It‚Äôs a sanity check: if your sophisticated model can‚Äôt outperform a baseline that always predicts the mean, something‚Äôs off.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#from-concept-to-code-fill-in-the-blanks",
    "href": "05_implementation.html#from-concept-to-code-fill-in-the-blanks",
    "title": "5¬† Implementation",
    "section": "From Concept to Code: Fill in the Blanks",
    "text": "From Concept to Code: Fill in the Blanks\nUsing your software design from Chapter 3, you can now translate your sketch into a code skeleton. Start by outlining the functions, place calls to them where needed, and add comments for any steps you‚Äôll figure out later. For example, the design from Figure¬†3.13 could result in the following draft:\nimport numpy as np\nimport pandas as pd\n\nclass MyModel:\n    def __init__(self, param1):\n        self.param1 = param1\n\n    def fit(self, x, y):\n        pass\n\n    def predict(self, x):\n        y = ...\n        return y\n\ndef preprocess(df):\n    df = ...\n    return df\n\ndef load_data(file_name):\n    df = pd.read_csv(file_name)\n    df = preprocess(df)\n    return df\n\ndef compute_r2(y, y_pred):\n    r2 = ...\n    return r2\n\ndef evaluate_plot(y, y_pred):\n    r2 = compute_r2(y, y_pred)\n    # ... create and save plot ...\n    return r2\n\ndef main(model):\n    df_train = load_data(\"train.csv\")\n    model.fit(df_train.x, df_train.y)\n    df_test = load_data(\"test.csv\")\n    y_pred = model.predict(df_test.x)\n    r2 = evaluate_plot(df_test.y, y_pred)\n\nif __name__ == '__main__':\n    # script is called as `python script.py seed param1 [param2]`\n    seed, param1, param2 = ...\n    np.random.seed(seed)\n    # TODO: check which type of model should be created (mine or baseline)\n    model = MyModel(param1)\n    main(model)\n\n\n\n\n\n\nOrder of functions\n\n\n\nYour script likely includes multiple functions, so you‚Äôll need to decide their order from top to bottom. Since scripts typically start with imports (e.g., of libraries like numpy) and end with a main function, personally I prefer to put more general functions (i.e., the ones that are at the lower levels of abstraction in your call hierarchy and that only rely on external dependencies) towards the top of the file. This ensures that, as you read the script from top to bottom, each function depends only on what was defined before it. Maintaining this order avoids circular dependencies and encourages you to write reusable, modular functions that serve as building blocks for the code that follows.\n\n\nOnce your skeleton stands, you ‚Äúonly‚Äù need to fill in the details, which is a lot less intimidating than facing a blank page. Plus, since you started with a thoughtful design, your final program is more likely to be well-structured and easy to understand. Compare this to writing code on the fly, where decisions about functions are often made haphazardly‚Äîyou‚Äôll appreciate the difference.\n\n\n\n\n\n\nUsing AI code generators\n\n\n\nAI assistants like ChatGPT, Claude, or GitHub Copilot can be helpful tools when writing code, especially at the level of individual functions. However, remember that these tools only reproduce patterns from their training data, which includes both good and bad code. As a result, the code they generate may not always be optimal. For instance, they might use inefficient for-loops instead of more elegant matrix operations. Similarly, support for less popular programming languages may be subpar.\nTo get better results, consider crafting prompts like: ‚ÄúYou are a senior Python developer with 10 years of experience writing efficient, edge-case-aware code. Write a function ‚Ä¶‚Äù\n\n\n\nKeep It Compact\nWhen writing code, aim to achieve your goals while using as little screen space as possible‚Äîthis applies to both the number of lines and their length.\n\n\n\n\n\n\nTips to create compact, reusable code\n\n\n\n\nAvoid duplication: Instead of copying and pasting code in multiple places, consolidate it into a reusable function to save lines (DRY principle).\nPrefer ‚Äòdeep‚Äô functions: Avoid extracting very short code fragments (1-2 lines) into a separate function, especially if this function would require many arguments. Such shallow functions with wide interfaces increase complexity without meaningfully reducing line count. Instead, strive for deep functions (spanning multiple lines) with narrow interfaces (e.g., only 1-3 input arguments, i.e., fewer arguments than the function has lines of code), which tend to be more general and reusable [8].\nAddress nesting: If your code becomes overly nested, this can be a sign that parts of the code should be moved into a separate function. This simplifies logic and shortens lines.\nUse guard clauses: Deeply nested if-statements can make code harder to read. Instead, use guard clauses [1] to handle preconditions (e.g., checking for wrong user input) early, leaving the ‚Äúhappy path‚Äù clear and concise. For example:\nif condition:\n    if not other_condition:\n        # do something\n        return result\nelse:\n    return None\nCan be refactored into:\nif not condition:\n    return None\nif other_condition:\n    return None\n# do something\nreturn result\nThis approach reduces nesting and improves readability.\n\n\n\n\n\nBreaking Code into Modules\nStarting a new project often begins with all your code in a single script or notebook. This is fine for quick and small tasks, but as your project grows, keeping everything in one file becomes messy and overwhelming. To keep your code organized and easier to understand, it‚Äôs a good idea to move functionality into separate files, also called (sub)modules. Separating code into modules makes your project easier to navigate and can lay the foundation for your own library of reusable functions and classes, useful across multiple projects.\nA typical first step is splitting the main logic of your analysis (main.py) from general-purpose helper functions (utils.py). Over time, as utils.py expands, you‚Äôll notice clusters of related functionality that can be moved into their own files, such as data_utils.py, models.py, or results.py. To create cohesive modules, you should group code that tends to change together, which increases maintainability as you don‚Äôt need to switch between files when implementing a new feature. Modules based on domains or use cases, instead of technical layers, are therefore preferred, as the changes required to implement a new feature are generally limited to a single domain [7].\nThis approach naturally leads to a clean directory structure, which might look like this for a larger Python project:1\nsrc/\n‚îú‚îÄ‚îÄ main.py\n‚îî‚îÄ‚îÄ my_library/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ data_utils.py\n    ‚îú‚îÄ‚îÄ models/\n    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_a.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_b.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ interface.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ my_model.py\n    ‚îî‚îÄ‚îÄ results.py\nIn main.py, you can import the relevant classes and functions from these modules to keep the main script clean and focused:\nfrom my_library.models.my_model import MyModel\nfrom my_library.data_utils import load_data\n\nif __name__ == '__main__':\n    # steps that will be executed when running `python main.py`\n    model = MyModel()\n\n\n\n\n\n\nKeep helper functions separate\n\n\n\nAlways separate reusable helper functions from the main executable code. This also means that files like data_utils.py should not include a main function, as they are not standalone scripts. Instead, these modules provide functionality that can be imported by other scripts‚Äîjust like external libraries such as numpy.\n\n\nAs you tackle more projects, you may develop a set of functions that are so versatile and useful that you find yourself reusing them across multiple projects. At that point, you might consider packaging them as your own open-source library, allowing other researchers to install and use them as well.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "href": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "title": "5¬† Implementation",
    "section": "Documentation & Comments: A Note to Your Future Self",
    "text": "Documentation & Comments: A Note to Your Future Self\nWhile you write it, everything seems obvious. However, when revisiting your code a few months later (e.g., to try a different experiment), you‚Äôre often left wondering what the heck you were doing. This is especially true when some external constraint (like a library quirk) forced you to create a workaround instead of opting for the straightforward solution. When returning to such code, you might be tempted to replace the awkward implementation with something more elegant, only to rediscover why you chose that approach in the first place. This is where comments can save you some trouble. And they are even more important when collaborating with others who need to understand your code.\nWe distinguish between documentation and comments: Documentation provides the general description of when and how to use your code, such as function docstrings explaining what the function computes, its input arguments, and return values. This is particularly important for open source libraries where you can‚Äôt personally explain the code‚Äôs purpose and usage to others. Comments help developers understand why your code was written in a certain way, like explaining that unintuitive workaround. Additionally, for scientific code, you may also need to document the origin of certain values or equations by referencing the corresponding paper in the comments.\n\n\n\n\n\n\nCode should be self-documenting\n\n\n\nIdeally, your code should be written so clearly that it‚Äôs self-explanatory. Comments shouldn‚Äôt explain what the code does, only why it does that (when not obvious). Comments and documentation, like code, need to be maintained‚Äîif you modify code, you need to update the corresponding comments or they become misleading and harmful rather than helpful. Using comments sparingly minimizes the risk of confusing, outdated comments.\nInformative variable and function names are essential for self-explanatory code. When you‚Äôre tempted to write a comment that summarizes what the following block of code does (e.g., # preprocess data), consider moving these lines into a separate function with an informative name, especially if they contain significant, reusable logic.\n\n\n\nNaming Is Hard\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n‚Äì Phil Karlton2\n\nFinding informative names for variables, functions, and classes can be challenging, but good names are crucial to make the code easier to understand for you and your collaborators.\n\n\n\n\n\n\nTips for effective naming\n\n\n\n\nNames should reveal intent. Longer names (consisting of multiple words in snake_case or camelCase, depending on the conventions of your chosen programming language) are usually better. However, stick to domain conventions‚Äîif everyone understands X and y as feature matrix and target vector, use these despite common advice denouncing single letter names.\nBe consistent: similar names should indicate similar things. This makes it easier to recognize patterns and extrapolate what you‚Äôve learned about one implementation (e.g., BaselineAModel) to others (e.g., BaselineBModel), which reduces the mental effort required to understand the code [6]. On the other hand, if you name two things similarly even though they behave differently, this increases the cognitive load as you need to explicitly memorize this exception.\nAvoid reserved keywords (i.e., words your code editor colors differently, like Python‚Äôs input function).\nUse verbs for functions, nouns for classes.\nUse affirmative phrases for booleans (e.g., is_visible instead of is_invisible).\nUse plurals for collections (e.g., cats instead of list_of_cats).\nAvoid encoding types in names (e.g., sample_array), since if you decide to change the data type later, you either need to rename the variable everywhere or the name is now misleading.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#tests-protect-what-you-love",
    "href": "05_implementation.html#tests-protect-what-you-love",
    "title": "5¬† Implementation",
    "section": "Tests: Protect What You Love",
    "text": "Tests: Protect What You Love\nWe all want our code to be correct. During development, we often verify this manually by running the code with example inputs to check if the output matches our expectations. While this approach helps ensure correctness initially, it becomes cumbersome to recreate these test cases later when the code needs changes. The simple solution? Package your manual tests into a reusable test suite that you can run anytime to check your code for errors.\nTests typically use assert statements to confirm that the actual output matches the expected output. For example:\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    # verify correctness with examples, including edge cases\n    # syntax: assert (expression that should evaluate to True), \"error message\"\n    assert add(2, 2) == 4, \"2 + 2 should equal 4\"\n    assert add(5, -6) == -1, \"5 - 6 should equal -1\"\n    assert add(-2, 10.6) == 8.6, \"-2 + 10.6 should equal 8.6\"\n    assert add(0, 0) == 0, \"0 + 0 should equal 0\"\n\n\n\n\n\n\nTesting in Python with pytest\n\n\n\nConsider using the pytest framework for your Python tests. Organize all your test scripts in a dedicated tests/ folder to keep them separate from the main source code.\n\n\nAs discussed in Chapter 3, pure functions‚Äîthose without side effects like reading or writing external files‚Äîare especially easy to test because you can directly supply the necessary inputs. Placing your main logic into pure functions therefore simplifies testing the critical parts of your code. For impure functions, such as those interacting with databases or APIs, you can use techniques like mocking to simulate external resources, ideally combined with dependency injection.\nWhen designing your tests, focus on edge cases‚Äîunusual or extreme scenarios like values outside the normal range or invalid inputs (e.g., dividing by zero or passing an empty list). The more thorough your tests, the more confident you can be in your code. Each time you make significant changes, run all your tests to ensure the code still behaves as expected.\nSome developers even adopt Test-Driven Development (TDD), where they write tests before the actual code. The process begins with writing tests that fail (or don‚Äôt even compile), then creating the code to make them pass. TDD can be highly motivating as it provides clear goals, but it requires discipline and may not always be practical in the early stages of development when function definitions are still evolving.\n\n\n\n\n\n\nTesting at different levels\n\n\n\nIdeally, you‚Äôll test your software at all levels:\n\nUnit Tests: Test individual units (e.g., single functions) to verify basic logic. These should make up the bulk of your test code.\nIntegration/System Tests: Check that different parts of the system work together as expected. These often require more complex setups, like running multiple services at the same time, to test the flow end-to-end.\nManual Testing: Identify unexpected behavior or overlooked edge cases. Whenever a bug is found, create an automated test to reproduce it and prevent regression.\nUser Testing: Evaluate the user interface (UI) with real users to ensure clarity and usability. UX designers often perform these tests using design mockups before coding begins.\n\n\n\n\nDebugging\nWhen your code doesn‚Äôt work as intended, you‚Äôll need to debug‚Äîsystematically identify and fix the problem. Debugging becomes easier if your code is organized into small, testable functions covered by unit tests. These tests often help narrow down the source of the issue. If your existing tests didn‚Äôt catch the bug, first write a new, failing test to reproduce it before fixing it. This confirms your fix works and prevents a regression‚Äîmeaning the bug won‚Äôt sneak back into the code later.\nTo isolate the exact line causing the error:\n\nUse print statements to log variable values at key points and understand the program‚Äôs flow.\nAdd assert statements to verify intermediate results.\nUse a debugger, often integrated into your IDE, to set breakpoints where execution will pause, allowing you to step through the program manually and inspect variables.\n\nDebugging is an essential skill, not only to identify the root cause of bugs, but also to improve your understanding of the code and its behavior.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#make-it-fast",
    "href": "05_implementation.html#make-it-fast",
    "title": "5¬† Implementation",
    "section": "Make It Fast",
    "text": "Make It Fast\n\nMake it run, make it right, make it fast.\n‚Äì Kent Beck (or rather his dad, Douglas Kent Beck3)\n\nNow that your code works and produces the right results (as you‚Äôve dutifully confirmed with thorough testing), it‚Äôs time to think about performance.\n\n\n\n\n\n\nReadability over performance\n\n\n\nAlways prioritize writing code that‚Äôs easy to understand. Performance optimizations should not come at the cost of readability. More time is spent by humans reading and maintaining code than machines executing it.\n\n\n\nFind and fix the bottlenecks\nInstead of randomly trying to speed up everything, focus on the parts of your code that are actually slow. A simple way to identify bottlenecks is to insert log statements with timestamps at key points in your code to measure the time elapsed between steps. And if you manually interrupt a (Python) script during a long run and it always stops in the same place, that‚Äôs also often an indication that this step could be the issue. For a more systematic approach, use a profiler. Profilers analyze your code and show you how much time each part takes, helping you decide where to focus your efforts.\nAccessing files on disk or fetching data over the network is one of the slowest operations in most programs. Whenever possible, cache the results by storing the loaded data in memory to avoid repeated access to external resources. Just be mindful of how frequently the external data changes and invalidate the cache when the information becomes outdated.\n\n\n\n\n\n\nRun it in the cloud\n\n\n\nWorking with large datasets may trigger Out of Memory errors as your computer runs out of RAM. While optimizing your code can help, sometimes the quickest solution is to run it on a larger machine in the cloud. Platforms like AWS, Google Cloud, Azure, or your institution‚Äôs own compute cluster make this cost-effective and accessible. That said, always look for simple performance improvements first!\n\n\n\n\nThink About Big O\nSome computations have unavoidable limits. For example, finding the maximum value in an unsorted list requires checking every item‚Äîthere is no way around this. The ‚ÄúBig O‚Äù notation is used to describe these limits, helping you understand how your code scales as data grows (both in terms of execution time and required memory).\n\nConstant time (\\(\\mathcal{O}(1)\\)): Independent of dataset size (e.g., looking up a key in a dictionary).\nLinear time (\\(\\mathcal{O}(n)\\)): Grows proportionally to data size (e.g., finding the maximum in a list).\nProblematic growth (e.g., \\(\\mathcal{O}(n^3)\\) or \\(\\mathcal{O}(2^n)\\)): Polynomial or exponential scaling can make algorithms impractical for large datasets.\n\nWhen developing a novel algorithm, you should examine its scaling behavior both theoretically (e.g., using proofs) and empirically (e.g., timing it on datasets of different sizes). Designing a more efficient algorithm is a major achievement in computational research!\n\n\nDivide & Conquer\nIf your code is too slow or your dataset too large, try splitting the work into smaller, independent chunks and combining the results. Such a ‚Äúdivide and conquer‚Äù approach is used in many algorithms, like the merge sort algorithm, and in big data frameworks like MapReduce.\n\nExample: MapReduce\nMapReduce [2] was one of the first frameworks developed to work with ‚Äòbig data‚Äô that does not fit on a single computer anymore. The data is split into chunks and distributed across multiple machines, where each chunk is processed in parallel (map step), and then the results are combined into the final output (reduce step).\nFor instance, if you want to train a machine learning model on a very large dataset, you could train separate models on subsets of the data and then aggregate their predictions (e.g., by averaging them), thereby creating an ensemble model.\n\n\n\n\n\n\nReplace For-Loops with Map/Filter/Reduce\n\n\n\nSequential for loops can often be replaced with map, filter, and reduce operations for better readability and potential parallelism:\n\nmap: Transform each element in a sequence.\nfilter: Keep elements that meet a condition.\nreduce: Aggregate elements recursively (e.g., summing values).\n\nFor example:\nfrom functools import reduce\n\n### Simplify this loop:\nresult_sum = 0\nresult_max = -float('inf')\nfor i in range(10000):\n    new_i = i**0.5\n    # the modulo operator x % y gives the remainder when diving x by y\n    # i.e., we're checking for even numbers, where the rest is == 0\n    if (round(new_i) % 2) == 0:\n        result_sum += new_i\n        result_max = max(result_max, new_i)\n\n### Using map/filter/reduce:\n# map(function to apply, list of elements)\nnew_i_all = map(lambda x: x**0.5, range(10000))\n# filter(function that returns true or false, list of elements)\nnew_i_filtered = filter(lambda x: (round(x) % 2) == 0, new_i_all)\n# reduce(function to combine current result with next element, list of elements, initial value)\nresult_sum = reduce(lambda acc, x: acc + x, new_i_filtered, 0)\nresult_max = reduce(lambda acc, x: max(acc, x), new_i_filtered, -float('inf'))\n# (of course, for these simple cases you could just use sum() and max() on the list directly)\nIn Python, list comprehensions also offer concise alternatives:\nnew_i_filtered = [i**0.5 for i in range(10000) if (round(i**0.5) % 2) == 0]\n\n\n\n\n\nExploit Parallelism\nMany scientific computations are ‚Äúembarrassingly parallelizable,‚Äù meaning tasks can run independently. For example, running simulations with different model configurations, initial conditions, or random seeds. Each of these experiments can be submitted as a separate job and run in parallel on a compute cluster. By identifying parts of your code that can be parallelized, you can save time and make full use of available resources.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#sec-refactoring",
    "href": "05_implementation.html#sec-refactoring",
    "title": "5¬† Implementation",
    "section": "Refactoring: Make Change Easy",
    "text": "Refactoring: Make Change Easy\nRefactoring is the process of modifying existing code without altering its external behavior [4]. In other words, it preserves the ‚Äúcontract‚Äù (interface) between your code and its users while improving its internal structure.\nCommon refactoring tasks include:\n\nRenaming: Giving (internally used) variables, functions, or classes more meaningful and descriptive names.\nExtracting Functions: Breaking large functions into smaller, more focused ones (with a single responsibility).\nEliminating Duplication: Consolidating repeated code into reusable functions (DRY principle).\nSimplifying Logic: Reducing deeply nested code structures or introducing guard clauses for clarity.\nReorganizing Code: Grouping related functions or classes into appropriate files or modules.\n\n\nWhy refactor?\nRefactoring is typically done for two main reasons:\n\nAddressing Technical Debt:\nWhen code is written quickly‚Äîoften to meet deadlines‚Äîit may include shortcuts that make future changes harder. This accumulation of compromises is called ‚Äútechnical debt.‚Äù Refactoring cleans up this debt, improving code quality and making the code easier to understand.\n\nExample: Revisiting old code can be like tidying up a messy campsite. Just as a good scout leaves the campground cleaner than they found it, a responsible developer leaves the codebase better for the next person (or themselves in the future).\n\nMaking Change Easier:\nSometimes, implementing a new feature in your existing code feels like forcing a square peg into a round hole. Instead of struggling with awkward workarounds, you should first refactor your code to align with the new requirements. The goal of software design isn‚Äôt to predict every possible future change (which is impossible) but to adapt gracefully when those changes arise. This promotes an evolutionary architecture, where you solve problems once you understand them better [3].\n\nBefore adding a new feature, clean up your code so that the change feels natural and seamless. This not only simplifies the task at hand but also results in a more general, reusable functions and classes.\n\n\n\n\n\n\n\n\nTips when refactoring\n\n\n\n\nTest as you refactor: Always run tests before and after refactoring to ensure no functionality is accidentally broken. Writing or expanding automated tests is often part of the process to safeguard against regressions.\nLeverage IDE support: Modern IDEs like PyCharm or Visual Studio Code provide tools for automated refactoring, such as renaming, extracting functions, or moving files. These can save time and reduce errors.\nAvoid over-refactoring: While cleaning up code is valuable, avoid making unnecessary changes that don‚Äôt improve functionality or clarity. Over-refactoring wastes time and can confuse collaborators.\n\n\n\n\n\nRefactorings to simplify changes\n\nFor each desired change, make the change easy (warning: this may be hard), then make the easy change.\n‚Äì Kent Beck4\n\n\nReplace Magic Numbers with Constants: Magic numbers‚Äîvalues with unclear meaning‚Äîcan make code harder to understand and maintain. By replacing them with constants, you create a single source of truth that‚Äôs easy to modify.\n# before:\nif status == 404:\n    ...\n\n# after:\nERROR_NOT_FOUND = 404\nif status == ERROR_NOT_FOUND:\n    ...\nYou can also use enumerations to specify a set of related constants. Enums can be especially helpful to make a function‚Äôs interface explicit. For example, by specifying that an input argument should be an HTTPStatus, users of the function know that they can‚Äôt just pass any arbitrary integer:\nfrom enum import Enum\n\nclass HTTPStatus(Enum):\n    OK = 200\n    CREATED = 201\n    FORBIDDEN = 403\n    NOT_FOUND = 404\n    INTERNAL_SERVER_ERROR = 500\n    SERVICE_UNAVAILABLE = 503\n\ndef get_status_message(status_code: HTTPStatus):\n    \"\"\"Returns the HTTP status message for a given HTTPStatus enum value.\"\"\"\n    return f\"{status_code.name.replace('_', ' ').title()} ({status_code.value})\"\n\nif __name__ == \"__main__\":\n    print(get_status_message(HTTPStatus.OK))  # Output: Ok (200)\n    print(get_status_message(HTTPStatus.NOT_FOUND))  # Output: Not Found (404)\nDon‚Äôt Repeat Yourself (DRY): Copying and pasting code may seem like a quick fix, but it leads to problems later. If the logic changes, you‚Äôll need to update it everywhere it‚Äôs duplicated, which is error-prone. Instead, move the logic into a reusable function or method.\n# before:\nif (model.a &gt; 5) and (model.b == 3) and (model.c &lt; 8):\n    ...\n\n# after:\nclass MyModel:\n    def is_ready(self):\n        return (self.a &gt; 5) and (self.b == 3) and (self.c &lt; 8)\n\nif model.is_ready():\n    ...\nImplement Wrappers: When working with external libraries or APIs, their provided interface might not align with your needs, and adapting to it directly can lead to awkward implementations in your code. A better solution is to create a wrapper that implements the interface you wish you had, translating the external API‚Äôs inputs and outputs into the format that best suits your implementation. This approach keeps your code clean, consistent, and easier to maintain, while confining the less-than-ideal API interactions to a single location. Plus, if the external API changes, you only need to update the wrapper instead of changing your code everywhere‚Äîwhich is why these wrappers are also called anti-corruption layers.\nUse Alternative Constructors: Similar to a wrapper, you can add a class method to create objects in a way that‚Äôs different from the regular constructor. This is useful when the input data doesn‚Äôt directly match what the constructor needs. For instance, imagine you have a configuration file that specifies settings for a simulation. If the names or structure of these settings don‚Äôt match the constructor‚Äôs parameters, you can create a from_config method to handle the translation and then call the constructor with the correct arguments. The advantage is that if the format of the configuration file changes in the future, you only need to update this one method, keeping the rest of your code the same.\nclass Date:\n  def __init__(self, year, month, day):\n      self.year = year\n      self.month = month\n      self.day = day\n\n  @classmethod\n  def from_str(cls, date_str):\n      # parse the string and create a new instance\n      year, month, day = map(int, date_str.split('-'))\n      return cls(year, month, day)\n\n# usage:\ndate1 = Date(2025, 01, 30)\ndate2 = Date.from_str(\"2025-01-30\")\nOrganize for Cohesion: Keep code elements that need to change together in the same file or module. Conversely, separate unrelated parts of your code to prevent unnecessary entanglement. This way, changes are localized, which reduces cognitive load.\nIn larger codebases shared by multiple teams, this is even more critical. When changes require excessive communication and coordination between teams, it signals a need to reorganize the code. Clear ownership and reduced dependencies help teams work independently by making sure the system loosely coupled through agreed upon interfaces.\n\n\n\nLarge-scale Refactoring\nWhile smaller portions of code are often tidied up as you go [1], larger refactorings that span multiple files or repositories require more upfront planning [5][6][7].\nThe following steps will help keep larger refactorings on track and ensure they deliver real improvements:\n\nIdentify the problems this refactoring should address.\nDon‚Äôt refactor just because the code is ugly‚Äîrefactor because it‚Äôs causing problems. For example, the current structure may make it difficult to implement important new features or maintain the code efficiently. List all the problems the code creates and rank them by severity to ensure your refactoring tackles the most critical issues.\nEnvision the ideal state.\nCode can become suboptimal for many reasons. Maybe a looming deadline forced developers to take shortcuts, leading to technical debt. Maybe an inexperienced developer made a design choice that no longer fits. But most likely, the requirements have evolved since the code was written, making what was once a good solution no longer suitable.\nTry to break free from the existing structure and its limitations. If you were designing the system from scratch today, given everything you now know about current and future requirements, what would you build? What should the code look like once refactored?\nVerify that the ideal state solves the most important issues.\nRevisit your list of problems and ensure that your envisioned ideal state actually addresses them. If necessary, iterate on your vision until you have a solution that tackles the most critical challenges. This gives you a better understanding of which changes to the codebase are actually necessary to achieve your goal.\nMake a realistic plan to get closer to your ideal state.\nWhile it might be tempting to rewrite everything from scratch, this is rarely practical. A complete rewrite would likely take longer than expected and introduce new, unforeseen issues, as the existing code likely accounts for edge cases and hidden requirements you‚Äôve forgotten.\nInstead, translate your ideal state into small, targeted changes to the existing codebase that still provide significant benefits. Ideally, each change should:\n\nBe independent of the others, allowing for incremental progress.\nDeliver some immediate value on its own.\n\nCreate a prioritized list of independent changes, considering both:\n\nImpact: Which of the original problems does this change solve?\nEffort: How difficult would this be to implement? What dependencies or additional steps are required (e.g., database migrations, external system changes, or involvement from other teams)?\n\nTo accurately assess effort, detail your plan‚Äîoutline which files and functions will be affected and identify any external dependencies. Once you‚Äôve evaluated impact vs.¬†effort, decide which changes are essential, which are nice-to-have, and which might not be worth the effort, while taking into account that some steps might depend on the successful completion of other changes.\nGet feedback on your plan.\nIf possible, discuss your plan with collaborators and stakeholders‚Äîespecially those affected by the changes. They might catch overlooked dependencies or identify potential blockers before you run into them during implementation.\nExecute incrementally and merge frequently.\nInstead of implementing all changes at once, work step by step, merging updates back into the codebase as quickly as possible. This minimizes risk, ensures early testing and validation, and helps maintain motivation‚Äîsince every small change delivers immediate value.\n\nBy refactoring regularly and following these practices, you‚Äôll create a cleaner, more maintainable codebase that is adaptable to future needs and enjoyable to work with.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to transform your concept into code.\nSome best practices to write code that is easy to understand and maintain.\n\n\n\n\n\n\n\n[1] Beck K. Tidy First? O‚ÄôReilly Media, Inc. (2023).\n\n\n[2] Dean J, Ghemawat S. MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM 51(1), 107‚Äì113 (2008).\n\n\n[3] Ford N, Parsons R, Kua P, Sadalage P. Building Evolutionary Architectures. O‚ÄôReilly Media, Inc. (2022).\n\n\n[4] Fowler M. Refactoring: Improving the Design of Existing Code. Addison-Wesley Professional (2018).\n\n\n[5] Lemaire M. Refactoring at Scale. O‚ÄôReilly Media, Inc. (2020).\n\n\n[6] Lilienthal C. Sustainable Software Architecture: Analyze and Reduce Technical Debt. dpunkt.verlag (2019).\n\n\n[7] Lilienthal C, Schwentner H. Domain-Driven Transformation: Monolithen Und Microservices Zukunftsf√§hig Machen. dpunkt.verlag (2023).\n\n\n[8] Ousterhout JK. A Philosophy of Software Design. Yaknyam Press Palo Alto, CA, USA (2018).",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#footnotes",
    "href": "05_implementation.html#footnotes",
    "title": "5¬† Implementation",
    "section": "",
    "text": "The __init__.py file is needed to turn a directory into a package from which other scripts can import functionality. Usually, the file is completely empty.‚Ü©Ô∏é\nhttps://martinfowler.com/bliki/TwoHardThings.html‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/704385198301904896‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/250733358307500032‚Ü©Ô∏é",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "06_from_research_to_production.html",
    "href": "06_from_research_to_production.html",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Components of Software Products\nYour results look great, the paper is written, the conference talk is over‚Äînow you‚Äôre done, right?! Well, in academia, you might be. But in industry, this is often just the beginning.\nIn this chapter, we explore some concepts and tools that can elevate your code to the next level: the additional components required to build a full-fledged software product that users can interact with, as well as some strategies for deploying your code and delivering it to your end users (Figure¬†6.1).\nWho knows‚Äîmaybe you‚Äôll even be inspired to turn your project into a deployable app, which could become a great showcase in your next job application.\nSo far, your code might consist of scripts or notebooks with analyses and a set of reusable helper functions in your personal library. The next step? Making your code accessible to others by turning it into standalone software with a graphical user interface (GUI). Additionally, we‚Äôll explore how to expand beyond static data sources like CSV or Excel files.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_from_research_to_production.html#components-of-software-products",
    "href": "06_from_research_to_production.html#components-of-software-products",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Graphical User Interface (GUI)\nSoftware shines when users can interact with it easily. Instead of using a command-line interface (CLI), these days, users expect intuitive GUIs with buttons and visual elements.\nWe can broadly categorize software programs into:\n\nStand-alone desktop or mobile applications, which users download and install on their devices.\nWeb-based applications that run in a browser, like Google Docs. These are increasingly popular thanks to widespread internet access.\n\nFor web apps, the GUI that users interact with is also referred to as the frontend, while the backend handles behind-the-scenes tasks like data storage and processing. Even seemingly standalone desktop clients often connect to a backend server for cloud storage or to enable collaboration on shared documents. We‚Äôll explore how this works in the section on APIs.\nIn research, the goal is often to make results more accessible, for example, by transforming a static report into an interactive dashboard where users can explore data. To do this, we recommend you start with a web-based app.\nMany books and tutorials are written on the topic of building user-friendly software applications and a lot of it is very specific to the programming language and framework you‚Äôre using‚Äîplease consult your favorite search engine to discover more resources on this topic. üòâ\n\n\n\n\n\n\nWeb apps in Python\n\n\n\nIf you use Python, try the Streamlit framework to turn your analysis scripts into a web app with just a few additional lines of code. To create more advanced web apps, you can also check out the Reflex framework.\n\n\n\n\nDatabases\nSo far, we‚Äôve assumed that your data is stored in spreadsheets (like CSV or Excel files) on your computer. While this works for smaller datasets and simple workflows, it becomes less practical as your data grows or is generated dynamically, such as through user interactions, and needs to be accessed and updated by multiple people at the same time. This is where databases come in, offering a more efficient and scalable way to store, retrieve, and manage data [5].\nDatabases come in many forms, each suited to different types of data and use cases. Two key considerations when choosing a database are [9]:\n\nthe kind of data you need to store, and\nhow that data will be used.\n\n\nTypes of Data in Databases\nDifferent kinds of databases are ideal for different types of data (see also Section 2.2.1):\n\nStructured data: This resembles spreadsheet data, with rows for records and columns for attributes. Structured data is typically stored in relational (SQL) databases, where data is organized into multiple interrelated tables. Each table has a schema‚Äîa strict definition of the fields it contains and their types, such as text or numbers. If data doesn‚Äôt match the schema, it‚Äôs rejected.\n\n\n\n\n\n\nNormalization in relational databases\n\n\n\nA process called normalization reduces redundancy by splitting data into separate tables. For example, instead of storing material_type, material_supplier, and material_quality directly in a table of samples, you‚Äôd create a materials table with a unique ID for each material, then reference the material_id in the samples table. This avoids duplication and makes updates easier but requires more complex queries to combine tables and extract all the data needed for analysis.\n\n\nSemi-structured data: JSON or XML documents contain data in flexible key-value pairs and are often stored in NoSQL databases. Unlike SQL databases, these databases don‚Äôt enforce a strict schema, which makes them ideal for handling complex, nested data structures or dynamically changing datasets. For example, APIs often exchange data in JSON format, which can be stored as-is to avoid breaking it into tables and reconstructing it later.\nModern relational databases, such as Postgres, blur the line between structured and semi-structured data by supporting JSON columns alongside traditional tables.\nUnstructured data: Files like images, videos, and text documents are typically stored on disk. Data lakes (e.g., AWS S3) provide additional tools to manage these files, but fundamentally, this is similar to organizing files in folders on your computer. If your data is processed through a pipeline, it‚Äôs often a good idea to save copies of the files at each stage (e.g., in raw/ and cleaned/ folders).\nStreaming data: High-volume, real-time data (e.g., IoT sensor logs) is best managed in specialized databases optimized for streaming, such as Apache Kafka.\n\n\n\nUse Cases for Databases\nWhen choosing a database, you‚Äôll also want to consider how the data will be used later:\n\nTransactional processing (OLTP): In this use case, individual records are frequently created and retrieved (e.g., financial transactions). These systems prioritize fast write speeds and maintaining an up-to-date view of the data.\nBatch analytics (OLAP): Data analysis is often performed in large batches to generate reports or insights, such as identifying which products users purchased last month. To avoid overloading the operational database with complex queries, data is typically copied from transactional systems into analytical systems (e.g., data warehouses) using an ETL process (Extract-Transform-Load).\nReal-time analytics: For applications requiring live data (e.g., interactive dashboards), databases and frameworks optimized for streaming or in-memory processing (e.g., Apache Flink) are ideal.\n\nScaling your database system is another critical factor. Consider how many users will access it simultaneously, how much data they‚Äôll retrieve, and how often it will be updated.\n\n\nCRUD Operations\nDatabases support four basic operations collectively called CRUD:\n\nCreate: Add new records.\nRead: Retrieve data.\nUpdate: Modify existing records.\nDelete: Remove records.\n\nThese operations are performed using queries, often written in SQL (Structured Query Language). For example, SELECT * FROM table; retrieves all records from a table. If you‚Äôre new to SQL, this tutorial is a great place to start.\n\n\nORMs and Database Migrations\nWriting raw database queries can be tedious, especially when working with complex schemas. Object-Relational Mappers (ORMs) simplify this by mapping database tables to objects in your code. With ORMs, you can interact with your database using familiar programming constructs and even define the schema directly in your code.\nWhen designing a database schema and implementing the corresponding ORMs, it‚Äôs helpful to first sketch out the structure of the data (Figure¬†6.2). Start by identifying the key objects (which map to database tables), their fields, and their relationships.\n\n\n\n\n\n\nFigure¬†6.2: The classes User, Order, Product, and Category are mapped to the corresponding tables users, orders, products, and categories. Every record in a table is uniquely identified by a primary key, often named id. Fields in a table can either store data directly (e.g., text or boolean values) or reference records in another table, establishing relationships between tables. These relationships are defined using foreign keys, which store the primary key of a related record. For example, an Order references a single User ID (indicating the user who placed the order) and multiple Product IDs (the items included in the order).\nRelationships between tables can be bidirectional or unidirectional, depending on the use case. For instance, when querying a Product, we want to list all the categories it belongs to, and vice versa. In contrast, the relationship between Order and Product only goes one way: when retrieving an Order, we want to know which products are included, but querying a Product doesn‚Äôt usually require listing all the orders it appears in.\n\n\n\nDatabase migrations, or schema changes, often require careful coordination between code and database updates. For instance, renaming a field means you have to update your database and modify the code accessing it at the same time. Keeping migration scripts and application code (including ORMs) in the same repository helps ensure consistency during such updates.\n\n\n\n\n\n\nManaging databases in Python\n\n\n\nThe SQLModel library is highly recommended when working with relational databases in Python and also includes a great tutorial to learn more about ORMs and databases in general. For database migrations, check out Alembic.\n\n\n\n\n\nAPIs\nIn contrast to a user interface, through which a human interacts with a program, an Application Programming Interface (API) enables software components to communicate with one another. Think of it as a contract that defines how different systems interact. For example, an API might specify the classes and functions a library provides so developers can integrate it effectively.\nAPIs are often associated with Web APIs, which provide functionality over the internet. These can either be external services, like the Google Maps API for retrieving directions, or a custom-built backend that serves as an abstraction layer for a database. This abstraction is useful because it can combine data, enforce rules (e.g., verifying user permissions), and maintain a consistent interface even when the database structure changes.\n\nInteracting with APIs\nWeb APIs typically use four HTTP methods that correspond to the CRUD (Create, Read, Update, Delete) operations in databases:\n\nGET: Retrieves data, most commonly used when accessing websites. You can include additional parameters by appending a ? to the URL. For example, https://www.google.com/search?q=web+api searches for ‚Äúweb api‚Äù using the query parameter q. To pass multiple parameters, separate them with &.\nPOST: Sends data to create a new record, often as a JSON object, for example, when submitting a form.\nPUT: Updates an existing record.\nDELETE: Removes a record.\n\nYou typically interact with APIs through a website‚Äôs frontend, which triggers these API calls in the background. However, APIs can also be queried directly to access raw data, usually returned in JSON format.\n\n\n\n\n\n\nAPI keys and authentication\n\n\n\nMany APIs require an API key to access their functionality. This key serves as an identifier, allowing the API to authenticate users, track usage, and apply rate limits to prevent abuse. Always keep your API keys secure and avoid exposing them in public repositories or client-side code.\n\n\nThere are many free public APIs you can explore. As an example, we‚Äôll use The Cat API to demonstrate how to interact with an API.\nYou can perform a GET request directly in your web browser. For instance, by visiting https://api.thecatapi.com/v1/images/search?limit=10, you‚Äôll receive a JSON response containing a list of 10 random cat image URLs along with additional details like the image IDs.\nFor more advanced requests, such as POST, you‚Äôll need specialized tools. Popular GUI clients include Postman, Insomnia, and Bruno. If you prefer command-line tools, curl is a powerful option. Alternatively, you can interact with APIs programmatically using your preferred programming language and relevant libraries.\n\n\n\n\n\n\nInteracting with APIs programmatically\n\n\n\nIn the examples below, we use curl and Python to interact with The Cat API to retrieve the latest votes for cat images with a GET request and submit a new vote using a POST request.\nUsing curl\nEnsure curl is installed by running which curl in a terminal‚Äîthis should return a valid path to your installation.\n# GET request to view the last 10 votes for cat images\ncurl \"https://api.thecatapi.com/v1/votes?limit=10&order=DESC\" \\\n-H \"x-api-key: DEMO-API-KEY\"\n\n# POST request to submit a new vote\n# the payload after -d is the JSON object submitted to the API\ncurl -X POST \"https://api.thecatapi.com/v1/votes\" \\\n-H \"Content-Type: application/json\" \\\n-H \"x-api-key: DEMO-API-KEY\" \\\n-d '{\n  \"image_id\": \"HT902S6ra\",\n  \"sub_id\": \"my-user-1234\",\n  \"value\": 1\n}'\n# now run the GET request again to see your new vote\nUsing Python\nPython‚Äôs requests library is great for working with APIs.\nimport requests\n\nBASE_URL = \"https://api.thecatapi.com/v1/votes\"\nAPI_KEY = \"DEMO-API-KEY\"\n\n# GET request to fetch the last 10 votes\ndef get_last_votes():\n    response = requests.get(\n        BASE_URL,\n        headers={\"x-api-key\": API_KEY},\n        params={\"limit\": 10, \"order\": \"DESC\"}\n    )\n    if response.status_code == 200:\n        print(response.json())\n    else:\n        print(f\"Error: {response.status_code}\")\n\n# POST request to submit a new vote\ndef submit_vote(image_id, sub_id, value):\n    data = {\"image_id\": image_id, \"sub_id\": sub_id, \"value\": value}\n    response = requests.post(\n        BASE_URL,\n        headers={\"Content-Type\": \"application/json\", \"x-api-key\": API_KEY},\n        json=data\n    )\n    if response.status_code == 201:\n        print(\"Vote submitted!\")\n    else:\n        print(f\"Error: {response.status_code}\")\n\nif __name__ == '__main__':\n    get_last_votes()\n    submit_vote(\"HT902S6ra\", \"my-user-1234\", 1)\n\n\n\n\nImplementing APIs\nWhen designing an API, specifically a REST (REpresentational State Transfer) API, it‚Äôs important to understand the concept of an endpoint. An endpoint is a specific URL in your API where a resource can be accessed or modified. For example, if you‚Äôre building a photo-sharing app, an endpoint like /images might allow users to view or upload images. Endpoints should be named using descriptive, plural nouns (e.g., /users, /images) to clearly represent the resources being accessed. It‚Äôs also best practice to avoid including verbs in endpoint names (e.g., /get_users), since the HTTP method (like GET or POST) already specifies the action being taken, such as retrieving or creating data.\nAnother key design principle is statelessness. Similar to the concept of pure functions, this means that each API request should contain all the information needed to complete the action, like user authentication tokens. This way, the server doesn‚Äôt need to remember anything about previous requests, making the API easier to scale. This is especially important in cloud-based environments where multiple requests from the same user may be routed to different servers [1].\nData that needs to be persisted can be stored either in the frontend (client) or a central database in the backend (server), depending on its purpose. Temporary data, like a shopping cart, can be maintained on the user‚Äôs machine using cookies or local storage. Permanent data, such as a purchase order, is best stored in the backend database to ensure long-term accessibility. This approach supports stateless APIs, as the backend server doesn‚Äôt need to keep the session state in memory. Instead, all necessary data is either included in the request or can be fetched from the database, allowing each request to be processed independently.\n\n\n\n\n\n\nImplementing APIs in Python with FastAPI\n\n\n\nFastAPI is a Python framework that makes building APIs straightforward. With just a few lines of code, you can turn a function into an endpoint that validates input and returns a JSON response. It‚Äôs beginner-friendly and highly performant.\n\n\n\n\nCommunicating with a Server: Use Cases\nCode that runs on our local devices (like your laptop or smartphone) is often limited by the available hardware. For example, if you play a chess game on your phone against a bot, this bot will only have limited capabilities since the processor on your phone is not sufficient to run an advanced AI model (Figure¬†6.3).\n\n\n\n\n\n\nFigure¬†6.3: The user interacts only with their device to play chess against a simple bot locally on their phone. The small database icon next to the phone symbolizes the local storage on your phone, where the current game state is saved after each move in case the app is closed so that you can retrieve this saved state the next time you open the app and continue with the game.\n\n\n\nA server in the cloud, on the other hand, has the necessary hardware (in this case multiple GPUs) to run an advanced AI-based chess bot (Figure¬†6.4). However, to play against this more challenging opponent, you need to have internet access to be able to submit your current game state to the server and receive the next bot move as a response. This also means that you can‚Äôt play the game in case your internet connection is not sufficient or the server is down.\n\n\n\n\n\n\nFigure¬†6.4: The user interacts with their device and data is submitted to and received from a server in the cloud hosting an advanced AI to determine the next bot move.\n\n\n\nFinally, by communicating with a server we can also play against other human players (Figure¬†6.5). For this, both players take turns in submitting and receiving their next moves to the server. Since the chess API may run on multiple server nodes to handle a high load of requests, it can‚Äôt be guaranteed that both players will interact with the same node, i.e., we need a stateless design. To accomplish this, the server persists and retrieves data in a central database that can be accessed by all nodes.1\n\n\n\n\n\n\nFigure¬†6.5: Player 1 interacts with their device and data about the next move is submitted to a server node, where it is stored in a central database. The device from player 2 requests an update from a different server node, which accesses the same database to retrieve the move submitted by player 1.\n\n\n\n\n\n\nAsynchronous Communication\nWhen your script calls a library function or API and waits for it to return before continuing with the rest of the code, this is an example of synchronous communication. It‚Äôs similar to a conversation where one person speaks and then waits and listens while the other person responds.\nIn contrast, asynchronous (async) communication allows the program to keep running while waiting for a response. Once the response arrives, it is processed and integrated into the workflow, but until then the code just continues without it. Just like when you send an email to someone asking for some data and they send you the results a few hours later.\nFor example, a website might fetch data from multiple APIs, showing placeholders until the responses arrive. This approach improves the user experience because it keeps the user interface (UI) responsive and enables faster loading by processing multiple tasks in parallel.\n\nEvent-Driven Architecture\nFor most applications, communicating directly with external services‚Äîwhether synchronously or async‚Äîis the right approach, because eventually the requested data is needed to finish the original task. But there are also use cases where it‚Äôs enough that your message was received and you don‚Äôt need to wait for a response. For example, when placing an order in an online shop, users only care that the order was submitted successfully. They don‚Äôt wait in front of the screen until it was packaged and shipped‚Äîwhich could take days. An email notification can inform them of progress later.\nSuch a scenario calls for an event-driven architecture, which takes async communication to the extreme. Here, multiple services can operate independently by exchanging information via events using a message queue (MQ), a system that temporarily stores event messages like JSON documents. These messages act as instructions, containing all relevant details about an event, such as a user‚Äôs order information. Publishers (senders) create events, and subscribers (receivers) process them based on their type, such as Order Submitted.\nAn event-driven architecture offers several advantages. By decoupling components, it allows publishers and subscribers to run independently, even in different programming languages or environments. This makes it easier to scale systems and assign teams to own specific components without needing to understand the full system. Additionally, one event can trigger multiple actions. For instance, when an order is packed, one system might update the user database while another generates a shipping label. This approach thereby simplifies the propagation of data to multiple services and can also facilitate replication of live data to testing and staging environments.\nHowever, this type of architecture also brings with it some challenges. Since no single component has a full view of the system, tracking the state of a specific task, such as whether an order is still waiting or in progress, can be difficult. Furthermore, while MQs often guarantee that each message is handled at least once, the system requires careful design in case a message is processed multiple times. For example, if a subscriber crashes after processing a message but before confirming its completion, the MQ might reassign the task to another instance, potentially leading to duplicate processing. For these reasons, event-driven architectures should only be used when direct communication between services is not an option [8].\n\n\n\nBatch Jobs\nUnlike continuously running services such as web APIs, batch jobs are scripts or workflows used to process accumulated data in one go. They are particularly effective when tasks don‚Äôt require immediate processing or when grouping tasks can improve efficiency. To automate recurring tasks, batch workflows can be scheduled at specific intervals using tools like cron jobs.2\nExamples of scenarios where batch jobs are useful include:\n\nFetching new messages from a queue every 10 minutes to process them in bulk, reducing overhead.\nGenerating a sales report for the marketing department every Monday at midnight.\nRunning nightly data validation to check for data drift or anomalous patterns.\nRetraining a machine learning model every week using newly collected data to create updated recommendations, like Spotify‚Äôs ‚ÄúDiscover Weekly‚Äù playlist [3].\n\nFor large-scale jobs, distributed systems might be necessary to ensure they complete within an acceptable timeframe.\n\n\nSoftware Design Revisited\nAs your software evolves beyond a simple script, it becomes a system composed of multiple interconnected components. Each component can be viewed as a subsystem with its own defined boundaries and interface, responsible for specific functionalities while interacting with other parts of the system.\nTo manage this growing complexity, it‚Äôs best to think of these components‚Äîsuch as a GUI/client/frontend, API/server/backend, and data storage (files, databases, message queues)‚Äîas distinct layers. A clean design follows the principle of layered communication, where each layer interacts only with the layer directly below it [6]. For example, the client communicates with the server, and the server interacts with the database, avoiding ‚Äúskip connections‚Äù where one layer bypasses another.\nThis design principle minimizes dependencies and makes the system easier to maintain: If the interface of one component changes, only the layer directly above it has to be adapted, for example, if a field in the database is renamed, only the API needs to be updated.\nWhen you design these more complicated systems, it‚Äôs even more important to sketch the overall architecture before you start with the implementation. Visualizing how the layers interact can reveal potential bottlenecks or unnecessary complexity and gives you and your collaborators clarity on the big picture.\nInstead of lumping everything into ‚Äúyour code‚Äù versus ‚Äúthe outside world‚Äù as we did in Figure¬†3.13, you can use swimlanes to separate the process steps performed by each component and distinguish between the different layers (Figure¬†6.6).\nBegin by visualizing the flow from the user‚Äôs perspective, focusing on how they interact with the client to complete their intended task. Once the user experience is clear, continue with the server and database layers and map out how these can support and implement each step:\n\nWhat data is needed to render the GUI?\nWhat data needs to be persisted in the database for later?\n\nFinally, in accordance with the read and write (CRUD) operations on the database, you can design the ORMs used to represent and store the required data (Figure¬†6.2).\n\n\n\n\n\n\nFigure¬†6.6: A simplified flow showing what happens when a user orders something online: the user opens a product page, which triggers a request to the API to fetch the corresponding product details from the database (DB). The user then clicks the ‚Äúadd to cart‚Äù button, which places the product into the shopping cart (in local storage managed by the client). The user then views the shopping cart and clicks ‚Äúpurchase‚Äù, which triggers a POST request to the API, submitting the user‚Äôs cart contents. The API creates a new record in the orders table to store the purchase details and submits an Order event to the message queue (MQ), thereby alerting other services that a new order needs to be packed and shipped. The endpoint returns with the status code 201 (‚Äúsuccess‚Äù) and the client redirects the user to a page that tells them the purchase was successful, at which point the user closes the tab.\n\n\n\n\nDomain-Driven Design\nAs a software product grows, it eventually becomes too complex for a single team to manage. To reduce cognitive load and improve maintainability, the system should be broken into smaller subsystems, each of which can be owned and managed autonomously by a dedicated team.\nTo minimize inter-team dependencies and reduce communication overhead, it‚Äôs best to split the system along domain or use-case boundaries [4][7]. For example, in an e-commerce platform, one team might own the ‚Äúsearch & browse‚Äù domain, while another handles ‚Äúpurchasing.‚Äù Although implementing a new feature may involve both client and server changes, those changes typically stay within a single domain, allowing the responsible team to implement them independently.\nTechnically, this often means breaking a monolith into domain-based microservices [7], where each service operates independently (which also means it needs its own database). The key challenge is balancing local vs.¬†global complexity: Each microservice should be small enough to be fully understood and maintained by one team, but large enough to encapsulate meaningful logic. If services are too granular, they require excessive inter-service communication to get anything done, leading to tightly coupled systems, only now with the added complexity of a distributed system and multiple deployments.",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_from_research_to_production.html#delivery-deployment",
    "href": "06_from_research_to_production.html#delivery-deployment",
    "title": "6¬† From Research to Production",
    "section": "Delivery & Deployment",
    "text": "Delivery & Deployment\nModern software development requires reliable and efficient processes to build, test, and deploy applications [2]. Delivery and deployment strategies ensure that new features and updates are released quickly, safely, and at scale, minimizing disruptions to users while maintaining quality.\n\nCI/CD Pipelines: Automating Development Cycles\nContinuous Integration (CI) and Continuous Delivery/Deployment (CD) pipelines are the backbone of modern software practices. CI focuses on automating the process of integrating code changes into a shared repository. Every change triggers automated tests to ensure that the new code works harmoniously with the existing codebase. CD extends this by automating the preparation or deployment of changes into production, either ready for manual approval (Continuous Delivery) or fully automated (Continuous Deployment). This drastically reduces manual effort, minimizes human error, and enables faster iteration cycles.\nCI/CD pipelines are either included directly into version control platforms, such as GitHub Actions and GitLab CI/CD, or can be run using external tools like Jenkins or CircleCI.\n\nOptimizing CI/CD Pipelines\nTo enhance pipeline efficiency and reliability, consider the following practices:\n\nDependency Caching: Cache dependencies to reduce the time spent downloading and installing them for each build.\nSelective Testing: Run only the tests affected by recent changes to speed up feedback.\nReal-Time Notifications: Notify developers immediately when a pipeline fails, enabling faster issue resolution.\n\n\n\n\n\n\n\nSecurity in CI/CD pipelines\n\n\n\nSecurity must be a priority in any CI/CD process. For example, it is best practice to include a dependency scanning step to detect vulnerabilities in third-party libraries. Furthermore, you should never include sensitive information‚Äîsuch as API tokens, database credentials, or private keys‚Äîdirectly in your code. However, because CI jobs often require access to this information, you can securely store secrets using dedicated CI/CD variables or external secret management tools like HashiCorp Vault or AWS Secrets Manager.\n\n\nA well-designed CI/CD pipeline not only saves time and resources but also ensures a consistent and high-quality delivery of software.\n\n\n\nContainers in the Cloud\nContainers, powered by tools like Docker, encapsulate applications with their dependencies, ensuring consistency across different environments. This portability simplifies deployment and reduces issues caused by environment differences.\nFor managing containerized applications at scale, Kubernetes (k8s) is the industry standard. Kubernetes automates the orchestration of containers, providing features like:\n\nAuto-scaling: Adjust resources dynamically based on workload.\nSelf-healing: Automatically restart failed containers.\nLoad Balancing: Distribute traffic efficiently across services.\n\n\nUsing Cloud Platforms\nCloud platforms like AWS, Google Cloud Platform (GCP), and Microsoft Azure offer robust infrastructures for deploying and scaling applications. For simpler workflows, managed services like Render or Heroku abstract away much of the operational complexity.\nManaging costs effectively is critical in cloud deployments. Key strategies include:\n\nResource Scaling: Reduce unused resources during off-peak hours.\nServerless Computing: Use serverless models, like AWS Lambda, for infrequent workloads to save costs.\nCost Monitoring Tools: Leverage AWS Cost Explorer or GCP Billing to track and optimize spending.\n\n\n\nInfrastructure as Code (IaC)\nInstead of configuring your cloud setup manually through the platform‚Äôs GUI, it is highly recommended to use Infrastructure as Code tools like Terraform and AWS CloudFormation to manage cloud infrastructure programmatically. The IaC configuration files can then be version-controlled, which ensures:\n\nReproducible setups for consistent environments.\nEasier onboarding for new team members.\nReduced risk of configuration drift.\n\n\n\nTesting and Staging Environments\nDeploying changes directly to production is risky. To ensure stability:\n\nUse staging environments that mimic production to validate changes before release.\nMaintain testing environments for early experimentation and debugging.\n\nTechniques like A/B testing and feature toggles allow gradual rollouts or controlled exposure of new features, minimizing user disruption. This can be achieved using deployment strategies like:\n\nBlue-Green Deployments: Maintain two environments (blue and green) and switch traffic between them for A/B tests or to reduce downtime during updates.\nCanary Releases: Gradually expose updates to a small group of users, monitoring for issues before full deployment.\n\n\n\n\nScaling Considerations\nAs applications grow, scaling requires thoughtful architectural design. You should consider:\n\nTask Separation: For example, train machine learning models periodically as batch jobs, while keeping prediction services running continuously. This is particularly important when services have vastly different user bases (e.g., hundreds of admins versus millions of regular users), as they require varying replication rates for horizontal scaling. Especially if services rely on distinct dependencies, combining them into a single Docker container can result in a large, inefficient image, which increases the services‚Äô startup time.\nTeam Autonomy: Design services such that teams can own and work on individual components independently, thereby reducing communication overhead and speeding up development cycles [10].\n\n\n\nMonitoring and Observability\nTo ensure smooth operation and detect issues proactively, monitoring and observability are essential. Focus on:\n\nSystem Performance: Monitor the ‚Äúgolden signals‚Äù‚Äîlatency, traffic, errors, and saturation of your services. Tools like Prometheus and Grafana are commonly used for this.\nData Quality: Track changes in input data distributions and monitor metrics like model accuracy to detect data drift.\nSynthetic Monitoring: Simulate user behavior to identify bottlenecks and improve responsiveness. Complement this with chaos engineering tools like Chaos Monkey to test your system‚Äôs resilience by deliberately introducing failures, ensuring your infrastructure can handle unexpected disruptions effectively.\nDistributed Tracing: Debug across microservices using tools like Jaeger or OpenTelemetry.\n\nWhen issues arise, having a rollback strategy is crucial. Options include:\n\nReverting to a stable container image.\nRolling back database migrations.\nUsing feature toggles to disable problematic updates.\n\nBy combining robust delivery pipelines, thoughtful architecture, and effective monitoring, teams can ensure that their applications remain reliable, scalable, and adaptable to changing needs.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nWhich additional steps you could take to make your research project production-ready.\n\n\n\n\n\n\n\n[1] Davis C. Cloud Native Patterns: Designing Change-Tolerant Software. Manning Publications (2019).\n\n\n[2] Forsgren N, Humble J, Kim G. Accelerate: The Science of Lean Software and Devops: Building and Scaling High Performing Technology Organizations. IT Revolution (2018).\n\n\n[3] Huyen C. Designing Machine Learning Systems. O‚ÄôReilly Media, Inc. (2022).\n\n\n[4] Khononov V. Learning Domain-Driven Design. O‚ÄôReilly Media, Inc. (2021).\n\n\n[5] Kleppmann M. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O‚ÄôReilly Media, Inc. (2017).\n\n\n[6] Lilienthal C. Sustainable Software Architecture: Analyze and Reduce Technical Debt. dpunkt.verlag (2019).\n\n\n[7] Lilienthal C, Schwentner H. Domain-Driven Transformation: Monolithen Und Microservices Zukunftsf√§hig Machen. dpunkt.verlag (2023).\n\n\n[8] Richards M, Ford N. Fundamentals of Software Architecture: An Engineering Approach. O‚ÄôReilly Media, Inc. (2020).\n\n\n[9] Serra J. Deciphering Data Architectures. O‚ÄôReilly Media, Inc. (2024).\n\n\n[10] Skelton M, Pais M. Team Topologies: Organizing Business and Technology Teams for Fast Flow. IT Revolution (2019).",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_from_research_to_production.html#footnotes",
    "href": "06_from_research_to_production.html#footnotes",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "This use case could also be implemented using an event-driven architecture as discussed in the next section. With this setup, both players would publish their next moves to a message queue and subscribe to it to receive updates about the moves of their opponent.‚Ü©Ô∏é\nTools like Crontab Guru can help configure these schedules.‚Ü©Ô∏é",
    "crumbs": [
      "Writing Code",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "afterword.html",
    "href": "afterword.html",
    "title": "Afterword",
    "section": "",
    "text": "You‚Äôre still at the beginning of your journey towards professional software engineering. But I hope this book could give you a glimpse into what lies ahead.\nIf you want to learn more, I recommend these for your next read:\n\n[2] The Pragmatic Programmer: Your Journey to Mastery by David Thomas and Andrew Hunt (20th Anniversary Edition, 2019) ‚Äì This book expands on many of the coding principles we‚Äôve touched on, offering practical advice for succeeding as a programmer in industry.\n[1] The Algorithm Design Manual by Steven Skiena (2020) ‚Äì A deep dive into algorithms and data structures to sharpen your algorithmic thinking skills.\n\nFinally, I‚Äôm always looking to improve the contents of this book (or any other resources you can find on my website). Therefore, I would be eternally grateful for your feedback‚Äîwhether you just found a typo, you think an explanation is unclear, or there are other topics that you think this book should cover‚Äîplease send me an email to hey@franziskahorn.de and let me know what you think!\n\n\n\n\n[1] Skiena SS. The Algorithm Design Manual. Springer (2020).\n\n\n[2] Thomas D, Hunt A. The Pragmatic Programmer: Your Journey to Mastery, 20th Anniversary Edition. Addison-Wesley Professional (2019).",
    "crumbs": [
      "Afterword"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] Aulet B. Disciplined Entrepreneurship: 24\nSteps to a Successful Startup, Expanded & Updated. John Wiley\n& Sons (2024).\n\n\n[2] Beck K. Tidy First? O‚ÄôReilly Media,\nInc. (2023).\n\n\n[3] Callaway E. Chemistry Nobel Goes to Developers\nof AlphaFold AI That Predicts Protein Structures. Nature\n634(8034), 525‚Äì526 (2024).\n\n\n[4] Christiansen J. Building Science Graphics:\nAn Illustrated Guide to Communicating Science Through Diagrams and\nVisualizations. AK Peters/CRC Press (2022).\n\n\n[5] Davis C. Cloud Native Patterns: Designing\nChange-Tolerant Software. Manning Publications (2019).\n\n\n[6] Dean J, Ghemawat S. MapReduce: Simplified Data\nProcessing on Large Clusters. Communications of the ACM 51(1),\n107‚Äì113 (2008).\n\n\n[7] Foote B, Yoder J. Big Ball of Mud. Pattern\nlanguages of program design 4, 654‚Äì692 (1997).\n\n\n[8] Ford N, Parsons R, Kua P, Sadalage P.\nBuilding Evolutionary Architectures. O‚ÄôReilly Media, Inc.\n(2022).\n\n\n[9] Forsgren N, Humble J, Kim G. Accelerate:\nThe Science of Lean Software and Devops: Building and Scaling High\nPerforming Technology Organizations. IT Revolution (2018).\n\n\n[10] Fowler M. Refactoring: Improving the Design\nof Existing Code. Addison-Wesley Professional (2018).\n\n\n[11] Freiesleben T, Molnar C. Supervised Machine Learning for\nScience: How to Stop Worrying and Love Your Black Box.\n(2024).\n\n\n[12] Helmers L, Horn F, Biegler F, Oppermann T,\nM√ºller K-R. Automating the Search for a Patent‚Äôs Prior Art with a Full\nText Similarity Search. PLoS ONE 14(3), e0212103\n(2019).\n\n\n[13] Horn F. A Practitioner‚Äôs Guide to\nMachine Learning. (2021).\n\n\n[14] Horn F. Similarity Encoder: A Neural\nNetwork Architecture for Learning Similarity Preserving Embeddings.\nTechnische Universitaet Berlin (Germany) (2020).\n\n\n[15] Huyen C. Designing Machine Learning\nSystems. O‚ÄôReilly Media, Inc. (2022).\n\n\n[16] Irving D, Hertweck K, Johnston L, Ostblom J,\nWickham C, Wilson G. Research Software\nEngineering with Python. (2021).\n\n\n[17] Khononov V. Balancing Coupling in Software\nDesign: Universal Design Principles for Architecting Modular Software\nSystems. Addison-Wesley Professional (2024).\n\n\n[18] Khononov V. Learning Domain-Driven\nDesign. O‚ÄôReilly Media, Inc. (2021).\n\n\n[19] Kleppmann M. Designing Data-Intensive\nApplications: The Big Ideas Behind Reliable, Scalable, and Maintainable\nSystems. O‚ÄôReilly Media, Inc. (2017).\n\n\n[20] Knaflic CN. Storytelling with Data: A Data\nVisualization Guide for Business Professionals. John Wiley &\nSons (2015).\n\n\n[21] Lemaire M. Refactoring at Scale.\nO‚ÄôReilly Media, Inc. (2020).\n\n\n[22] Lilienthal C. Sustainable Software\nArchitecture: Analyze and Reduce Technical Debt. dpunkt.verlag\n(2019).\n\n\n[23] Lilienthal C, Schwentner H. Domain-Driven\nTransformation: Monolithen Und Microservices Zukunftsf√§hig Machen.\ndpunkt.verlag (2023).\n\n\n[24] McChrystal GS, Collins T, Silverman D, Fussell\nC. Team of Teams: New Rules of Engagement for a Complex World.\nPenguin (2015).\n\n\n[25] Normand E. Grokking Simplicity: Taming\nComplex Software with Functional Thinking. Manning Publications\n(2021).\n\n\n[26] Ousterhout JK. A Philosophy of Software\nDesign. Yaknyam Press Palo Alto, CA, USA (2018).\n\n\n[27] Richards M, Ford N. Fundamentals of\nSoftware Architecture: An Engineering Approach. O‚ÄôReilly Media,\nInc. (2020).\n\n\n[28] Serra J. Deciphering Data\nArchitectures. O‚ÄôReilly Media, Inc. (2024).\n\n\n[29] Skelton M, Pais M. Team Topologies:\nOrganizing Business and Technology Teams for Fast Flow. IT\nRevolution (2019).\n\n\n[30] Skiena SS. The Algorithm Design\nManual. Springer (2020).\n\n\n[31] Thomas D, Hunt A. The Pragmatic Programmer:\nYour Journey to Mastery, 20th Anniversary Edition. Addison-Wesley\nProfessional (2019).",
    "crumbs": [
      "References"
    ]
  }
]
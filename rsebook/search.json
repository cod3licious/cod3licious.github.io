[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Software Engineering: A Primer",
    "section": "",
    "text": "Preface\nThis book is meant to empower researchers to code with confidence and clarity.\nIf you studied something other than computer science‚Äîespecially in the natural sciences like physics, chemistry, or biology‚Äîit‚Äôs likely you were never taught how to properly develop software. Yet, you‚Äôre often still expected to write code as part of your daily work. Maybe you‚Äôve taken a programming course like Python for Biologists and can put together functional scripts through trial and error (with a little help from ChatGPT). But chances are, no one ever showed you how to write well-structured, maintainable, and reusable code that could make your life‚Äîand collaborating with your colleagues‚Äîso much easier.\nThis book is for you if you want to:\n\nWrite functional software more quickly\nUse a structured approach to design better programs\nReuse your code in future projects\nFeel confident about what your scripts are doing\nPrepare your research code for production\nShare your work with pride.\n\nWhether you‚Äôre just beginning your scientific journey‚Äîperhaps working on your first major project like a master‚Äôs thesis or your first paper‚Äîor you‚Äôre contemplating a move from academia to industry, the practical advice in this book can guide you along the way.\nWhile the book contains some example code in Python to illustrate the concepts, the general ideas are independent of any programming language.\nThis is still a draft version! Please write me an email, if you have any suggestions for how this book could be improved!\nEnjoy! üòä\n\nAcknowledgments\nThe texts in this book were edited and refined with the help of ChatGPT, however, all original content is my own.\n\n\nHow to cite\n@book{horn2025rseprimer,\n  author = {Horn, Franziska},\n  title = {Research Software Engineering: A Primer},\n  year = {2025},\n  url = {https://franziskahorn.de/rsebook/},\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_purpose.html",
    "href": "01_purpose.html",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Types of Analyses\nBefore writing your first line of code, it‚Äôs crucial to have a clear understanding of what you‚Äôre trying to achieve‚Äîspecifically, the purpose of your research. This clarity will not only help you reach your desired outcomes more efficiently but will also be invaluable when collaborating with others. Being able to explain your goals effectively ensures everyone is aligned and working toward the same objective.\nWe‚Äôll begin with an overview of common research goals and the types of data analysis needed to achieve them. Then, we‚Äôll explore how to visually communicate your research purpose, as visual representations are often the most effective way to convey complex ideas. Finally, we‚Äôll discuss how to quantify the outcomes you‚Äôre trying to achieve.\nMost research questions can be categorized into four broad groups, each associated with a specific type of analytics approach (Figure¬†1.1).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#types-of-analyses",
    "href": "01_purpose.html#types-of-analyses",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Figure¬†1.1: Descriptive, diagnostic, predictive, and prescriptive analytics, with increasing computational complexity and need to write custom code.\n\n\n\n\nDescriptive Analytics\nThis approach focuses on observing and describing phenomena, often for the first time. Examples include:\n\nIdentifying animal and plant species in unexplored regions of the deep ocean.\nMeasuring the physical properties of a newly discovered material.\nSurveying the political views of the next generation of teenagers.\n\nMethodology:\n\nCollect a large amount of data (e.g., samples or observations).\nCalculate summary statistics like averages, ranges, or standard deviations, typically using standard software tools.\n\n\n\nDiagnostic Analytics\nHere, the goal is to understand relationships between variables and uncover causal chains to explain why phenomena occur. Examples include:\n\nInvestigating how CO2 emissions from burning fossil fuels drive global warming.\nEvaluating whether a new drug reduces symptoms and under what conditions it works best.\nExploring how economic and social factors influence shifts toward right-wing political parties.\n\nMethodology:\n\nPerform exploratory data analysis, such as looking for correlations between variables.\nConduct statistical tests to support or refute hypotheses (e.g., comparing treatment and placebo groups).\nDesign of experiments to control for external factors (e.g., randomized clinical trials).\nBuild predictive models to simulate relationships. If these models match real-world observations, it suggests their assumptions correctly represent causal effects.\n\n\n\nPredictive Analytics\nThis method involves building models to describe and predict relationships between independent variables (inputs) and dependent variables (outputs). These models often rely on insights from diagnostic analytics, such as which variables to include in the model and how they might interact (e.g., linear or nonlinear dependence). Examples include:\n\nWeather forecasting models.\nDigital twin of a wind turbine to simulate how much energy is generated under different conditions.\nPredicting protein folding based on amino acid sequences.\n\nMethodology:\nThe key difference lies in how much domain knowledge informs the model:\n\nWhite-box (mechanistic) models: Based entirely on known principles, such as physical laws or experimental findings. These models are often manually designed, with parameters fitted to observed data.\nBlack-box (data-driven) models: Derived purely from observational data. Researchers usually test different model types (e.g., neural networks or Gaussian processes) and choose the one with the highest accuracy.\nGray-box (hybrid) models: These combine mechanistic and data-driven approaches. For example, the output of a mechanistic model may serve as an input to a data-driven model, or the data-driven model may predict residuals (i.e., prediction errors) from the mechanistic model, where both outputs combined yield the final prediction.\n\n\n\n\n\n\n\nResources to learn more about data-driven models\n\n\n\n\n\nIf you want to learn more about how to create data-driven models and the machine learning (ML) algorithms behind them, these two free online books are highly recommended:\n\n[1] Supervised Machine Learning for Science by Christoph Molnar & Timo Freiesleben; A fantastic introduction focused on applying black-box models in scientific research.\n[2] A Practitioner‚Äôs Guide to Machine Learning by me; A broader overview of ML methods for a variety of use cases.\n\n\n\n\nAfter developing an accurate model, researchers can analyze its behavior (e.g., through a sensitivity analysis, which examines how outputs change with varying inputs) to gain further insights about the system (to feed back into diagnostic analytics).\n\n\nPrescriptive Analytics\nThis approach focuses on decision-making and optimization, often using predictive models. Examples include:\n\nScreening thousands of drug candidates to find those most likely to bind with a target protein.\nOptimizing reactor conditions to maximize yield while minimizing energy consumption.\n\nMethodology:\n\nDecision support: Use models for ‚Äúwhat-if‚Äù analyses to predict outcomes of different scenarios. For example, models can estimate the effects of limiting global warming to 2¬∞C versus exceeding that threshold, thereby informing policy decisions.\nDecision automation: Use models in optimization loops to systematically test input conditions, evaluate outcomes (e.g., resulting predicted material quality), and identify the best conditions automatically.\n\nPlease note: These recommendations are only as good as the underlying models. Models must accurately capture causal relationships and often need to extrapolate beyond the data used to build them (e.g., for disaster simulations). Data-driven models are typically better at interpolation (predicting within known data ranges), so results should ideally be validated through additional experiments, such as testing the recommended new materials in the lab.\nTogether, these four types of analytics form a powerful toolkit for tackling real-world challenges: descriptive analytics provides a foundation of understanding, diagnostic analytics uncovers the causes behind observed phenomena, predictive analytics models future scenarios based on this understanding, and prescriptive analytics turns these insights into actionable solutions. Each step builds on the previous one, creating a systematic approach to answering complex questions and making informed decisions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#draw-your-why",
    "href": "01_purpose.html#draw-your-why",
    "title": "1¬† Research Purpose",
    "section": "Draw your Why",
    "text": "Draw your Why\nIn research, your goal is to improve the status quo, whether by filling a knowledge gap or developing a new method, material, or process with better properties. When sharing your idea‚Äîwith collaborators, in a talk, or through a publication‚Äîa visual representation of what you‚Äôre trying to achieve can be incredibly useful.\nOne effective way to illustrate the improvement you‚Äôre working on is by creating ‚Äúbefore and after‚Äù visuals, depicting the problem with the status quo and your solution (Figure¬†1.2).\nThe ‚Äúbefore‚Äù scenario might show a lack of data, an incomplete understanding of a phenomenon, poor model performance, or an inefficient process or material. The ‚Äúafter‚Äù scenario highlights how your research addresses these issues and improves on the current state, such as refining a predictive model or enhancing the properties of a new material.\n\n\n\n\n\n\nFigure¬†1.2: Exemplary research goals and corresponding ‚Äúbefore and after‚Äù visuals for descriptive, diagnostic, predictive, and prescriptive analytics tasks.\n\n\n\nAt this point, your ‚Äúafter‚Äù scenario might be based on a hypothesis or an educated guess about what your results will look like‚Äîand that‚Äôs totally fine! The purpose of visualizing your goal is to guide your development process. Later, you can update the picture with actual results if you decide to include it in a journal publication, for example.\nOf course, not all research goals are tied directly to analytics. Sometimes the main improvement is more qualitative, for example, focusing on design or functionality (Figure¬†1.3). Even in these cases, however, you‚Äôll often need to demonstrate that your new approach meets or exceeds existing solutions in terms of other key performance indicators (KPIs), such as energy efficiency, speed, or quality parameters like strength or durability.\n\n\n\n\n\n\nFigure¬†1.3: This example illustrates a task where a robot must reach its target (represented by money) as efficiently as possible. Original approach (left): The robot relied on information encoded in the environment as expected rewards. To determine the shortest path to the target, the robot required a large sensor (shown as a yellow circle) capable of scanning nearby fields to locate the highest reward. New approach (right): Instead of relying on reward values scattered across the environment, the optimal direction is now encoded directly in the current field. This eliminates the need for large sensors, as the robot only needs to read the value of its current position, enabling it to operate with a much smaller sensor and thereby reducing hardware costs. Additional quantitative evaluation: It still needs to be demonstrated that with the new approach, the robot reaches is target at least as quickly as with the original approach.\n\n\n\nGive it a try‚Äîdoes the sketch help you explain your research to your family?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#evaluation-metrics",
    "href": "01_purpose.html#evaluation-metrics",
    "title": "1¬† Research Purpose",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nThe ‚Äúbefore and after‚Äù visuals help illustrate the improvement you‚Äôre aiming for in a qualitative way. However, to make a compelling case, it‚Äôs important to back up your findings with quantifiable results that show the extent of your improvement. Common evaluation metrics include:\n\nNumber of samples: This refers to the amount of data you‚Äôve collected, such as whether you surveyed 100 or 10,000 people. Larger sample sizes can provide more robust and reliable results.\nReliability of measurements: This evaluates the consistency of your data. For example, how much variation occurs if you repeat the same measurement, e.g., run a simulation with different random seeds. Other factors, like sampling bias (i.e., when your sample is not representative for the whole population), can also affect the validity of your conclusions.\nStatistical significance: The outcome of a statistical hypothesis test, such as a p-value that indicates whether the difference in symptom reduction between the treatment and placebo groups is significant.\nModel accuracy: This measures how well your model predicts or matches new observational data. Common metrics include \\(R^2\\), which indicates how closely the model‚Äôs predictions align with actual outcomes.\nAlgorithm performance: This includes metrics like memory usage and the time required to fit a model or make predictions, and how these values change as the dataset size increases. Efficient algorithms are crucial when scaling to large datasets or handling complex simulations.\nKey Performance Indicators (KPIs): These are specific metrics tied to the success of your optimized process or product. For example, KPIs might include yield, emissions, energy efficiency, or quality parameters like durability, strength, or purity of a chemical compound.\nConvergence time: This refers to how quickly a process (or simulation thereof) reaches optimal results and stabilizes without fluctuating. A shorter convergence time often suggests a more efficient and reliable process.\n\nYour evaluation typically involves multiple metrics. For example, in prescriptive analytics, you need to demonstrate both the accuracy of your model and that the recommendations generated with it led to a genuinely optimized process or product.\nIdeally, you should already have an idea of how existing solutions perform on these metrics (e.g., based on findings from other publications) to establish the baseline your solution should outperform (i.e., your ‚Äúbefore‚Äù). You‚Äôll likely need to replicate at least some of these baseline results (e.g., by reimplementing existing models) to ensure your comparisons are not influenced by external factors. But understanding where the ‚Äúcompetition‚Äù stands can also help you identify secondary metrics where your solution could excel. For example, even if there‚Äôs little room to improve model accuracy, existing solutions might be too slow to handle large datasets efficiently.1\nThese results are central to your research (and publications), and much of your code will be devoted to generating them, along with the models and simulations behind them. Clearly defining the key metrics needed to demonstrate your research‚Äôs impact will help you focus your programming efforts effectively.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe problem you‚Äôre trying to solve.\nExisting solutions to this problem, i.e., the baseline you‚Äôre competing against.\nWhich metrics should be used to quantify your improvement on the current state.\n\n\n\n\n\n\n\n1. Freiesleben T, Molnar C (2024) Supervised machine learning for science: How to stop worrying and love your black box.\n\n\n2. Horn F (2021) A practitioner‚Äôs guide to machine learning.\n\n\n3. Callaway E (2024) Chemistry nobel goes to developers of AlphaFold AI that predicts protein structures. Nature 634: 525‚Äì526.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#footnotes",
    "href": "01_purpose.html#footnotes",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "For example, currently, a lot of research aims to replace traditional mechanistic models with data-driven machine learning models, as these enable significantly faster simulations. A notable example is the AlphaFold model, which predicts protein folding from amino acid sequences‚Äîa breakthrough so impactful it was recognized with a Nobel Prize [3].‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "02_data.html",
    "href": "02_data.html",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Data Types\nIn the previous chapter, we‚Äôve gained clarity on the problem you‚Äôre trying to solve and how to quantify the improvements your research generates. Now it‚Äôs time to dive deeper into what these results might actually look like and the data on which they are built.\nIn one form or another, you‚Äôre research will rely on data, both collected or generated by yourself and possibly others.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-types",
    "href": "02_data.html#data-types",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Structured vs.¬†Unstructured Data\nData can take many forms, but one key distinction is between structured and unstructured data (Figure¬†2.1).\n\n\n\n\n\n\nFigure¬†2.1: Structured and unstructured data.\n\n\n\nStructured data is organized in rows and columns, like in Excel spreadsheets, CSV files, or relational databases. Each row represents a sample or observation (a data point), while each column corresponds to a variable or measurement (e.g., temperature, pressure, household income, number of children).\nUnstructured data, in contrast, lacks a predefined structure. Examples include images, text, audio recordings, and videos, typically stored as separate files on a computer or in the cloud. While these files might include structured metadata (e.g., timestamps, camera settings), the data content itself can vary widely‚Äîfor instance, audio recordings can range from seconds to hours in length.\nStructured data is often heterogeneous, meaning it includes variables representing different kinds of information with distinct units or scales (e.g., temperature in ¬∞C and pressure in kPa). Unstructured data tends to be homogeneous; for example, there‚Äôs no inherent difference between one pixel and the next in an image.\n\n\n\n\n\n\nNote\n\n\n\nEven though unstructured data is common in science (e.g., microscopy images), for simplicity, this book focuses on structured data. Furthermore, for now we‚Äôll assume that your data is stored in an Excel or CSV file, i.e., a spreadsheet with rows (samples) and columns (variables), on your computer. Later in Chapter 6, we‚Äôll discuss more advanced options for storing and accessing data, such as databases and APIs.\n\n\n\n\nProgramming Data Types\nEach variable in your dataset (i.e., each column in your spreadsheet) is represented as a specific data type, such as:\n\nNumbers (integers for whole numbers or floats for decimals)\nStrings (text)\nBoolean values (true/false)\n\nIn programming, these are so-called primitive data types (as opposed to composite types, like arrays or dictionaries containing multiple values, or user-defined objects) and define how information is stored in computer memory.\n\n\n\n\n\n\nData types in Python\n\n\n\n\n\n# integer\ni = 42\n# float\nx = 4.1083\n# string\ns = \"hello world!\"\n# boolean\nb = False\n\n\n\n\n\nStatistical Data Types\nEven more important than how your data is stored, is understanding what your data means. Variables fall into two main categories:\n\nContinuous (numerical) variables represent measurable values (e.g., temperature, height). These are usually stored as floats or integers.\nDiscrete (categorical) variables represent distinct options or groups (e.g., nationality, product type). These are often stored as strings, booleans, or sometimes integers.\n\n\n\n\n\n\n\nMisleading data types\n\n\n\nBe cautious: a variable that looks numerical (e.g., 1, 2, 3) may actually represent categories. For example, a material_type column with values 1, 2, and 3 might correspond to aluminum, copper, and steel, respectively. In this case, the numbers are IDs, not quantities.\n\n\nRecognizing whether a variable is continuous or discrete is crucial for creating meaningful visualizations and using appropriate statistical models.\n\n\nTime Series Data\nAnother consideration is whether your data points are linked by time. Time series data often refers to numerical data collected over time, like temperature readings or sales numbers. These datasets are usually expected to exhibit seasonal patterns or trends over time.\nHowever, nearly all datasets involve some element of time. For example, if your dataset consists of photos, timestamps might seem unimportant, but they could reveal trends‚Äîlike changes in image quality due to new equipment.\n\n\n\n\n\n\nImportant\n\n\n\nAlways include timestamps in your data or metadata to help identify potential correlations or unexpected trends over time.\n\n\nSometimes, you may be able to collect truly time-independent data (e.g., sending a survey to 1,000 people simultaneously and they all answer within the next 10 minutes). But usually, your data collection will take longer and external factors‚Äîlike an election during a longer survey period‚Äîmight unintentionally affect your results. By tracking time, you can assess and adjust for such influences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-analysis-results",
    "href": "02_data.html#data-analysis-results",
    "title": "2¬† Data & Results",
    "section": "Data Analysis Results",
    "text": "Data Analysis Results\nWhen analyzing data, the process is typically divided into two phases:\n\nExploratory Analysis: This involves generating a variety of plots to gain a deeper understanding of your data, such as identifying correlations between variables. It‚Äôs often a quick and dirty process to help you familiarize yourself with the dataset.\nExplanatory Analysis: This focuses on creating refined, polished plots intended for communicating your findings, such as in a publication or presentation. These visuals are designed to clearly convey your results to an audience that may not be familiar with your data.\n\n\nExploratory Analysis\nIn this initial analysis, the goal is to get acquainted with the data, check if the trends and relationships you anticipated are present, and uncover any unexpected patterns or insights.\n\nExamine the raw data:\n\nIs the dataset complete, i.e., does it contain all the variables and samples you expected?\n\nExamine summary statistics (e.g., mean, standard deviation (std), min/max values, missing value count, etc.):\n\nWhat does each variable mean? Given your understanding of the variable, are its values in a reasonable range?\nAre missing values encoded as NaN (Not a Number) or as ‚Äòunrealistic‚Äô numeric values (e.g., -1 while normal values are between 0 and 100)?\nAre missing values random or systematic (e.g., in a survey rich people are less likely to answer questions about their income or specific measurements are only collected under certain conditions)? This can influence how missing values should be handled, e.g., whether it makes sense to impute them with the mean or some other specific value (e.g., zero).\n\nExamine the distributions of individual (continuous) variables:\n\n\n\nHistogram, strip plot, violin plot, box plot, and summary statistics of the same values.\n\n\n\nAre there any outliers? Are these genuine edge cases or can they be ignored (e.g., due to measurement errors or wrongly encoded data)?\nIs the data normally distributed or does the plot show multiple peaks? Is this expected?\n\nExamine trends over time (by plotting variables over time, even if you don‚Äôt think your data has a meaningful time component, e.g., by lining up representative images according to their timestamps to see if there is a pattern):\n\n\n\nWhat caused these trends and what are their implications for the future? This plot shows fictitious data of the pressure in a pipe affected by fouling‚Äîthat is, a buildup of unwanted material on the pipe‚Äôs surface, leading to increased pressure. The pipe is cleaned at regular intervals, causing a drop in pressure. However, because the cleaning process is imperfect, the baseline pressure gradually shifts upward over time.\n\n\n\nAre there time periods where the data was sampled irregularly or samples are missing? Why?\nAre there any (gradual or sudden) data drifts over time? Are these genuine changes (e.g., due to changes in the raw materials used in the process) or artifacts (e.g., due to a malfunctioning sensor recording wrong values)?\n\nExamine relationships between two variables:\n\n\n\nDepending on the variables‚Äô types (continuous or discrete), relationships can be shown in scatter plots, box plots, or a table. Please note that not all interesting relations between the two variables can be detected through a high correlation coefficient, so you should always check the scatter plot for details.\n\n\n\nAre the observed correlations between variables expected?\n\nExamine patterns in multidimensional data (using a parallel coordinate plot):\n\n\n\nEach line in a parallel coordinate plot represents one data point, with the corresponding values for the different variables marked at the respective y-axis. The screenshot here shows an interactive plot created using the Python plotly library. By selecting value ranges for the different dimensions (indicated by the pink stripes), it is possible to spot interesting patterns resulting from a combination of values across multiple variables.\n\n\n\nDo the observed patterns in the data match your understanding of the problem and dataset?\n\n\n\n\nExplanatory Analysis\nMost of the plots you create during an exploratory analysis are likely for your eyes only. Any plots you do choose to share with a broader audience‚Äîsuch as in a paper or presentation‚Äîshould be refined to clearly communicate your findings. Since your audience is much less familiar with the data and likely lacks the time or interest to explore it in depth, it‚Äôs essential to make your results more accessible. This process is often referred to as explanatory analysis [1].\n\n\n\n\n\n\nWarning\n\n\n\nDon‚Äôt ‚Äújust show all the data‚Äù and hope that your audience will make something of it‚Äîunderstand what they need to answer the questions they have.\n\n\n\nStep 1: Choose the right plot type\n\nGet inspired by visualization libraries (e.g., here or here), but avoid the urge to create fancy graphics; sticking with common visualizations makes it easier for the audience to correctly decode the presented information.\nDon‚Äôt use 3D effects!\nAvoid pie or donut charts (angles are hard to interpret).\nUse line plots for time series data.\nUse horizontal instead of vertical bar charts for audiences that read left to right.\nStart the y-axis at 0 for area & bar charts.\nConsider using small multiples or sparklines instead of cramming too much into a single chart.\n\n\n\n\nLeft: Bar charts (especially in 3D) make it hard to compare numbers over a longer period of time. Right: Trends over time can be more easily detected in line charts. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 2: Cut clutter / maximize data-to-ink ratio\n\nRemove border.\nRemove gridlines.\nRemove data markers.\nClean up axis labels.\nLabel data directly.\n\n\n\n\nCut clutter! [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 3: Focus attention\n\nStart with gray, i.e., push everything in the background.\nUse pre-attentive attributes like color strategically to highlight what‚Äôs most important.\nUse data labels sparingly.\n\n\n\n\nStart with gray and use pre-attentive attributes strategically to focus the audience‚Äôs attention. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 4: Make data accessible\n\nAdd context: Which values are good (goal state), which are bad (alert threshold)? Should the value be compared to another variable (e.g., actual vs.¬†forecast)?\nLeverage consistent colors when information is spread across multiple plots (e.g., data from a certain country is always drawn in the same color).\nAnnotate the plot with text explaining the main takeaways (if this is not possible, e.g., in interactive dashboards where the data keeps changing, the title can instead include the question that the plot should answer, e.g., ‚ÄúIs the material quality on target?‚Äù).\n\n\n\n\nTell a story. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#draw-your-what",
    "href": "02_data.html#draw-your-what",
    "title": "2¬† Data & Results",
    "section": "Draw your What",
    "text": "Draw your What\nYou might not have looked at your data yet‚Äîor maybe you haven‚Äôt even collected it‚Äîbut it‚Äôs important to start programming with the end in mind. Think about how, in software development, a UX designer typically creates mockups of a user interface (like the screens of a mobile app) before developers begin coding. Similarly, in our case, we want to start with a clear picture of what the output of our program should look like. The difference is that, instead of users interacting with the software themselves, they‚Äôll only see the plots or tables that your program generated, maybe in a journal article.1\nBased on your takeaways from the previous chapter‚Äîabout the problem you‚Äôre solving and the metrics you should use to evaluate your solution‚Äîtry sketching what your final results might look like. Ask yourself: What figures or tables would best communicate the advantages of my approach?\nDepending on your research goals, your results might be as simple as a single number, such as a p-value or the total number of people surveyed. However, if you‚Äôre reading this, you‚Äôre likely tackling something that requires a more complex analysis. For example, you might compare your solution‚Äôs overall performance to several baseline approaches or illustrate how your solution converges over time (Figure¬†2.2).\n\n\n\n\n\n\nFigure¬†2.2: Exemplary envisioned results: The plots show the outcome of a multi-agent simulation, where ‚Äòmy approach‚Äô clearly outperforms two baseline methods. In this simulation, a group of agents is tasked with locating a food source and transporting the food back to their home base piece by piece. The ideal algorithm identifies the shortest path to the food source quickly to maximize food collection. Each algorithmic approach was tested 10 times using different random seeds to evaluate reliability. The plots display the mean and standard deviations across these runs. Left: How quickly each algorithm converges to the shortest path (resulting in the highest number of agents delivering food back to the home base per step). Right: Cumulative food collected by the end of the simulation.\n\n\n\nIt‚Äôs important to remember that your actual results might look very different from your initial sketches‚Äîthey might even show that your solution performs worse than the baseline. This is completely normal. The scientific method is inherently iterative, and unexpected results are often a stepping stone to deeper understanding. By starting with a clear plan, you can generate results more efficiently and quickly pivot to a new hypothesis if needed. When your results deviate from your expectations, analyzing those differences can sharpen your intuition about the data and help you form better hypotheses in the future.\nOnce you‚Äôve visualized the results you want, work backward to figure out what data you need to create them. This is especially important when you‚Äôre generating the data yourself, such as through simulations. For instance, if you plan to plot how values change over time, you‚Äôll need to record variables at every time step rather than just saving the final outcome of a simulation (duh!). Similarly, if you want to report your model‚Äôs accuracy, you‚Äôll need (Figure¬†2.3):\n\nInput variables for each data point to generate predictions (= model output).\nThe actual (true) values for each data point.\nA way to compute the overall deviation between predictions and true values, such as using an evaluation metric like \\(R^2\\).\n\n\n\n\n\n\n\nFigure¬†2.3: Work backwards from the desired results to determine what data is necessary to create them.\n\n\n\nBy working backward from your desired results to the required data, you can design your code and analysis pipeline to ensure your program delivers exactly what you need.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe specific results (tables and figures) you want to create to show how your solution outperforms existing approaches (e.g., in terms of accuracy, speed, etc.).\nThe underlying data needed to produce these results (e.g., what rows and columns should be in your spreadsheet).\n\n\n\n\n\n\n\n1. Knaflic CN (2015) Storytelling with data: A data visualization guide for business professionals, John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#footnotes",
    "href": "02_data.html#footnotes",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "A former master‚Äôs student that I mentored humorously called this approach ‚Äúplot-driven development,‚Äù a nod to test-driven development (TDD) in software engineering, where you write a test for your function first and then implement the function to pass the test. You could even use these sketches of your results as placeholders if you‚Äôre already drafting a paper or presentation.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "03_tools.html",
    "href": "03_tools.html",
    "title": "3¬† Tools",
    "section": "",
    "text": "Programming languages\nBefore we continue with creating your results - i.e., actually start with the programming part - here is a quick overview of the tools that could come in handy during your software engineering journey.\nWhile the examples will use the Python programming language, the general principles discussed in the book are equally applicable to other programming languages.\nchoose the one that is most common in your field - you‚Äôll find more relevant resources and libraries that will come in handy and it‚Äôs easier to collaborate with co-workers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#programming-languages",
    "href": "03_tools.html#programming-languages",
    "title": "3¬† Tools",
    "section": "",
    "text": "Before you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to set up your development environment to code efficiently.\nThe fundamental syntax of your programming language of choice (incl.¬†key scientific libraries) to get started.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Horn\nF (2021) A practitioner‚Äôs\nguide to machine learning.\n\n\n2. Freiesleben T, Molnar C (2024) Supervised machine learning for\nscience: How to stop worrying and love your black box.\n\n\n3. Callaway E (2024) Chemistry nobel goes to\ndevelopers of AlphaFold AI that predicts protein structures.\nNature 634: 525‚Äì526.\n\n\n4. Knaflic CN (2015) Storytelling with data: A\ndata visualization guide for business professionals, John Wiley &\nSons.",
    "crumbs": [
      "References"
    ]
  }
]
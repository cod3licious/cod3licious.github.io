[
  {
    "objectID": "interaction_flow_mapping.html",
    "href": "interaction_flow_mapping.html",
    "title": "InterAction Flow Mapping",
    "section": "",
    "text": "Organizations run on processes. Most of them are messy, span multiple roles, combine human judgment with software automation, and exist partly in people’s heads. Improving these processes—by simplifying them, automating parts of them, or supporting steps with software—requires a shared understanding of how they actually work today and how they should work in the future.\nThis is especially important when building software. Software does not exist in isolation: it is embedded in human workflows, exchanges data with other systems, depends on user inputs, and introduces new constraints. When developers, domain experts, and stakeholders carry different mental models of the same process, the result is friction, rework, and the classic “that’s not what I meant” conversation.\nSo this is where visual models come in: A diagram that depicts the sequence of steps over time and shows the interactions and handoffs between different actors provides far more clarity than a verbal description of a process.\nThe effect is even stronger when that diagram is created collaboratively. In a collaborative modeling (CoMo) session, the relevant people jointly build a visual model of the process, discuss it, and question it. The diagram becomes a shared thinking aid: assumptions are made explicit, gaps become visible, and the group gains a concrete reference point for reasoning about change.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#creating-shared-understanding",
    "href": "interaction_flow_mapping.html#creating-shared-understanding",
    "title": "InterAction Flow Mapping",
    "section": "",
    "text": "Organizations run on processes. Most of them are messy, span multiple roles, combine human judgment with software automation, and exist partly in people’s heads. Improving these processes—by simplifying them, automating parts of them, or supporting steps with software—requires a shared understanding of how they actually work today and how they should work in the future.\nThis is especially important when building software. Software does not exist in isolation: it is embedded in human workflows, exchanges data with other systems, depends on user inputs, and introduces new constraints. When developers, domain experts, and stakeholders carry different mental models of the same process, the result is friction, rework, and the classic “that’s not what I meant” conversation.\nSo this is where visual models come in: A diagram that depicts the sequence of steps over time and shows the interactions and handoffs between different actors provides far more clarity than a verbal description of a process.\nThe effect is even stronger when that diagram is created collaboratively. In a collaborative modeling (CoMo) session, the relevant people jointly build a visual model of the process, discuss it, and question it. The diagram becomes a shared thinking aid: assumptions are made explicit, gaps become visible, and the group gains a concrete reference point for reasoning about change.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#why-another-model",
    "href": "interaction_flow_mapping.html#why-another-model",
    "title": "InterAction Flow Mapping",
    "section": "Why Another Model?",
    "text": "Why Another Model?\nYes, there already exist plenty of modeling notations and workshop formats that have proven their value in many contexts (check out the book Visual Collaboration Tools for an overview). This article is not an argument against any of these tools.\nStill, despite being a great source of inspiration, somehow none of these methods felt exactly right for what I was trying to accomplish when creating the diagrams for my latest book. So, just in case I’m not the only one out there yearning for an alternative approach, I wanted to share my notation and the ideas behind it in more detail.\n\nModeling from Different Perspectives\nLet’s start by acknowledging that systems can be modeled from different perspectives (Figure 1).\nOne perspective focuses on a system’s structure: how individual elements are organized and composed into larger subsystems (often forming bottom-up in natural systems). Examples include:\n\nCells → Organs → Human body\nEmployees → Teams → Departments → Company\nLines of code → Functions and classes → Modules (files and folders) → Repository or deployable service\n\nTools such as Context Mapping and the C4 Model help describe the structural perspective of (software) systems.\nAnother perspective focuses on behavior: what the system does over time. Examples include:\n\nDigesting food\nOnboarding a new employee\nA user interacting with a mobile app to accomplish a task\n\nThese behaviors can be described as processes, which are the focus of this article.\nProcesses themselves can be examined at different levels of granularity. Here, we deliberately stay at a higher level of abstraction.\nTechniques such as Example Mapping are better suited for zooming in further and uncovering the detailed business rules that govern individual steps.\n\n\n\n\n\n\nFigure 1: A system has a structure, defined by how individual elements interact to form subsystems, and exhibits behaviors that can be described as processes. Zooming into a process step reveals business rules and logic, where individual operations, decision points, and loops detail how each step is executed.\n\n\n\n\n\nProcess Modeling\nExisting tools to model processes all optimize for slightly different goals, for example:\n\nUML and BPMN are well-established standards for modeling software behavior and business processes. They allow for very detailed and precise diagrams, but their specialized notation often makes the resulting models difficult to grasp for non-experts. They were also primarily intended for documentation rather than collaborative exploration.\nEventStorming is a popular collaborative modeling technique that emphasizes accessibility by using plain-language domain events. It works particularly well for discovering domain concepts and behavior, especially in unfamiliar problem spaces. However, the initial chaotic exploration phase can quickly become visually overwhelming, and as more structure and technical details are added, the resulting model often becomes difficult to follow. Additionally, expressing events in the past tense is unintuitive for many participants.\nDomain Storytelling focuses on interactions between actors and objects and produces diagrams that are easy to understand and discuss. While effective for creating a shared overview, the temporal sequence of a process is represented indirectly through numbered arrows, which can make it harder to perceive the overall flow at a glance in more complex scenarios.\n\nInterAction Flow Mapping is not supposed to replace these approaches. Instead, it fills a specific gap by combining the simplicity of Domain Storytelling with the precision of UML:\n\nA simple diagram that is easy to understand, even for non-technical domain experts.\nA clear view of the process over time, with explicit separation of the different actors involved.\nA notation that supports direct translation of the modeled process into a software implementation.\n\nThe technique that comes closest to InterAction Flow Mapping is Event Modeling. It visualizes wireframes alongside read and write operations that aggregate event-based data from multiple sources. The resulting diagrams are easy for domain experts to follow while still providing a rough implementation blueprint for developers. In practice, however, I found these models map more naturally to event sourcing architectures and often require an extra translation step when implementing a process with a simpler CRUD-based design.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#the-interaction-flow-notation",
    "href": "interaction_flow_mapping.html#the-interaction-flow-notation",
    "title": "InterAction Flow Mapping",
    "section": "The InterAction Flow Notation",
    "text": "The InterAction Flow Notation\nAn InterAction Flow models a process as a sequence of interactions between actors. It can be used to visualize both analog processes as well as software behavior.\nThe modeling canvas is simple:\n\nThe horizontal axis represents time.\nThe vertical axis represents actors in swimlanes—typically starting with a user or customer at the top (who initiates the process), followed by software modules or organizational units, and ending with data stores or records at the bottom.\n\nThe visual elements are intentionally minimal (Figure 2):\n\nAction boxes describe what an actor does, labeled with a short phrase in present tense starting with a verb (e.g., verify credentials). These actions should be meaningful at the business level and plausibly map to a function, API endpoint, or responsibility in software.\nStart and end markers indicate where the process begins and terminates. These borrow from BPMN but are optional—feel free to use normal stickies in a CoMo session.\nArrows connect actions across swimlanes and are labeled with the information being exchanged. In software terms, these often correspond to input parameters and return values.\nResource icons indicate where data is read or written during an action.1\n\n\n\n\n\n\n\nFigure 2: Visual elements used in an InterAction Flow diagram. Essentially, it’s all about boxes (actions) and arrows (information).\n\n\n\n\nInterAction Flows in Action\nA simple example of an analog process—reserving a hotel room by phone—modeled as an InterAction Flow is shown in Figure 3, while the corresponding software-enabled process is shown in Figure 4.\n\n\n\n\n\n\nFigure 3: Analog process to reserve a hotel room: the guest calls the hotel, tells the manager the dates and number of guests, the hotel manager checks which types of rooms are available during this time, and the guest decides which room to reserve.\n\n\n\n\n\n\n\n\n\nFigure 4: Software-enabled process to reserve a hotel room: the user opens the hotel’s website, enters the dates and number of guests, sees which types of rooms are available during this time, and then proceeds to reserve one of them. Some actions happen in parallel (for example, the user entering data while the website is being displayed). Not all arrows are annotated with exchanged information: some operations require no additional input (e.g., a GET request without parameters), while other information is transmitted indirectly, such as via the rendered web page.\n\n\n\nIt is also possible to model multiple subactions in sequence for a single actor when a particular step is especially complex and decomposing it adds clarity (Figure 5).\n\n\n\n\n\n\nFigure 5: Process excerpt showing a conversation with an AI agent, with the goal of finding a hotel room: the user submits a prompt; the Large Language Model (LLM) powering the agent analyzes it, determines which tools to use to gather the required information, calls those tools (in this case, an external API for hotel search available via MCP), and then synthesizes a response from the results. Since this process step is more complex, it makes sense to spell out the individual subactions explicitly—while avoiding too much detail, so the overall process still remains understandable at a high level. In addition to the final output, the LLM also provides updates (explaining its “thinking”), which do not result in a user action.\n\n\n\n\n\n\n\n\n\nTipTime-triggered processes\n\n\n\nIf your process is not triggered by an external event but rather runs on a schedule, simply put the corresponding time in the circle denoting the start of the process.\n\n\n\n\n\n\n\n\nNoteWhat about decision points and branches?\n\n\n\nIf you feel compelled to add conditionals or loops to your flow, this is often a signal to pause and reconsider the level of abstraction. A lot of branching behavior happens inside an action and does not need to be visible at the overview level.\nWhen branching paths are central to the discussion, start by modeling the happy path and add annotations where alternatives exist, or create a separate diagram that starts at the branching point to explore the alternative flow. In some cases, it is also possible to describe different paths using separate swimlanes (Figure 6).\n\n\n\n\n\n\nFigure 6: Using separate swimlanes (and color coding) to represent branching at a decision point: a smartwatch analyzes collected sensor measurements every five minutes to detect movement patterns, stress, and other conditions. When an internet connection is available, the data is sent to a cloud-based machine learning (ML) model for analysis; otherwise, a less accurate model running on the device is used.\n\n\n\n\n\n\n\nFrom Diagram to Code\nInterAction Flows translate naturally into software design:\n\nAction boxes often map directly to function calls or API endpoints.\nArrow labels make the required data contracts explicit.\nSwimlanes highlight ownership and boundaries between systems or teams.\nResource icons encourage early discussion about persistence and data ownership.\n\nWhen co-creating an InterAction Flow in a collaborative InterAction Flow Mapping session, it often makes sense to keep the diagram more abstract, as shown in the previous figures. That said, if you want to use an InterAction Flow as a blueprint for implementation, you can easily include additional technical details and wireframes to make the intended software behavior more explicit (Figure 7).\n\n\n\n\n\n\nFigure 7: Software-enabled process to book a hotel room with more details, including wireframes, the API endpoints that are called, and an external API used to process the payment.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#interaction-flow-mapping-workshops",
    "href": "interaction_flow_mapping.html#interaction-flow-mapping-workshops",
    "title": "InterAction Flow Mapping",
    "section": "InterAction Flow Mapping Workshops",
    "text": "InterAction Flow Mapping Workshops\nI propose two formats for InterAction Flow Mapping workshops, tailored to two common goals:\n\ndesigning a new (to-be) process, or\nimproving an existing (as-is) process.\n\nBoth workshops can be run in-person or remotely2. Feel free to adapt these workshop formats to your needs!\nFor more tips on how to conduct successful collaborative modeling sessions, please refer to the excellent book Collaborative Software Design.\n\nDesigning a New (To-Be) Process\nThis workshop format emphasizes shared design and exploration, like a typical CoMo session.\n\nPreparation\nPrepare the space and ensure participants know which process will be designed.\n\n\nRunning the Workshop\nDepending on how complex the process is, this workshop might take a full day or you can split it over multiple sessions.\nMain workshop steps:\n\nIntroduce goal and notation: Clarify (again) which process will be modeled and briefly explain the InterAction Flow elements if participants are unfamiliar with them.\nBraindrawing (together, alone): Give participants 5–10 minutes to independently sketch the process from their perspective, focusing on actors and their actions.\n\n\nFor in-person sessions, provide paper and pencils; for remote sessions, prepare an individual whiteboard frame with example elements for each participant.\nA useful approach is to start with user interactions and then move on to refining the required data, its sources, and persistence.\nWhen redesigning an existing process, participants should try to ignore current constraints and design an ideal target process from scratch.\n\nGallery walk: Allow time for participants to review each other’s diagrams.\nSelect a starting point: Choose one diagram as the basis for the shared InterAction Flow—preferably via dot voting during the gallery walk, or by facilitator choice if time is limited.\nIncremental refinement: Starting with the author of the selected diagram, participants take turns explaining their view while the group incrementally refines the shared InterAction Flow based on the new perspective.\n[Optional] Compare with the as-is process: When redesigning an existing process, compare the new flow with the current-state diagram (including any identified pain points) to ensure no key steps are missing and major issues are addressed.\nFor software-based processes, proceed by defining a refactoring plan with iterative improvements that gradually move the current process toward the target state, rather than attempting to implement a new flow from scratch while maintaining the existing one in parallel until a full replacement is possible.\n\nFor software projects, the InterAction Flow can be refined further to aid the implementation (as in Figure 7) by splitting into two groups:\n\nGroup 1 (incl. a UX Designer / Product Owner, Frontend Developer): Refines user interactions, adds wireframes, and identifies data needed to render the UI.\nGroup 2 (incl. a Backend Developer): Refines API and data store layers, including database schemas or ORMs.\n\nThis can be done in breakout sessions or asynchronously. The groups then reconvene to align their results and ensure the overall flow remains consistent and data needs are met end to end. At this point it can also be helpful to perform a Data Completeness Check.\n\n\n\n\n\n\nTipData Completeness Check\n\n\n\nAn interesting concept in Event Modeling is the Data Completeness Check: Each dynamic element in a wireframe is highlighted in a specific color to indicate whether the data shown there needs to be retrieved from (green) or persisted in (blue) a data store. By then linking these elements to our database schema, we can check whether we are storing all data required to enable the user experience we want (Figure 8).\n\n\n\n\n\n\nFigure 8: Wireframes with colored elements to indicate whether the shown data needs to be retrieved from (green) or persisted in (blue) a data store, as well as the corresponding fields in our database schema that contain these values. This helps us notice in advance whether our data model is incomplete.\n\n\n\n\n\n\n\n\nImproving an Existing (As-Is) Process\nThis workshop is ideal for analyzing and improving an existing process, surfacing problems, and identifying solutions. It draws inspiration from the book Flow Engineering.\n\nPreparation\nSince here the focus is on identifying problems rather than creating a new process, preparing the InterAction Flow is less of a creative exercise and can often be done in advance. Talk to key stakeholders to get different perspectives and draft a diagram that can serve as a conversation starter in the workshop. Consider sharing the diagram with participants in the invitation so they can review it beforehand.\n\n\nRunning the Workshop\nPlan for 1–3 hours depending on process complexity. The workshop typically follows six steps (Figure 9):\n\nAlign on the InterAction Flow: Walk participants through the diagram and refine it collaboratively.\nBrainstorm pain points: Participants add pink stickies for any problem they notice, from minor UI issues to tedious manual steps.\nPrioritize pain points: Rank the pink stickies by dimensions relevant to your context, like severity (“how annoying is this?”) and frequency (“how often does it happen?”).\nBrainstorm solutions: Participants suggest solutions for the prioritized pain points, adding green stickies to corresponding problems.\nPrioritize solutions: Rank solution ideas by impact (“by how much could this improve the overall situation?”) and feasibility (“how easy would it be to implement?”).\nMake a plan: Use dot voting to select 3–5 solutions to implement, assign owners and deadlines, and track progress in the team’s project management tool.\n\n\n\n\n\n\n\nFigure 9: Overview of the workshop steps.\n\n\n\nFollow up on assigned tasks and revisit the board for further improvement ideas as needed.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#footnotes",
    "href": "interaction_flow_mapping.html#footnotes",
    "title": "InterAction Flow Mapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne reason for modeling these resources separately is to make it easier to distinguish between pure and impure functions in a software flow.↩︎\nIn terms of digital whiteboarding tools, I recommend using Miro (or their WebWhiteboard to get started without an account) or Excalidraw.↩︎",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html",
    "href": "2026-01-cdd-humans-ai.html",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "",
    "text": "This article is still very much work in progress; more details on the practical example will be added over time.\nWhen I attended my first computer science class in 2009, programming—writing text that makes a computer do something—felt like a magical superpower. But over the years, as I shifted towards designing larger systems, typing out the necessary code to materialize these systems started to feel more and more like a chore. That’s why I’ve been really excited when, around Nov/Dec 2025, AI coding agents finally reached a point where they produced code I didn’t feel I had to refactor from top to bottom—and suddenly software development became fun again.\nIn this article, I want to give a glimpse into how I personally use AI to develop software (as of February 2026). But more importantly, I want to explain what I don’t let AI do: the conceptual work that happens before I start coding—with or without AI—and which gives me the confidence that the software is actually worth building in the first place. I call this approach Clarity-Driven Development (CDD), which you can also read more about in my latest book, Clarity-Driven Development of Scientific Software.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#where-humans-are-still-in-the-loop",
    "href": "2026-01-cdd-humans-ai.html#where-humans-are-still-in-the-loop",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Where Humans Are Still in the Loop",
    "text": "Where Humans Are Still in the Loop\nAI—more specifically, generative AI in the form of large language models (LLMs)—is now available both through web-based chat tools like ChatGPT and through more autonomous agents like Claude Code. These systems can support us in different ways, with varying levels of oversight (Figure 1).\n\n\n\n\n\n\nFigure 1: Interactions with AI lie on a spectrum. How much oversight you need to provide depends on your goal, use case, and personal preference.\n\n\n\nWhile vibe coding may be appropriate for throwaway prototypes, so far I still lean toward the “human control” end of the spectrum. That said, as models improve, I’m increasingly surprised by the quality of AI-generated code and find myself manually verifying less and less—while still having guardrails in place: tests, type checking, linting, and similar feedback loops that also help AI agents validate their own work before calling a task done.\nIn that respect, I found that working with an AI agent feels a lot like working with a newly hired colleague. At first, you watch their output closely—who knows, maybe they oversold themselves in the interview, just like marketing promises and flashy AI hype posts don’t exactly mirror reality. But over time, as they repeatedly deliver, trust grows and the need for detailed checks drops (assuming you’re not a micromanaging control freak who already scared away their best people).\nBut even your most capable colleagues can only deliver high-quality work if you provide them with enough context and clearly describe the outcome you want. By that I don’t mean prescribing specific steps they need to follow (“first do X, then Y, then Z”)—humans value autonomy over how they accomplish their tasks, and modern AIs are capable of figuring out these steps themselves. But you do need to provide a clear definition of done. And CDD helps you achieve exactly that: define the result you envision so your colleague—human or AI—has a real chance of delivering something you’ll be happy with.\nThis also means the spectrum in Figure 1 is too one-dimensional. Human control is exerted in two steps (Figure 2): when providing the input to the AI (by writing more specific prompts) and when reviewing and manually correcting the AI’s output.\n\n\n\n\n\n\nFigure 2: Humans are in the loop in two places: when providing inputs and when scrutinizing the AI’s output. I prefer investing effort on the input side and would happily stop verifying outputs one day.\n\n\n\nMy hope is that the second step eventually disappears—that I can some day trust AI-generated code the way I trust compiler output. I’m perfectly happy, however, to keep full control on the input side. I still want to think through the problem and gain clarity about what it is I actually want to build. Now, I’m not saying this process won’t also involve AI—it can actually be a great sounding board for your ideas, similar to how brainstorming together with colleagues provides valuable perspectives. But I believe human thinking is still essential for guiding AI, not least because the software we build is ultimately for human users.\n\nMy Goal\nWhat the future of autonomous AI agents will look like—whether it will be common practice to have fleets of agents implementing tickets, opening pull requests, and reviewing each other’s work to produce working software—is still an open question. However, even as AI handles more of the implementation, we must not outsource the thinking.\n\nSo what I need is a framework that helps me think—to better understand the problem and the kind of solution I want. Ideally, the resulting artifacts should then naturally serve as the foundation for implementation: providing context detailed enough for a capable engineer with good judgment (human or AI) to build the software, but lightweight enough to avoid documentation overhead. And that’s where Clarity-Driven Development comes in.\n\n\n\n\n\n\nCautionWhat about the next generation of developers?\n\n\n\nA common concern is that using AI to produce code will prevent junior developers from acquiring the skills needed to become seniors. But I’d argue that those skills were never primarily about coding. The biggest difference I’ve observed between junior and senior developers is that seniors think deeply about the problem and the trade-offs between possible solutions, while juniors tend to jump straight into implementation.\nCDD explicitly pushes you to think before writing code. It helps you practice core skills: decomposing complex problems into smaller steps, designing data structures and flows, abstracting details behind clean interfaces, and deciding what the right thing to build actually is.\nWhen junior developers then implement the resulting designs with AI, they should think of it as their personal tutor. Rather than just accepting generated code, they need to stay curious and genuinely interested in how the software works. The AI can then enable them by explaining the solution, clarifying complex parts, and justifying decisions through their trade-offs. This creates a learning experience similar to shadowing a senior developer, and they’ll arguably learn more by being exposed to advanced patterns than by wasting hours struggling with specific tools or libraries. Even when writing code themselves, they should have AI review and critique their approach to see how a more experienced engineer might improve it.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#gaining-clarity",
    "href": "2026-01-cdd-humans-ai.html#gaining-clarity",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Gaining Clarity",
    "text": "Gaining Clarity\nI didn’t create Clarity-Driven Development (CDD) as a method for coding with AI. After more than a decade of professional software development, I simply noticed that I kept going through the same steps at the start of every major project. Over time, I refined those steps into a more explicit approach. And since the resulting artifacts are genuinely useful to me as a human developer, it’s no surprise they also turned out to be effective at guiding AI agents—a win on both sides.\nAt its core, CDD structures your thinking by asking three questions before you start coding (Figure 3):\n\nWhy is your idea worth implementing?\nWhat should your implementation look like to users?\nHow could you best implement this?\n\n\n\n\n\n\n\nFigure 3: Before writing code—yourself or via an AI—it’s worth gaining clarity on why the solution should exist, what users should experience, and how it might be implemented.\n\n\n\nWorking through these questions forces you to confront gaps in your understanding. I refer to the documents that capture the corresponding answers as sketches, since they provide a rough but intentional outline of the system to be built. However, the main goal is gaining clarity through the thinking process, not producing detailed documentation—so these sketches are far lighter than full requirements or formal specifications and still require a generous dose of sound judgment from a capable engineer to turn them into working software.\nIn the following sections, I’ll walk through what answering these questions looks like in practice. I’ll use an example from the domain of predictive quality—more complex than some toy To-Do app, but familiar enough for me to confidently evaluate the results. The example draws on my prior work at BASF (the chemical company) and at alcemy, a climate tech startup applying machine learning to optimize cement recipes and reduce CO₂ emissions. Since this example is illustrative rather than commercial, I’ll deliberately skip some details, which I’ll return to at the end of the article.\n\n\n\n\n\n\nNoteClarity- vs. Spec-Driven Development\n\n\n\nA related approach that has gained traction recently is Spec-Driven Development (SDD), which emphasizes detailed specifications, plans, and task breakdowns, often produced with AI support. However, I’m not convinced we should treat those specs—rather than the working code—as the source of truth. In the end, both describe your system, but the code is naturally more precise. And as an industry, we have moved away from hundred-page requirement documents towards more lightweight user stories for a reason.\nCDD instead encourages you to create just enough documentation for you and your collaborators—human and AI—to understand the problem and the conceptual solution. It prioritizes thinking over formal specifications, with sketches serving as tools for gaining clarity rather than documents to maintain.\n\n\n\nThe Why\nThe first question we need to answer is why our idea is worth implementing in the first place. This comes down to why users would be better off with our software than with the alternatives—whether those alternatives are competing products, spreadsheets, or no software whatsoever. Identifying a gap in the current solution-landscape requires knowing who our target users are (e.g., by creating user personas) and what is important to them. Their priorities might be related to specific functionality, usability, aesthetics, or anything else that helps them perform their job better and improve the metrics by which their work is evaluated.\nIf your team includes a product manager / owner, they should be able to help you with these points.\n\n\nPredictive Quality Example\n\nA common challenge in process industries like chemical or cement production is maintaining product quality despite changing conditions like fluctuating raw material quality, process degradation (e.g., catalyst depletion), and other latent factors. This requires the production lead (= our target user) to constantly adjust setpoints using heuristics to keep quality on target while maintaining throughput and managing energy costs and other resources (= their key priorities).\nWhat makes determining the right setpoints especially difficult is that actual product quality often isn’t known until hours or days later, since quality tests can be very time-intensive. For example, for cement, compressive strength can only be measured after a block hardens for 28 days. This means despite the production lead’s efforts to adjust setpoints, they may discover days later that their entire production run fell below quality standards—when it’s too late to correct.\nBy using a machine learning (ML) model to predict final product quality from readily available lab measurements, near-real-time process control becomes possible (Figure 4). Setpoints can then be automatically determined to maintain stable quality while additionally optimizing secondary metrics like throughput and resource consumption. This frees the production lead to focus on creative, less monotonous work like tweaking recipes, developing improved product variations, or optimizing the overall production process.\n\n\n\n\n\n\nFigure 4: Process optimization using machine learning (ML) predictions: Previously (without the dashed box), the production lead set ingredient amounts and process parameters based on heuristics. Now, an ML model predicts product quality based on lab measurements. The predicted quality is compared to the production lead’s target quality, and the system automatically adjusts setpoints to maintain quality on target.\n\n\n\n\n\n\nThe What\nIn the next step, we need to examine what our implementation should look like to users. This typically means creating wireframes and user interface mockups, and—ideally—testing them with potential users to see where they struggle. These mockups make assumptions about user workflows explicit and often expose misunderstandings about the problem itself.\nI often start with simple pencil-and-paper sketches, but tools like Google’s Stitch can generate polished mockups in seconds using AI. That said, input from a UX designer is invaluable here. And when designs already exist, it’s worth understanding why they make sense and how exactly they address user needs.\n\n\nPredictive Quality Example\n\nThe software we’re building is primarily a data product. At its core is an ML model acting as a soft sensor for product quality. However, reliable predictions alone aren’t sufficient—they must be presented to the production lead in actionable ways through effective data visualizations. This requires understanding what questions production leads need to answer and what decisions they must make. Key use cases our software must support include:\n\nMonitoring quality (actual when available, otherwise predicted), throughput, and costs over time\nIdentifying possible root causes for quality degradation\nInspecting ML model accuracy and learned patterns to build trust in its predictions\n\nAdditionally, the application requires inputs from the production lead, such as configuring recipes for all product variants they produce.\nEach of these user needs translates into one or more screens. For each screen, I created a wireframe and a brief description of the screen, including which backend endpoints it should communicate with to read or write the required data (finalized in the next step). For example, the resulting sketches for the quality monitoring view are shown in Figure TODO, while sketches for other screens can be found in the project’s GitHub repo (TODO: link).\n\n\n\nThe How\nFinally, we need to ponder how we could best implement this solution. This includes designing interfaces, data structures, and flows. Specifically, this means defining the inputs and outputs of major endpoints or scripts and outlining the database schema. If possible, these sketches should then be discussed with a more experienced software engineer or architect.\nI usually begin by identifying the endpoints required to support the user experience, based on the data needs revealed by each screen’s mockup. Next, I determine any additional processes needed to support them, such as batch jobs for data processing. For each relevant interface, I note down:\n- script name or endpoint path\n- purpose (high-level description of the provided functionality)\n- input arguments & return values (name, type, brief description)\n- side effects: how external resources such as databases, message queues, or other services are accessed\n- implementation notes, especially key domain logic, invariants, and edge cases\nIn larger projects, these interface sketches can be organized into files and folders that mirror the intended module structure. I usually limit these sketches to interfaces that define the system’s external behavior. For core functionality, however, it may be useful to additionally outline key functions or classes in this way as well. All other details, such as how the internals are decomposed, are up to our colleague and left to be handled during implementation.\nTo define database schemas, I like using drawDB. It lets you model the schema visually and export the corresponding SQL, which is excellent input for an AI when generating ORMs or other database-related code.\n\n\nPredictive Quality Example\n\nBased on the screens and their data needs identified in the previous step, most required endpoints are already clear. Additionally, we need some batch jobs: regular scripts that handle data cleaning and retrain prediction models weekly on newly collected data. Detailed descriptions of these endpoints and scripts can be found on GitHub (TODO: link).\nFinally, we need to determine how data for our application, including trained models, should be structured and stored. Here, I aim to balance normalization (avoiding redundant entries through related tables with foreign keys) against query simplicity (avoiding excessive joins across multiple tables and complicated subqueries). The database schema I came up with for this software is shown in Figure TODO.\n\n\nAt this stage, our results are independent of any specific programming language or technology choices. That makes them cheap in the sense that no hard-to-reverse decisions have been made yet. Which also means now is a good opportunity—if you haven’t already—to share your idea and sketches with others (human and/or AI) and gather feedback before committing to an implementation built on a potentially flawed design.\n\nThinking through a problem and solution in this way has always been the fun part for me. In the past, I’d often stop here—once the application “worked in my head,” the challenge was essentially over, and turning my sketches into code just felt like a chore. Fortunately, that tedious work can now be offloaded to an AI that won’t get bored so easily.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#implementing-with-ai",
    "href": "2026-01-cdd-humans-ai.html#implementing-with-ai",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Implementing with AI",
    "text": "Implementing with AI\nNow that we have clarity on our desired destination, we’re ready to let AI take the driver’s seat.\n\n\n\n\n\n\nImportantThis is not a prompt engineering tutorial\n\n\n\nThe workflow descriptions below are kept deliberately high-level. Concrete tips tend to become obsolete almost as soon as they’re written, given how fast AI models evolve. If you want detailed, tactical advice, just search for “prompt engineering.” For example, the best practices provided by Anthropic are a good starting point.\nFor advanced agentic engineering techniques, Peter Steinberger (creator of OpenClaw and numerous other AI-built open source tools) is worth following—several tips in this article were inspired by insights he’s shared about his workflow.\n\n\n\nUp-front Decisions\nEvery project starts with a set of structural decisions: programming language, codebase layout, tools, deployment strategy, patterns, framework choices, and so on. I often chat with an AI to think these through, get initial suggestions, and weigh trade-offs.\nFor example, I usually work in Python, so for a web app I would normally default to using Streamlit (despite all its flaws) to get something working quickly. For this project, though, I wanted something more production-ready and challenging, and decided to build a proper frontend using a JavaScript framework. So I asked an AI whether React or Angular would be more appropriate for my use case (Figure 5).\n\n\n\n\n\n\nFigure 5: How Claude helped me pick a frontend framework.\n\n\n\nThe arguments for React sound reasonable, though of course I don’t have enough frontend experience to fully validate them. However, had I asked an experienced web developer for their recommendation, their answer might also be biased, shaped by personal framework preferences and not necessarily tuned to my specific constraints.\n\n\n\n\n\n\nCautionMind the training data cutoff\n\n\n\nBecause the training data behind most LLMs is typically a few months out of date, they miss recent advances and may recommend frameworks or tooling that are no longer current. You often need to explicitly tell the AI to search the web for the latest state of the art, and you should also keep up with developments in your field so you can recognize when its recommendations are outdated.\n\n\nOf course, knowing which trade-offs matter, and identifying the system’s constraints in the first place, is a skill that comes with experience. If you’re not sure which factors might be relevant for your use case, you can also tell the AI to first ask you some questions before making a recommendation. In any case, using an AI to help with architectural decisions is almost certainly better than simply adopting whatever technology happens to be the latest shiny thing.\nBut please note: since LLMs are probabilistic models, your results may vary. I’ve found the following system prompt1 very helpful to improve answer quality:\n\nTell it like it is; don’t sugar-coat responses. Take a forward-thinking view. Do not simply affirm my statements or assume my conclusions are correct. Your goal is to be an intellectual sparring partner, not just an agreeable assistant. Where necessary, question my assumptions and provide counterpoints / alternative perspectives to make sure my reasoning holds up under scrutiny and there are now gaps or flaws in my argument. Correct me where I’m wrong.\n\n\nEncode Decisions in Agent Context\nThe choices you’ve made up to now are crucial context for everyone working on this project. If you’re lucky, your team is already in the habit of tracking them in architectural decision records (ADRs). But often this knowledge exists only in people’s heads. When collaborating with an AI agent, it is critical that you document this information in writing—and do so concisely to reduce token consumption. An AGENTS.md file2 is perfect for this to ensure the instructions are passed to the LLM as a system prompt.\nFor example, my AGENTS.md file for this project (TODO: add link) includes\n\na project overview derived from the CDD “why” sketches\nwhich role the AI agent should assume (capable staff engineer)\na description of the project structure and instructions on how the agent should interact with the codebase (e.g., using the ty type checker instead of the former default mypy for Python—another example where specific hints are necessary when your project setup deviates from the mainstream conventions the AI encountered in its training data)\nstyle guidelines reflecting what I consider good code\na checklist outlining the desired agent workflow (e.g., ask questions if anything is unclear, write tests before the implementation, make sure type checker, linter, and all tests pass)\n\nHowever, this is very much a living document—whenever the AI does something stupid, I’ll revise the instructions to improve the likelihood of better results in future runs.\n\n\n\nPartitioning the Work\nAI context windows are still limited, so dumping all sketches into a single chat at once won’t get you a working system. But AI can help you break down the work into individual tasks and generate prompts for each step.\nTo ensure your sketches and instructions are complete, first ask the AI whether anything about your project vision is ambiguous or unclear, or whether the information provided would be sufficient for an experienced engineer to begin implementation. After any potential revisions, I then ask the AI to decompose the work into tasks screen by screen, starting with the database schema and an initial set of endpoints, then adding the corresponding frontend components, and continuing this pattern until all features are covered.\nTODO: add task decomposition from AI\n\n\nCoding\nNow for the implementation itself, my AI-assisted coding setup is actually really basic. I’m not (yet ;-)) running a swarm of sub-agents overnight to parallelize tasks, review pull requests, and hand me a finished app the next morning. Partly that’s because I still have enough trust issues and would rather supervise the development step by step. But if we’re being honest, it’s also because I haven’t yet had the patience to dive this deep into the fast-moving ecosystem of AI agents and tooling (how many VS Code clones with AI integration do we have now?!) and prefer to maintain a reasonably predictable workflow.\nSo what I actually use is the Zed IDE, built from the start with AI-assisted coding in mind and driving recent standards like the agent client protocol, together with Claude Code. I have Claude Code running inside the IDE rather than as a CLI, because I really appreciate a thoughtful GUI, for example, to review the agent’s code changes. I selected Claude Code based on some preliminary experiments comparing different AI models and because Anthropic has set the standard for innovation in coding agents, while other companies mainly scrambled to keep up.\nTODO: add screenshot of IDE\nAfter all the preparations to gain clarity on what we want to build, implementing the system is now fairly straightforward. Or as Bent Flyvbjerg argues in How Big Things Get Done: a solid, well-thought-out plan gets you halfway to smooth execution and delivery.\nEssentially, I feed the individual task prompts to the AI one by one, together with the general guidelines captured in AGENTS.md, while verifying after each step that the results match my expectations. Along the way, I follow some best practices:\n\nAsk for a plan first. At the start of every new task, I ask the AI “You need to implement X—how would you do that?” Ideally, the agent then provides several options with trade-offs and step-by-step approaches. If one resonates, I continue with “now implement option Y.”\nSwitch threads regularly. To avoid “context rot”—where results deteriorate as the context window fills and earlier instructions get compressed or forgotten—I start a fresh thread for each new task.\nClose the feedback loop. Like human developers, agents often don’t produce perfect code on the first try. I enable the agent to use build tools, linters, and a thorough test suite (by asking it to write tests first when implementing new tasks) so it can identify failures and self-correct before declaring a task done.\nMaintain modular architecture. As the codebase grows, good modularity ensures the agent knows where to find relevant files without polluting its context window with irrelevant code.\n\nEven with solid instructions, the implementation process remains iterative. Sketches inevitably contain gaps that only surface during implementation and you only really know what you actually wanted once you see the working software and can interact with its features—agile is not dead. In principle, you could then go back, update the sketch documents, and generate a new prompt from the diff. In practice, I usually find it faster to ask the AI to fix the issue directly and archive the sketches once they’ve been implemented.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#the-result",
    "href": "2026-01-cdd-humans-ai.html#the-result",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "The Result",
    "text": "The Result\nThe sketches have now served their purpose: they helped me think through a coherent solution before writing code. Moving forward, the working software will be my source of truth. TODO: The implementation took roughly X hours and consumed $X in Claude Code tokens. You can check out the code on GitHub and interact with the live application on Render.\nTODO: add screenshot of application\nKeep in mind this is still a toy project: achieving the first 80% was relatively straightforward, but—as always—the remaining 20% (or likely a lot more in this case) is where the real challenges lie. From my experience at alcemy, just a few of the critical components missing for a production-ready system include:\n\nUser management, authentication, and other security considerations\nIntegrating data with automated exports from last-century data management systems\nBuilding an ML model that delivers reliable predictions despite measurement errors, which usually requires integrating deep domain knowledge through feature engineering and hybrid models that combine data-driven approaches with a mechanistic understanding of the underlying processes\nRunning the system in production, including provisioning infrastructure, handling ongoing data issues, retraining models regularly, and adapting to new edge cases\nChange management: ensuring users adopt and ideally enjoy using your product\n\nMost of these points require substantial human creativity, domain expertise, and empathy for users.\n\nWhere to Go From Here\nThe most important advice that I can give you for AI-assisted coding—beyond thinking through and clearly defining what you want to build—is to keep experimenting. It takes practice to discover which prompts work and when the AI goes off the rails. And then you’ll need to relearn this again every few months as new models and tools appear. But it’s also fun to keep being positively surprised by your new AI colleague and see how it can amplify your creativity and handle the tedious tasks, letting you focus on the ideas that matter.\n\nAcknowledgements\nThe creation of this article was inspired by some great conversations with my wonderful colleagues at WPS (but any potentially controversial opinions expressed here are my own 😉).",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#footnotes",
    "href": "2026-01-cdd-humans-ai.html#footnotes",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA system prompt is a set of instructions automatically included in all conversations. Depending on how you use an AI, this can live in personal preferences (for web interfaces) or in a configuration file such as an AGENTS.md.↩︎\nAt the time of writing, AGENTS.md is unfortunately not yet a universally accepted standard across IDEs and coding agents, which is problematic when collaborating with people using different setups. In my case, I work around this by referencing @AGENTS.md from my CLAUDE.md file so Claude Code reliably picks up the instructions. Ideally, this won’t be necessary in the future.↩︎",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Thanks for reading!\nI’m always looking for feedback or suggestions – don’t hesitate to contact me at hey@franziskahorn.de. 😊\nYou might also be interested in my books:\n\nClarity-Driven Development of Scientific Software (2025) — also available for purchase on Leanpub\nA Practitioner’s Guide to Machine Learning (2021)",
    "crumbs": [
      "Home"
    ]
  }
]
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Supervised Learning Models – A Practitioner's Guide to Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06_pitfalls.html" rel="next">
<link href="./04_supervised.html" rel="prev">
<link href="./favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-f3084fe83d417c7d07102af91575287a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05_supervised_models.html"><span class="chapter-title">Supervised Learning Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A Practitioner’s Guide to Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01a_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01b_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01c_python.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">ML with Python</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Data Analysis &amp; Preprocessing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_unsupervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Unsupervised Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Supervised Learning Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_supervised_models.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Supervised Learning Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pitfalls.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Avoiding Common Pitfalls</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_advanced_topics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advanced Topics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Conclusion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-linear-models" id="toc-sec-linear-models" class="nav-link active" data-scroll-target="#sec-linear-models">Linear Models</a></li>
  <li><a href="#sec-neural-networks" id="toc-sec-neural-networks" class="nav-link" data-scroll-target="#sec-neural-networks">Neural Networks</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#tips-tricks" id="toc-tips-tricks" class="nav-link" data-scroll-target="#tips-tricks">Tips &amp; Tricks</a></li>
  </ul></li>
  <li><a href="#sec-decision-trees" id="toc-sec-decision-trees" class="nav-link" data-scroll-target="#sec-decision-trees">Decision Trees</a></li>
  <li><a href="#sec-ensemble-methods" id="toc-sec-ensemble-methods" class="nav-link" data-scroll-target="#sec-ensemble-methods">Ensemble Methods</a></li>
  <li><a href="#sec-knn" id="toc-sec-knn" class="nav-link" data-scroll-target="#sec-knn">k-Nearest Neighbors (kNN)</a></li>
  <li><a href="#sec-kernel-methods" id="toc-sec-kernel-methods" class="nav-link" data-scroll-target="#sec-kernel-methods">Kernel Methods</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Supervised Learning Models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Now that you’re familiar with the basics of supervised learning – problem types, algorithmic approaches, model evaluation and selection – we discuss the different features- and similarity-based models in more detail:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/04_supervised_intro/comparison_all4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<section id="sec-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="sec-linear-models">Linear Models</h2>
<div class="quarto-layout-panel" data-layout="[ 20, 80 ]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<p><img src="../images/04_supervised_intro/comparison_linear.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<p>The first type of supervised learning model that we’ll look at in more detail are linear models, which are a type of features-based model that are very efficient (i.e., can be used with large datasets), but, as the name suggests, are only capable of describing linear relationships between the input and target variables.</p>
</div>
</div>
</div>
<p>Linear models can be used for regression problems (→ the standard linear regression model that you might have already heard of) as well as for classification problems (→ logistic regression, which predicts a probability score between 0 and 1):</p>
<div class="quarto-layout-panel" data-layout="[ -5, 45, 45, -5 ]">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: center;">
<p><img src="../images/05_supervised_other/linreg.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: center;">
<p><img src="../images/05_supervised_other/logreg.png" class="img-fluid"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<p><strong>Main idea:</strong><br>
Prediction is a linear combination of the input features (and intercept <span class="math inline">\(b\)</span>):</p>
<p><span class="math display">\[
f(\mathbf{x}; \mathbf{w}) = b + \langle\mathbf{w}, \mathbf{x}\rangle = b + \sum_{k=1}^d w_k \cdot x_k = \hat{y}
\]</span></p>
<p><strong>Linear Regression</strong>:<br>
Find <span class="math inline">\(\mathbf{w}\)</span> that minimizes MSE <span class="math inline">\(\| \mathbf{y} - \mathbf{\hat y}\|_2^2\)</span> with <span class="math inline">\(\hat y\)</span> computed as in the formula above.</p>
<p><strong>Logistic Regression</strong> (→ for classification problems!):<br>
Make predictions as</p>
<p><span class="math display">\[
\sigma\left(b + \langle\mathbf{w}, \mathbf{x}\rangle\right) = \hat y\quad\quad \text{with: } \sigma(z) = \frac{1}{1+e^{-z}} \quad \Rightarrow\; \hat y  \in [0, 1]
\]</span></p>
<p>where <span class="math inline">\(\sigma(z)\)</span> is the so-called sigmoid (or logistic) function that squeezes the output of the linear model within the interval <span class="math inline">\([0, 1\)</span>] (i.e., the S-curve shown in the plot above).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression, LogisticRegression</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Pros:</strong></p>
<ul>
<li>Linear models are good for small datasets.</li>
<li>Extensions for nonlinear problems exist ⇒ feature engineering (e.g., including interaction terms), GAMs, etc.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Polynomial regression is just linear regression on engineered polynomial features">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Polynomial regression is just linear regression on engineered polynomial features
</div>
</div>
<div class="callout-body-container callout-body">
<p>When a statistician tells you that they did a “polynomial regression” what they really mean is that they did some feature engineering to include new variables like <span class="math inline">\(x_5^2\)</span> and <span class="math inline">\(x_2^3x_7\)</span> and then fitted a linear regression model on this extended set of features. This means the model is still linear in the parameters, i.e., the prediction is still a linear combination of the inputs, but some of the inputs are now polynomial terms computed from the original features.</p>
</div>
</div>
<p><strong>Careful:</strong></p>
<ul>
<li>Regularization (to keep <span class="math inline">\(\mathbf{w}\)</span> in check) is often a good idea.</li>
</ul>
<section id="regularization" class="level4">
<h4 class="anchored" data-anchor-id="regularization">Regularization</h4>
<p>Motivation: For uncorrelated but noisy data, which model should you choose?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/regularization_motivation2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>⇒ Regularization = assume no relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> unless the data strongly suggests otherwise.</p>
<p>This is accomplished by imposing constraints on the model’s weights by adding penalty terms in the optimization:</p>
<p><span class="math display">\[
\min_\mathbf{w}\; \underbrace{\sum_{i=1}^n (y_i - (b + \langle\mathbf{w},\mathbf{x}_i\rangle))^2}_{\text{Linear Regression}} + \lambda_1 \underbrace{\sum_{k=1}^d |w_k|}_{L1} + \lambda_2 \underbrace{\sum_{k=1}^d w_k^2}_{L2}
\]</span></p>
<p>This means the optimal solution now not only achieves a low MSE between the true and predicted values (i.e., the normal linear regression error), but additionally does so with the smallest possible weights. (The regularization therefore also defines a unique solution in the face of collinearity.)</p>
<p><strong><em>L1</em> Regularization</strong> (→ Lasso Regression): Sparse weights (i.e., many 0, others normal)<br>
→ Good for data with possibly irrelevant features.</p>
<p><strong><em>L2</em> Regularization</strong> (→ Ridge Regression): Small weights<br>
→ Computationally beneficial; can help for data with outliers.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Use L1 to select features, then L2 for robust modeling">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Use L1 to select features, then L2 for robust modeling
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you’re working with a new dataset, it often includes lots of variables, many of which might not be relevant for the prediction problem. In this case, an <em>L1</em>-regularized model is helpful to sort out irrelevant features. Then, when you are sure which input variables are relevant for the prediction problem, an <em>L2</em>-regularized model gives a robust performance.</p>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> RidgeCV, LassoLarsCV</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled" title="Be careful: in sklearn, `alpha` increases regularization, `C` decreases it">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Be careful: in sklearn, <code>alpha</code> increases regularization, <code>C</code> decreases it
</div>
</div>
<div class="callout-body-container callout-body">
<p>Regularization is also used in many other <code>sklearn</code> models. Depending on the type of model (for historical reasons), what we denoted as <span class="math inline">\(\lambda\)</span> in the formula above is a hyperparameter that is either called <code>alpha</code> or <code>C</code>, where you have to be careful, because while for <code>alpha</code> higher values mean more regularization (i.e., this acts exactly as the <span class="math inline">\(\lambda\)</span> in the formula above), when the model instead has the hyperparameter <code>C</code>, here higher values mean <em>less</em> regularization!</p>
</div>
</div>
</section>
<section id="generalized-additive-models-gams" class="level4">
<h4 class="anchored" data-anchor-id="generalized-additive-models-gams">Generalized Additive Models (GAMs)</h4>
<p>GAMs are a very powerful generalization of linear models. While in a linear regression, the target variable is modeled as a sum of linear functions of the input variables (parametrized by the respective coefficients), GAMs instead fit a smooth function <span class="math inline">\(f_k(x_k)\)</span> to each input variable and then predict the target variable as a sum of these:</p>
<p><span class="math display">\[
\hat{y} = b + \sum_{k=1}^d \; w_k \cdot x_k \quad\quad \Rightarrow \quad\quad \hat{y} = b + \sum_{k=1}^d \; f_k(x_k)
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/gam2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>→ <code>gam</code> library in R; Python: <a href="https://pygam.readthedocs.io/"><code>pyGAM</code></a>, <a href="https://github.com/interpretml/interpret"><code>interpret</code></a></p>
</section>
</section>
<section id="sec-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="sec-neural-networks">Neural Networks</h2>
<div class="quarto-layout-panel" data-layout="[ 20, 80 ]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<p><img src="../images/04_supervised_intro/comparison_nn.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<p>Next up are neural networks (NN), which can be used to solve extremely complex problems (besides regular supervised learning tasks), but that are also rather data hungry (depending on the size of the network).<br>
We’ll only cover the basics here; more advanced NN architectures, like those used to process image and text data, are discussed in the chapter on <a href="07_advanced_topics.html#sec-deep-learning">Deep Learning</a>.</p>
</div>
</div>
</div>
<div class="custom-gray-block" style="text-align:center;">
<p><strong>Basic Math</strong> <span class="math display">\[
\small
\begin{aligned}
\begin{pmatrix}
W_{11} &amp; W_{12} &amp; \cdots &amp; W_{1j}\\
W_{21} &amp; W_{22} &amp; \cdots &amp; W_{2j}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
W_{i1} &amp; W_{i2} &amp; \cdots &amp; W_{ij}\\
\end{pmatrix}
\end{aligned}
\]</span></p>
<p><strong>Dangerous Artificial Intelligence</strong> <span class="math display">\[
\small
\begin{aligned}
\begin{pmatrix}
W_{11} &amp; W_{12} &amp; \cdots &amp; W_{1j}\\
W_{21} &amp; W_{22} &amp; \cdots &amp; W_{2j}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
W_{i1} &amp; W_{i2} &amp; \cdots &amp; W_{ij}\\
\end{pmatrix} \cdot
\begin{pmatrix}
W_{11} &amp; W_{12} &amp; \cdots &amp; W_{1k}\\
W_{21} &amp; W_{22} &amp; \cdots &amp; W_{2k}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
W_{j1} &amp; W_{j2} &amp; \cdots &amp; W_{jk}\\
\end{pmatrix} \cdot
\begin{pmatrix}
W_{11} &amp; W_{12} &amp; \cdots &amp; W_{1l}\\
W_{21} &amp; W_{22} &amp; \cdots &amp; W_{2l}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
W_{k1} &amp; W_{k2} &amp; \cdots &amp; W_{kl}\\
\end{pmatrix} \cdot
\begin{pmatrix}
W_{11} &amp; W_{12} &amp; \cdots &amp; W_{1m}\\
W_{21} &amp; W_{22} &amp; \cdots &amp; W_{2m}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
W_{l1} &amp; W_{l2} &amp; \cdots &amp; W_{lm}\\
\end{pmatrix}
\end{aligned}
\]</span></p>
<p><span class="small-text">Deep learning just means using more matrices.</span></p>
</div>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<section id="recap-linear-models" class="level4">
<h4 class="anchored" data-anchor-id="recap-linear-models">Recap: Linear Models</h4>
<p>Prediction is a linear combination of input features (and intercept / bias term <span class="math inline">\(b\)</span>):</p>
<p><span class="math display">\[
f(\mathbf{x}; \mathbf{w}) = b + \langle\mathbf{w},\mathbf{x}\rangle = b + \sum_{k=1}^d w_k \cdot x_k = \hat{y}
\]</span></p>
<p>In the case of multiple outputs <span class="math inline">\(\mathbf{y}\)</span> (e.g., in a multi-class classification problem <span class="math inline">\(\mathbf{y}\)</span> could contain the probabilities for all classes):</p>
<p><span class="math display">\[
f(\mathbf{x}; W) = \mathbf{x^\top}W = \mathbf{\hat{y}}
\]</span></p>
<p>For simplicity, we omit the bias term <span class="math inline">\(b\)</span> here; using a bias term is equivalent to including an additional input feature that is always 1.</p>
</section>
<section id="intuitive-explanation-of-neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-explanation-of-neural-networks">Intuitive Explanation of Neural Networks</h4>
<p><span class="small-text">[Adapted from: “AI for everyone” by Andrew Ng (coursera.org)]</span></p>
<p>Let’s say we have an online shop and are trying to predict how much of a product we will sell in the next month. The price we are willing to sell the product for will obviously influence the demand, as people are trying to get a good deal, i.e., the lower the price, the higher the demand; a negative correlation that can be captured by a linear model. However, the demand will never be below zero (i.e., when the price is very high, people wont suddenly return the product), so we need to adapt the model such that the predicted output is never negative. This can be achieved by applying the max function, in this context also called a nonlinear activation function, to the output of the linear model, so that now when the linear model would return a negative value, we instead predict 0.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/neural_network_intuitive_a4.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption>A very simple linear model with one input and one output variable and a nonlinear activation function (the max function).</figcaption>
</figure>
</div>
<p>This functional relationship can be visualized as a circle with one input <em>(price)</em> and one output <em>(demand)</em>, where the S-curve in the circle indicates that a nonlinear activation function is applied to the result. We will later see these circles as single units or “neurons” of a neural network.</p>
<p>To get better results, we can extend the model and use multiple input features for the prediction:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/neural_network_intuitive_b2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>A linear model with multiple inputs, where the prediction is computed as a weighted sum of the inputs, together with the max function to prevent negative values.</figcaption>
</figure>
</div>
<p>To improve the performance even further, we could now manually construct more informative features from the original inputs by combining them in meaningful ways (→ feature engineering) before computing the output:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/neural_network_intuitive_b6.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Our example is about an online shop, so the customers additionally have to pay shipping fees, which means to reflect the true affordability of the product, we need to combine the product price with the shipping costs. Next, the customers are interested in high quality products. However, not only the actual quality of the raw materials we used to make the product influences how the customers perceive the product, but we can also reinforce the impression that the product is of high quality with a marketing campaign. Furthermore, a high price also suggests that the product is superior. This means by creating these additional features, the price can actually contribute in two ways towards the final prediction: while, on the one hand, a lower price is beneficial for the affordability of the product, a higher price, on the other hand, results in a larger perceived quality.</figcaption>
</figure>
</div>
<p>While in this toy example, it was possible to construct such features manually, the nice thing about neural networks is that they do exactly that automatically: By using multiple layers, i.e., stacking multiple linear models (with nonlinear activation functions) on top of each other, it is possible to create more and more complex combinations of the original input features, which can improve the performance of the model. The more layers the network uses, i.e., the “deeper” it is, the more complex the resulting feature representations.</p>
<p>Since different tasks and especially different types of input data benefit from different feature representations, there exist different types of neural network architectures to accommodate this, e.g.</p>
<ul>
<li>Feed Forward Neural Networks (FFNNs), also called Multi-Layer Perceptrons (MLPs), for ‘normal’ (e.g., structured) data</li>
<li>Convolutional Neural Networks (CNNs) for images</li>
<li>Recurrent Neural Networks (RNNs) for sequential data like text or time series</li>
</ul>
<p>We’ll only cover FFNNs here; the other architectures are discussed in the chapter on <a href="07_advanced_topics.html#sec-deep-learning">Deep Learning</a>.</p>
<div class="custom-gray-block">
<p><strong>Pros:</strong></p>
<ul>
<li>State-of-the-art performance (especially on data with <a href="https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d">input invariances</a>).</li>
<li>Prediction for new test points is fast (just a few matrix multiplications).</li>
</ul>
<p><strong>Careful:</strong></p>
<ul>
<li>Can take a long time to train (use a GPU!!! (or <a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit">TPU</a>)).</li>
<li>Need a lot of data (depending on the size of the NN architecture).</li>
<li>Solution only a local optimum (which is usually not too problematic in practice, as there are many good optima).</li>
<li>Tricky to train: Performance depends on many parameters like learning rate, batch size, and even the random seed used when initializing the weights!</li>
</ul>
</div>
</section>
<section id="feed-forward-neural-network-ffnn" class="level4">
<h4 class="anchored" data-anchor-id="feed-forward-neural-network-ffnn">Feed Forward Neural Network (FFNN)</h4>
<p>This is the original and most straightforward neural network architecture, which we’ve already seen in the initial example, only that in practice such a model usually has a few more layers and units per layer.</p>
<p>Each layer here is basically a linear model, i.e., it consists of a weight matrix <span class="math inline">\(W_i\)</span> and some nonlinear activation function <span class="math inline">\(\sigma_i\)</span> that is applied to the output. These layers are applied sequentially to the input features <span class="math inline">\(\mathbf{x}\)</span>, i.e., the network computes a composite function (in this case for three layers):</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sigma_3(\sigma_2(\sigma_1(\mathbf{x^\top}W_1)W_2)W_3) = \mathbf{\hat{y}}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/neural_network.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Feed Forward Neural Network (FFNN) architecture: The input feature vector <span class="math inline">\(\mathbf{x}\)</span>, representing one data point, is multiplied by the first weight matrix <span class="math inline">\(W_1\)</span> to create a new vector, which, after applying the nonlinear activation function (e.g., the max function as we’ve seen in the initial example) results in the first hidden layer representation <span class="math inline">\(\mathbf{x}'\)</span>. This new vector is then multiplied by the second weight matrix <span class="math inline">\(W_2\)</span> and again a nonlinear activation function is applied to yield the second hidden layer representation of the sample, <span class="math inline">\(\mathbf{x}''\)</span>. Depending on how many layers the network has (i.e., how deep it is), this could be repeated multiple times now until finally the last layer computes the predicted output <span class="math inline">\(\mathbf{\hat{y}}\)</span>. While the network is trained, these predicted outputs gets closer and closer to the true outputs for the training samples.</figcaption>
</figure>
</div>
<p>For a regression problem, the output of the network would be a direct prediction of the target values (i.e., without applying a final nonlinear activation function), while for a classification problem, the output consists of a vector with probabilities for the different classes, created by applying a softmax activation function on the output, which ensures all values are between 0 and 1 and sum up to 1.</p>
<div class="custom-gray-block">
<p><strong>Test your understanding:</strong></p>
<ul>
<li>Why do we need the nonlinear activation functions between the layers, i.e., how could we simplify a network with multiple layers, if it didn’t have any nonlinear activation functions between the layers?</li>
<li>In what way could we manipulate the parameters (i.e., weight matrices) of an existing neural network without changing its predictions? (This is also a reason why there exist many equally good local optima.)</li>
</ul>
</div>
<p><strong>Number of hidden layers and units:</strong><br>
While the size of the input and output layers are determined by the number of input features and targets respectively, the dimensionality and number of hidden layers of the network is up to us. Usually, the hidden layers get smaller (i.e., have fewer units) as the data moves from the input to the output layer and when experimenting with different settings we can start with no hidden layers (which should give the same result as a linear model) and then progressively increase the size of the network until the performance stops improving. Just <a href="https://playground.tensorflow.org/">play around</a> a bit.</p>
</section>
<section id="training-neural-networks" class="level4 custom-gray-block">
<h4 class="anchored" data-anchor-id="training-neural-networks">Training Neural Networks</h4>
<p>To train a neural network, we first need to choose a loss function that should be optimized (e.g., mean squared error for regression problems or cross-entropy for classification problems).</p>
<p>While for a linear regression model, the optimal weights can be found analytically by setting the derivative of this loss function to 0 and solving for the weights, in a network with multiple layers and therefore many more weights, this is not feasible. Instead, the weights are tuned iteratively using a <strong>gradient descent</strong> procedure, where the derivative of the loss function w.r.t. each layer is computed step by step using the chain rule by basically pushing the error backwards through the network (i.e., from the output, where the error is computed, to the input layer), also called error <strong>backpropagation</strong>. At the beginning, all weight matrices are randomly initialized, so, for example, for a classification problem, for a given sample the network would predict approximately equal probabilities for all classes. The weights are then adapted according to their gradient, such that the next time the same samples are passed through the network, the prediction will be closer to the true output (and the value of the loss function is closer to a local minima).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/loss_surface.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption>Training loss surface over the network’s parameter space (simplified, i.e., showing only two weights (<span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>), while in reality, this would be a super complex nonlinear function in a very high-dimensional space), where every possible weight configuration of a network results in a different loss on the training set. By taking a step into the direction of steepest descent of this loss function, the weights of the network get closer to a configuration where the loss is at a local minimum.</figcaption>
</figure>
</div>
<p>Typically, the weights are adapted over multiple <strong>epochs</strong>, where one epoch means going over the whole training set once. By plotting the <a href="https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/">learning curves</a>, i.e., the training and validation loss against the number of training epochs, we can check whether the training was successful (i.e., whether the weights have converged to a good solution).</p>
<p>Since a training set usually contains lots of data points, it would be too computationally expensive to do gradient descent based on the whole dataset at once in each epoch, so we instead use mini-<strong>batches</strong>, i.e., subsets of usually 16-128 data points, for each training step.</p>
</section>
</section>
<section id="tips-tricks" class="level3">
<h3 class="anchored" data-anchor-id="tips-tricks">Tips &amp; Tricks</h3>
<ul>
<li><p>Scale the data (for classification tasks only inputs, for regression tasks also outputs or adapt the bias of the last layer; <code>StandardScaler</code> is usually a good choice) as otherwise the weights have to move far from their initialization to scale the data for us.</p></li>
<li><p>Use sample weights for classification problems with unequal class distributions.</p></li>
<li><p>NN are trained with gradient descent, which requires a good learning rate (i.e., step size for each training iteration → not too small, otherwise nothing is learned, not too big, otherwise it spirals out of control):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_nn/nn_learning_rate4.png" class="img-fluid figure-img" style="width:55.0%"></p>
<figcaption>A simple strategy to select a suitable initial learning rate is to train the network with different learning rates for one epoch on a subsample of the dataset and then check the loss after training. For too small learning rates (left), the loss will stay the same, while for too large learning rates (right) the loss will be higher after training.</figcaption>
</figure>
</div></li>
<li><p>Sanity check: A linear network (i.e., a FFNN with only one layer mapping directly from the inputs to the outputs) should achieve approximately the same performance as the corresponding linear model from sklearn.</p></li>
<li><p>Gradually make the network more complex until it can perfectly memorize a small training dataset (to get a network that has enough capacity to at least in principle capture the complexity of the task).</p></li>
<li><p>When selecting hyperparameters, always check if there is a clear trend towards an optimal setting; if the pattern seems random, initialize the network with different random seeds to see how robust the results are.</p></li>
<li><p>Using a learning rate scheduler (to decrease the learning rate over time to facilitate convergence) or early stopping (i.e., stopping the training when the performance on the validation set stops improving) can improve the generalization performance.</p></li>
<li><p>But often it is more important to train the network long enough, like, for hundreds of epochs (depending on the dataset size).</p></li>
</ul>
<p><span class="small-text">→ more tips for training NN: <a href="http://karpathy.github.io/2019/04/25/recipe/">Andrej Karpathy’s blog</a></span></p>
</section>
</section>
<section id="sec-decision-trees" class="level2">
<h2 class="anchored" data-anchor-id="sec-decision-trees">Decision Trees</h2>
<div class="quarto-layout-panel" data-layout="[ 20, 80 ]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<p><img src="../images/04_supervised_intro/comparison_dt.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<p>Next, we’ll look at decision trees, another type of features-based model that is very efficient as well, but can also capture more complex relationships between inputs and output.</p>
</div>
</div>
</div>
<p>We’ll describe the decision tree algorithm with the help of an example dataset:</p>
<p><strong><em>Example: Iris Dataset</em></strong><br>
The famous Iris dataset was initially studied by the influential statistician R. Fisher. It includes samples from three different types of Iris flowers, each described by four measurements. The task is to classify to which type of Iris flower a sample belongs:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/iris_dataset.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%"></p>
</figure>
</div>
<p><strong>Main idea:</strong><br>
Iteratively set a threshold on one of the features such that the remaining samples are split into two “cleaner” groups, where “clean” means that all samples in a group have a similar label, e.g., belong to the same class in case of a classification problem:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/decision_tree_iris2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption><em>Left:</em> Scatter plot of the data, which shows how the splits of the decision tree translate into a decision surface in the feature space, which also shows the characteristic cuts orthogonal to the respective axis (→ new points are classified according to the color of the shaded areas). <em>Right:</em> Illustration of the learned decision tree: Starting left at the root of the tree, the first decision is made to split the data at the ‘petal width’ feature, where all samples with a value smaller than 0.8 are classified as setosa Irises (⇒ perfectly clean group at this leaf) and the other samples advance to a followup decision. In the lower branch of the tree, the remaining samples are now split again according to their petal width and then again at their petal length (it is a coincidence that the same variable is used in both branches at this level to make the cut). Eventually, at each leaf (i.e., terminal node of the tree), the samples that ended up there are mostly from one single class, i.e., the decision tree succeeded in splitting the dataset into cleaner groups. (Please note that decision trees are normally drawn from top to bottom and, unlike real trees, their root is at the top and their branches and leaves grow towards the bottom – the computer scientist who came up with binary trees probably spent too much time indoors…).</figcaption>
</figure>
</div>
<p>To classify a new sample (i.e., if we went for a walk and found an Iris flower and decided to measure it), we compare the flower’s measurements to the thresholds in the tree (starting at the root) and depending on the leaf we end up in we get a prediction for the Iris type as the most frequent class of the training samples that ended up in this leaf. For regression problems, the tree is built in the same way, but the final prediction is given as the mean of the target variable of the training samples in a leaf.</p>
<p>The decision tree algorithm comes up with the decisions by essentially examining the histograms of all features at each step to figure out for which of the features the distributions for the different classes is separated the most and then sets a threshold there to split the samples.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, DecisionTreeRegressor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><u>Important Parameters:</u></p>
<ul>
<li><code>max_depth</code>: Maximum number of decisions to make about a sample.</li>
<li><code>min_samples_leaf</code>: How many samples have to end up in each leaf (at least), to prevent overly specific leaves with only a few samples.</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Easy to interpret (i.e., we know exactly what decisions were made to arrive at the prediction).</li>
<li>Good for heterogeneous data: no normalization necessary since all features are considered individually.</li>
</ul>
<p><strong>Careful:</strong></p>
<ul>
<li>If the hyperparameters (e.g., <code>min_samples_leaf</code>) aren’t set appropriately, it can happen that the tree becomes very specific and memorizes individual samples, which means it probably wont generalize well to new data points (also called “overfitting”, e.g., in the example above, one of the leaves contains only three samples, which might not have been a very useful split).</li>
<li>Unstable: small variations in the data can lead to very different trees.</li>
</ul>
</section>
<section id="sec-ensemble-methods" class="level2">
<h2 class="anchored" data-anchor-id="sec-ensemble-methods">Ensemble Methods</h2>
<p>What is better than one model? Multiple models!</p>
<p><strong>Main idea:</strong><br>
Train multiple models &amp; combine their predictions (regression: average; classification: most frequent class).</p>
<ul>
<li>Different types of models.</li>
<li>Same type of model but with different hyperparameter settings (this can also include the random seed used when initializing the model, e.g., for neural networks).</li>
<li>Models trained on different subsets of the data (different selections of samples and/or features).</li>
<li>Boosting: models are trained sequentially and each additional model focuses on those data points that the previous models got wrong.</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>More stable prediction (tip: use individual models that on their own overfit a bit).</li>
<li>Get an estimate of how certain the prediction is → how many models agree?</li>
</ul>
<p><strong>Careful:</strong></p>
<ul>
<li>Computationally expensive (depending on the models used).</li>
</ul>
<p><strong><em>Popular example: Random Forest</em></strong><br>
Multiple decision trees trained on random subsamples of the data, thereby exploiting the fact that decision trees can be sensitive to small variations in the dataset.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, RandomForestRegressor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For more advanced approaches, check out the voting ensemble and boosting methods from <a href="https://scikit-learn.org/stable/modules/ensemble.html">sklearn</a>, with which arbitrary models can be combined into an ensemble.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Ensemble methods excel on structured data">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Ensemble methods excel on structured data
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ensemble methods like random forests and gradient boosting trees give very good results on real world structured datasets and dominate the leader boards for many competitions at <a href="https://www.kaggle.com/">Kaggle</a>, a website where companies can upload datasets for data scientists to benchmark themselves against each other and even win prize money.</p>
</div>
</div>
</section>
<section id="sec-knn" class="level2">
<h2 class="anchored" data-anchor-id="sec-knn">k-Nearest Neighbors (kNN)</h2>
<div class="quarto-layout-panel" data-layout="[ 20, 80 ]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<p><img src="../images/04_supervised_intro/comparison_knn.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<p>The first similarity-based model we’ll look at is <em>k</em>-nearest neighbors (kNN), which follows a rather naive and straightforward approach, but nevertheless often achieves a good performance on complex problems.</p>
</div>
</div>
</div>
<p><strong>Main idea:</strong><br>
For a new sample, identify the <em>k</em> most similar training data points and predict the average of their target values / their most frequent class:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/knn.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>This kind of approach is also called <em>Lazy Learning</em>, since the model doesn’t actually learn any kind of internal parameters, but all the real computation only happens when we make a prediction for a new data point.<br>
(When calling the fit-method on the sklearn model, a search tree is built to efficiently identify the nearest neighbors for a new data point.)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor, KNeighborsClassifier</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><u>Important Parameters:</u></p>
<ul>
<li><code>n_neighbors</code>: How many nearest neighbors to consider when making a prediction.</li>
<li><code>metric</code>: How to compute the similarity between the samples (default: Euclidean distance).</li>
<li><code>weights</code>: By setting this to <code>'distance'</code> instead of the default <code>'uniform'</code>, the labels of the nearest neighbors contribute to the prediction proportionally to their distance to the new data point.</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Intuitive approach.</li>
</ul>
<p><strong>Careful:</strong></p>
<ul>
<li>Results completely depend on the similarity measure.</li>
</ul>
</section>
<section id="sec-kernel-methods" class="level2">
<h2 class="anchored" data-anchor-id="sec-kernel-methods">Kernel Methods</h2>
<div class="quarto-layout-panel" data-layout="[ 20, 80 ]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: center;">
<p><img src="../images/04_supervised_intro/comparison_kernel.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 80.0%;justify-content: flex-start;">
<p>Kernel methods are more sophisticated similarity-based models and were the hot stuff in the late 90s, when datasets were still of moderate size and computers weren’t fast enough to train large neural networks. They have some elegant math behind them, but please don’t be discouraged, if you don’t fully understand it the first time you read it – this is completely normal 😉.</p>
</div>
</div>
</div>
<p><strong>Main idea / “Kernel Trick”:</strong><br>
By working on similarities (computed with special <em>kernel functions</em>) instead of the original features, linear methods can be applied to solve nonlinear problems.</p>
<p>Assume there exists some function <span class="math inline">\(\phi(\mathbf{x})\)</span> (also called a ‘feature map’) with which the input data can be transformed in such a way that the problem can be solved with a linear method (basically the ideal outcome of feature engineering).<br>
For example, in the simple toy dataset shown below, the two classes become linearly separable when projecting the original input features into the space of their second order monomials:</p>
<div class="quarto-layout-panel" data-layout="[ 40, 20, 40 ]" style="text-align: center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 40.0%;justify-content: center;">
<p><img src="../images/05_supervised_other/kernels_xspace.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<p>→ <span class="math inline">\(\phi(\mathbf{x})\)</span> →</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 40.0%;justify-content: center;">
<p><img src="../images/05_supervised_other/kernels_zspace.png" class="img-fluid"></p>
</div>
</div>
</div>
<div class="fake-figcaption">
<p>In the original 2D representation on the left, we need a circle (i.e., a nonlinear function) to separate the blue from the red points, while in the 3D plot on the right, the data points are arranged in a cone shape, where we can cut off the red points at the tip of the cone with a hyperplane.</p>
</div>
<p>Of course, coming up with such a feature map <span class="math inline">\(\phi(\mathbf{x})\)</span>, especially for more complex problems, isn’t exactly easy. But as you will see, we don’t actually need to know what this transformation looks like!</p>
<p><u><strong>Example:</strong> Kernel Ridge Regression</u></p>
<p>Remember: In a linear regression model, the prediction for a new data point <span class="math inline">\(\mathbf{x}'\)</span> is computed as the scalar product of the feature vector with the weight vector <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\hat{y}' = \mathbf{x}' \mathbf{w}
\]</span></p>
<p>(for simplicity, we omit the bias term here; using a bias term is equivalent to including an additional input feature that is always 1).</p>
<p>The parameters of a linear ridge regression model are found by taking the derivative of the objective function,</p>
<p><span class="math display">\[
\| \mathbf{y} - \mathbf{Xw}\|_2^2 + \lambda \| \mathbf{w}\|_2^2
\]</span></p>
<p>with respect to <span class="math inline">\(\mathbf{w}\)</span>, setting it to 0, and solving for <span class="math inline">\(\mathbf{w}\)</span> (i.e.&nbsp;to find the minimum). This gives the following solution:</p>
<p><span class="math display">\[
\mathbf{w} = (\mathbf{X}^\top \mathbf{X} +\lambda I)^{-1}\mathbf{X}^\top \mathbf{y}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span> is the feature matrix of the training data and <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> are the corresponding target values.</p>
<p>Now we replace every occurrence of the original input features <span class="math inline">\(\mathbf{x}\)</span> in the formulas with the respective feature map <span class="math inline">\(\phi(\mathbf{x})\)</span> and do some linear algebra:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}' &amp;= \phi(\mathbf{x}') \mathbf{w} \quad\quad\quad\text{ with }\; \mathbf{w} = (\phi(\mathbf{X})^\top \phi(\mathbf{X}) +\lambda I)^{-1}\phi(\mathbf{X})^\top \mathbf{y}\\
&amp;= \phi(\mathbf{x}') (\phi(\mathbf{X})^\top \phi(\mathbf{X}) +\lambda I)^{-1}\phi(\mathbf{X})^\top \mathbf{y}\\
&amp; \quad\quad\vdots\\
&amp;= \underbrace{\phi(\mathbf{x}')\phi(\mathbf{X})^\top}_{\mathbf{k}'= k(\mathbf{x'}, \mathbf{X})} (\underbrace{\phi(\mathbf{X}) \phi(\mathbf{X})^\top}_{\mathbf{K} = k(\mathbf{X}, \mathbf{X})} +\lambda I)^{-1} \mathbf{y}\\
&amp;= \mathbf{k}'\underbrace{(\mathbf{K}  +\lambda I)^{-1} \mathbf{y}}_{\mathbf{\alpha}} = \sum_{i=1}^n k(\mathbf{x}', \mathbf{x}_i) \alpha_i
\end{aligned}
\]</span></p>
<p>After the reformulation, every <span class="math inline">\(\phi(\mathbf{x})\)</span> only occurs in scalar products with other <span class="math inline">\(\phi(\mathbf{x})\)</span>, and all these scalar products were replaced with a so-called <em>kernel function</em> <span class="math inline">\(k(\mathbf{x}', \mathbf{x})\)</span>, where <span class="math inline">\(k(\mathbf{X}, \mathbf{X}) = \mathbf{K} \in \mathbb{R}^{n \times n}\)</span> is the <em>kernel matrix</em>, i.e., the similarity of all training points to themselves, and <span class="math inline">\(k(\mathbf{x}', \mathbf{X}) = \mathbf{k}' \in \mathbb{R}^{n}\)</span> is the <em>kernel map</em>, i.e., the similarities of the new data point to all training points.</p>
<p>The prediction <span class="math inline">\(\hat{y}'\)</span> is now computed as a weighted sum of the similarities in the kernel map, i.e., while in the normal linear regression model the sum goes over the number of input features <em>d</em> and the weight vector is denoted as <span class="math inline">\(\mathbf{w}\)</span>, here the sum goes over the number of data points <em>n</em> and the weight vector is called <span class="math inline">\(\mathbf{\alpha}\)</span>.</p>
<section id="kernel-functions-kmathbfx-mathbfx" class="level4 custom-gray-block">
<h4 class="anchored" data-anchor-id="kernel-functions-kmathbfx-mathbfx">Kernel Functions <span class="math inline">\(\;k(\mathbf{x}', \mathbf{x})\)</span></h4>
<div class="callout callout-style-default callout-note callout-titled" title="Kernel functions are a special case of similarity functions">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Kernel functions are a special case of similarity functions
</div>
</div>
<div class="callout-body-container callout-body">
<p>A kernel function is basically a similarity function, but with the special requirement that the similarity matrix computed using this function, also called the <em>kernel matrix</em>, is positive semi-definite, i.e., has only eigenvalues <span class="math inline">\(\geq 0\)</span>.</p>
</div>
</div>
<p>There exist different kinds of kernel functions and depending on how they are computed, they induce a different kind of kernel feature space <span class="math inline">\(\phi(\mathbf{x})\)</span>, where computing the kernel function for two points is equivalent to computing the scalar product of their vectors in this kernel feature space.</p>
<p>We illustrate this on the initial example, where the data was projected into the space of second order monomials. When we compute the scalar product between the feature maps of two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, we arrive at a polynomial kernel of second degree:</p>
<p><span class="math display">\[
\langle\phi(\mathbf{a}), \phi(\mathbf{b})\rangle = (a_1^2, \sqrt{2}a_1a_2, a_2^2)(b_1^2, \sqrt{2}b_1b_2, b_2^2)^\top
= \langle\mathbf{a}, \mathbf{b}\rangle^2
=: k(\mathbf{a}, \mathbf{b})
\]</span></p>
<p>i.e, computing the kernel function <span class="math inline">\(k(\mathbf{a}, \mathbf{b}) = \langle\mathbf{a}, \mathbf{b}\rangle^2\)</span> between the two points is the same as computing the scalar product of the two feature maps <span class="math inline">\(\langle\phi(\mathbf{a}), \phi(\mathbf{b})\rangle\)</span>.</p>
<p>While for the polynomial kernel of second degree, <span class="math inline">\(\langle\mathbf{a}, \mathbf{b}\rangle^2\)</span>, we could easily figure out that the feature map consisted of the second order monomials, for other kernel functions, the corresponding feature map is not that easy to identify and the kernel feature space might even be infinite dimensional. However, since all the feature map terms <span class="math inline">\(\phi(\mathbf{x})\)</span> in the kernel ridge regression formulas only occurred in scalar products, we were able to replace them by a kernel function, which we can compute using the original input feature vectors. Therefore, we don’t actually need to know what the respective feature map looks like for other kernel functions!<br>
So while kernel methods are internally working in this really awesome kernel feature space <span class="math inline">\(\phi(\mathbf{x})\)</span>, induced by the respective kernel function, where all our problems are linear, all we need to know is how to compute the kernel function using the original feature vectors.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Use Kernel PCA to approximate the transformed feature vectors">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Use Kernel PCA to approximate the transformed feature vectors
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are curious what the kernel feature space looks like for some kernel function, you can use kernel PCA to get an approximation of <span class="math inline">\(\phi(\mathbf{x})\)</span>. Kernel PCA computes the eigendecomposition of a given kernel matrix, thereby generating a new representation of the data such that the scalar product of these new vectors approximates the kernel matrix. This means the new kernel PCA feature vector computed for some data point <span class="math inline">\(i\)</span> approximates <span class="math inline">\(\phi(\mathbf{x}_i)\)</span> – however, this still doesn’t tell us what the function <span class="math inline">\(\phi\)</span> itself looks like, i.e., how to get from <span class="math inline">\(\mathbf{x}\)</span> to <span class="math inline">\(\phi(\mathbf{x})\)</span>, we only get (an approximation of) the final result <span class="math inline">\(\phi(\mathbf{x})\)</span>.</p>
</div>
</div>
<p>In the example above you were already introduced to the polynomial kernel of second degree; the generalization, i.e., the polynomial kernel of degree <em>k</em>, is computed as <span class="math inline">\(\langle\mathbf{x}', \mathbf{x}_i\rangle^k\)</span>.</p>
<p>The most popular kernel function that works well in many use cases is the <strong>Radial Basis Function (RBF) / Gaussian kernel</strong>:</p>
<p><span class="math display">\[
k(\mathbf{x}', \mathbf{x}_i) = e^{-\gamma \|\mathbf{x}_i - \mathbf{x}' \|^2}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/rbf2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>With the RBF kernel function, the similarity of a new point <span class="math inline">\(\mathbf{x}'\)</span> to the training sample <span class="math inline">\(\mathbf{x}_i\)</span> is computed by centering a Gaussian function (with a width inversely proportional to <span class="math inline">\(\gamma\)</span>) over the <span class="math inline">\(i^{th}\)</span> training sample and then checking where the new point falls on this curve. Depending on the choice of <span class="math inline">\(\gamma\)</span>, either many samples will later contribute to the prediction (if the Gaussian function is very wide and therefore the similarity to multiple training points is large) or only the closest points will be considered (if the curve is very narrow and therefore the similarity to most training examples is close to 0).</figcaption>
</figure>
</div>
</section>
<section id="support-vector-machine-svm" class="level4">
<h4 class="anchored" data-anchor-id="support-vector-machine-svm">Support Vector Machine (SVM)</h4>
<p>A more efficient method, especially when the training set is large.<br>
<u>Key idea:</u> Only compute similarity to ‘most informative’ training points (= support vectors):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/05_supervised_other/svm.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p><span class="math display">\[
\sum_{i=1}^n k(\mathbf{x}', \mathbf{x}_i) \alpha_i  \quad \Rightarrow \quad  \sum_{i \in \{\text{SV}\}} k(\mathbf{x}', \mathbf{x}_i) \alpha_i
\]</span></p>
<p>This reduces the memory requirements and makes the prediction for new data points much faster, since it is only necessary to store the support vectors and compute the similarity to them instead of the whole training set.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> KernelPCA     <span class="co"># Kernel variant of PCA</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.kernel_ridge <span class="im">import</span> KernelRidge    <span class="co"># Kernel variant of ridge regression (→ use SVR instead)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC, SVR                <span class="co"># SVM for classification (C) and regression (R)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><u>Important Parameters:</u></p>
<ul>
<li><code>kernel</code>: The kernel function used to compute the similarities between the points (→ see <code>sklearn.metrics.pairwise</code>; usually <code>'rbf'</code>).</li>
<li>Additionally: Kernel parameters, e.g., <code>gamma</code> for <code>'rbf'</code> kernel.</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Nonlinear predictions with global optimum.</li>
<li>Fast to train (on medium size datasets; compared to, e.g., neural networks).</li>
</ul>
<p><strong>Careful:</strong></p>
<ul>
<li>Computationally expensive for large datasets (kernel matrix: <span class="math inline">\(\mathcal{O}(n^2)\)</span>).</li>
<li>Kernel functions, just like other similarity functions, benefit from scaled heterogeneous data.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script type="text/javascript"> // go once over all images and transform the percentage width into pixels relative to original body size
  const bodyWidth = parseFloat(getComputedStyle(document.documentElement).getPropertyValue('--custom-body-width')); // Get CSS variable
  document.querySelectorAll('.quarto-figure img').forEach((img) => {
    if (img.getAttribute('style')) {
      const styleWidth = img.getAttribute('style').match(/width:\s*([\d.]+)%/); // Extract percentage
      if (styleWidth) {
        const percentage = parseFloat(styleWidth[1]) / 100;
        img.style.width = `${bodyWidth * percentage}px`; // Apply calculated width
      }
    }
  });
</script>

<style type="text/css">
#footer {
  font-size: 80%; /* Makes the font 70% smaller */
  border-top: 1px solid #ccc; /* Adds a horizontal line above the footer */
  padding-top: 10px; /* Adds some spacing above the footer content */
}
</style>

<!-- FOOTER  -->
<div id="footer" class="outer">
  <footer class="inner">
    <p><b>Book/Course Feedback: </b> <a href="https://forms.gle/Ccv5h5zQxwPjWtCS7" target="_blank" rel="nofollow">Full Feedback Survey</a> or <a href="https://forms.gle/qK8T5ALzgpiZaxd49" target="_blank" rel="nofollow">Short Comment</a><br></p>

    <p>Find me on <a href="https://github.com/cod3licious/" target="_blank" rel="nofollow">GitHub</a> and <a href="https://www.linkedin.com/in/franziska-horn/" target="_blank" rel="nofollow">LinkedIn</a><br>
      <a href="https://franziskahorn.de/">Home</a> ~ <a href="mailto:hey@franziskahorn.de?Subject=Book%20Feedback" target="_blank" rel="nofollow">Contact</a> ~ <a href="https://franziskahorn.de/impressum.html">Impressum</a></p>

      <!-- CC license -->
   <p xmlns:cc="http://creativecommons.org/ns#">This work is licensed under <a href="https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0<img style="height:16px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:16px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""></a></p>
  </footer>
</div>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04_supervised.html" class="pagination-link" aria-label="Supervised Learning Basics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Supervised Learning Basics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06_pitfalls.html" class="pagination-link" aria-label="Avoiding Common Pitfalls">
        <span class="nav-page-text"><span class="chapter-title">Avoiding Common Pitfalls</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>
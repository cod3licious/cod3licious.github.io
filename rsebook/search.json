[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Software Engineering: A Primer",
    "section": "",
    "text": "Preface\nThis book is meant to empower researchers to code with confidence and clarity.\nIf you studied something other than computer science‚Äîespecially in the natural sciences like physics, chemistry, or biology‚Äîit‚Äôs likely you were never taught how to properly develop software. Yet, you‚Äôre often still expected to write code as part of your daily work. Maybe you‚Äôve taken a programming course like Python for Biologists and can put together functional scripts through trial and error (with a little help from ChatGPT). But chances are, no one ever showed you how to write well-structured, maintainable, and reusable code that could make your life‚Äîand collaborating with your colleagues‚Äîso much easier.\nThis book is for you if you want to:\n\nWrite functional software more quickly\nUse a structured approach to design better programs\nReuse your code in future projects\nFeel confident about what your scripts are doing\nPrepare your research code for production\nShare your work with pride.\n\nWhether you‚Äôre just beginning your scientific journey‚Äîperhaps working on your first major project like a master‚Äôs thesis or your first paper‚Äîor you‚Äôre contemplating a move from academia to industry, the practical advice in this book can guide you along the way. We will approach software design from first principles and tackle research questions with a product mindset.\nWhile the book contains some example code in Python to illustrate the concepts, the general ideas are independent of any programming language.\nThis is still a draft version! Please write me an email, if you have any suggestions for how this book could be improved!\nEnjoy! üòä\n\nAcknowledgments\nThe texts in this book were partly edited and refined with the help of ChatGPT and Claude.ai, however, all original content is my own.\n\n\nHow to cite\n@book{horn2025rseprimer,\n  author = {Horn, Franziska},\n  title = {Research Software Engineering: A Primer},\n  year = {2025},\n  url = {https://franziskahorn.de/rsebook/},\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_purpose.html",
    "href": "01_purpose.html",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Types of Research Questions\nBefore writing your first line of code, it‚Äôs crucial to have a clear understanding of what you‚Äôre trying to achieve‚Äîspecifically, the purpose of your research. This clarity will not only help you reach your desired outcomes more efficiently but will also be invaluable when collaborating with others. Being able to explain your goals effectively ensures everyone is aligned and working toward the same objective.\nWe‚Äôll begin with an overview of common research goals and the types of data analysis needed to achieve them. Then, we‚Äôll discuss how to quantify the outcomes you‚Äôre trying to achieve. Finally, we‚Äôll explore how to visually communicate your research purpose, as visual representations are often the most effective way to convey complex ideas.\nIn research, your goal is to improve the status quo, whether by filling a knowledge gap or developing a new method, material, or process with better properties. Most research questions can be categorized into four broad groups, each associated with a specific type of analytics approach (Figure¬†1.1).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#types-of-research-questions",
    "href": "01_purpose.html#types-of-research-questions",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Figure¬†1.1: Descriptive, diagnostic, predictive, and prescriptive analytics, with increasing computational complexity and need to write custom code.\n\n\n\n\nDescriptive Analytics\nThis approach focuses on observing and describing phenomena to establish baseline measurements or track changes over time.\nExamples include:\n\nIdentifying animal and plant species in unexplored regions of the deep ocean.\nMeasuring the physical properties of a newly discovered material.\nSurveying the political views of the next generation of teenagers.\n\nMethodology:\n\nCollect a large amount of data (e.g., samples or observations).\nCalculate summary statistics like averages, ranges, or standard deviations, typically using standard software tools.\n\n\n\nDiagnostic Analytics\nHere, the goal is to understand relationships between variables and uncover causal chains to explain why phenomena occur.\nExamples include:\n\nInvestigating how CO2 emissions from burning fossil fuels drive global warming.\nEvaluating whether a new drug reduces symptoms and under what conditions it works best.\nExploring how economic and social factors influence shifts toward right-wing political parties.\n\nMethodology:\n\nPerform exploratory data analysis, such as looking for correlations between variables.\nConduct statistical tests to support or refute hypotheses (e.g., comparing treatment and placebo groups).\nDesign of experiments to control for external factors (e.g., randomized clinical trials).\nBuild predictive models to simulate relationships. If these models match real-world observations, it suggests their assumptions correctly represent causal effects.\n\n\n\nPredictive Analytics\nThis method involves building models to describe and predict relationships between independent variables (inputs) and dependent variables (outputs). These models often rely on insights from diagnostic analytics, such as which variables to include in the model and how they might interact (e.g., linear or nonlinear dependence). Despite its name, this approach is not just about predicting the future, but used to estimate unknown values in general (e.g., variables that are difficult or expensive to measure). It also includes any kind of simulation model to describe a process virtually (i.e., to conduct in silico experiments).\nExamples include:\n\nWeather forecasting models.\nDigital twin of a wind turbine to simulate how much energy is generated under different conditions.\nPredicting protein folding based on amino acid sequences.\n\nMethodology:\nThe key difference lies in how much domain knowledge informs the model:\n\nWhite-box (mechanistic) models: Based entirely on known principles, such as physical laws or experimental findings. These models are often manually designed, with parameters fitted to observed data.\nBlack-box (data-driven) models: Derived primarily from observational data. Researchers usually test different model types (e.g., neural networks or Gaussian processes) and choose the one with the highest accuracy.\nGray-box (hybrid) models: These combine mechanistic and data-driven approaches. For example, the output of a mechanistic model may serve as an input to a data-driven model, or the data-driven model may predict residuals (i.e., prediction errors) from the mechanistic model, where both outputs combined yield the final prediction.\n\n\n\n\n\n\nResources to learn more about data-driven models\n\n\n\n\n\nIf you want to learn more about how to create data-driven models and the machine learning (ML) algorithms behind them, these two free online books are highly recommended:\n\n[2] Supervised Machine Learning for Science by Christoph Molnar & Timo Freiesleben; A fantastic introduction focused on applying black-box models in scientific research.\n[3] A Practitioner‚Äôs Guide to Machine Learning by me; A broader overview of ML methods for a variety of use cases.\n\n\n\n\n\nAfter developing an accurate model, researchers can analyze its behavior (e.g., through a sensitivity analysis, which examines how outputs change with varying inputs) to gain further insights about the system (to feed back into diagnostic analytics).\n\n\nPrescriptive Analytics\nThis approach focuses on decision-making and optimization, often using predictive models.\nExamples include:\n\nScreening thousands of drug candidates to find those most likely to bind with a target protein.\nOptimizing reactor conditions to maximize yield while minimizing energy consumption.\n\nMethodology:\n\nDecision support: Use models for ‚Äúwhat-if‚Äù analyses to predict outcomes of different scenarios. For example, models can estimate the effects of limiting global warming to 2¬∞C versus exceeding that threshold, thereby informing policy decisions.\nDecision automation: Use models in optimization loops to systematically test input conditions, evaluate outcomes (e.g., resulting predicted material quality), and identify the best conditions automatically.\n\n\n\n\n\n\nModel accuracy is crucial\n\n\n\nThese recommendations are only as good as the underlying models. Models must accurately capture causal relationships and often need to extrapolate beyond the data used to build them (e.g., for disaster simulations). Data-driven models are typically better at interpolation (predicting within known data ranges), so results should ideally be validated through additional experiments, such as testing the recommended new materials in the lab.\n\n\n\nTogether, these four types of analytics form a powerful toolkit for tackling real-world challenges: descriptive analytics provides a foundation for understanding, diagnostic analytics uncovers the causes behind observed phenomena, predictive analytics models future scenarios based on this understanding, and prescriptive analytics turns these insights into actionable solutions. Each step builds on the previous one, creating a systematic approach to answering complex questions and making informed decisions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#evaluation-metrics",
    "href": "01_purpose.html#evaluation-metrics",
    "title": "1¬† Research Purpose",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nTo demonstrate the impact of your work and compare your solution against existing approaches, it‚Äôs crucial to define what success looks like quantitatively. Consider these common evaluation metrics to measure the outcome of your research and generate compelling results:\n\nNumber of samples: This refers to the amount of data you‚Äôve collected, such as whether you surveyed 100 or 10,000 people. Larger sample sizes can provide more robust and reliable results. But you also need to make sure your sample is representative of the population as a whole, i.e., to avoid sampling bias.\nReliability of measurements: This evaluates the consistency of your data. For example, how much variation occurs if you repeat the same measurement, e.g., run a simulation with different random seeds. This is important as others need to be able to reproduce your results.\nStatistical significance: The outcome of a statistical hypothesis test, such as a p-value that indicates whether the difference in symptom reduction between the treatment and placebo groups is significant.\nModel accuracy: For predictive models, this includes:\n\nStandard metrics like \\(R^2\\) to measure how closely the model‚Äôs predictions align with observational data.\nCross-validation scores to assess performance on new data.\nUncertainty estimates to understand how confident the model is in its predictions.\n\nAlgorithm performance: This includes metrics like memory usage and the time required to fit a model or make predictions, and how these values change as the dataset size increases. Efficient algorithms are crucial when scaling to large datasets or handling complex simulations.\nKey Performance Indicators (KPIs): These are the practical measures that matter in your field. For example:\n\nFor a chemical process: yield, purity, energy efficiency\nFor a new material: strength, durability, cost\nFor an optimization task: convergence time, solution quality\n\n\nYour evaluation typically involves multiple metrics. For example, in prescriptive analytics, you need to demonstrate both the accuracy of your model and that the recommendations generated with it led to a genuinely optimized process or product. Before starting your research, review similar work in your field to understand which metrics are standard in your community.\nIdeally, you should already have an idea of how existing solutions perform on these metrics (e.g., based on findings from other publications) to establish the baseline your solution should outperform. You‚Äôll likely need to replicate at least some of these baseline results (e.g., by reimplementing existing models) to ensure your comparisons are not influenced by external factors. But understanding where the ‚Äúcompetition‚Äù stands can also help you identify secondary metrics where your solution could excel. For example, even if there‚Äôs little room to improve model accuracy, existing solutions might be too slow to handle large datasets efficiently.1\nThese results are central to your research (and publications), and much of your code will be devoted to generating them, along with the models and simulations behind them. Clearly defining the key metrics needed to demonstrate your research‚Äôs impact will help you focus your programming efforts effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#draw-your-why",
    "href": "01_purpose.html#draw-your-why",
    "title": "1¬† Research Purpose",
    "section": "Draw Your Why",
    "text": "Draw Your Why\nWhether you‚Äôre collaborating with colleagues, presenting at a conference, or writing a paper‚Äîclearly communicating the problem you‚Äôre solving and your proposed solution is essential.\nVisual representations are particularly powerful for conveying complex ideas. One effective approach is creating ‚Äúbefore and after‚Äù visuals that contrast the current state of the field with your proposed improvements (Figure¬†1.2).\nThe ‚Äúbefore‚Äù scenario might show a lack of data, an incomplete understanding of a phenomenon, poor model performance, or an inefficient process or material. The ‚Äúafter‚Äù scenario highlights how your research addresses these issues and improves on the current state, such as refining a predictive model or enhancing the properties of a new material.\n\n\n\n\n\n\nFigure¬†1.2: Exemplary research goals and corresponding ‚Äúbefore and after‚Äù visuals for descriptive, diagnostic, predictive, and prescriptive analytics tasks.\n\n\n\nAt this point, your ‚Äúafter‚Äù scenario might be based on a hypothesis or an educated guess about what your results will look like‚Äîand that‚Äôs totally fine! The purpose of visualizing your goal is to guide your development process. Later, you can update the picture with actual results if you decide to include it in a journal publication, for example.\nOf course, not all research goals are tied directly to analytics. Sometimes the main improvement is more qualitative, for example, focusing on design or functionality (Figure¬†1.3). Even in these cases, however, you‚Äôll often need to demonstrate that your new approach meets or exceeds existing solutions in terms of other key performance indicators (KPIs), such as energy efficiency, speed, or quality parameters like strength or durability.\n\n\n\n\n\n\nFigure¬†1.3: This example illustrates a task where a robot must reach its target (represented by money) as efficiently as possible. Original approach (left): The robot relied on information encoded in the environment as expected rewards. To determine the shortest path to the target, the robot required a large sensor (shown as a yellow circle) capable of scanning nearby fields to locate the highest reward. New approach (right): Instead of relying on reward values scattered across the environment, the optimal direction is now encoded directly in the current field. This eliminates the need for large sensors, as the robot only needs to read the value of its current position, enabling it to operate with a much smaller sensor and thereby reducing hardware costs. Additional quantitative evaluation: It still needs to be demonstrated that with the new approach, the robot reaches its target at least as quickly as with the original approach.\n\n\n\nGive it a try‚Äîdoes the sketch help you explain your research to your family?\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe problem you‚Äôre trying to solve.\nExisting solutions to this problem, i.e., the baseline you‚Äôre competing against.\nWhich metrics should be used to quantify your improvement on the current state.\n\n\n\n\n\n\n\n[1] Callaway E. Chemistry Nobel Goes to Developers of AlphaFold AI That Predicts Protein Structures. Nature 634(8034), 525‚Äì526 (2024).\n\n\n[2] Freiesleben T, Molnar C. Supervised Machine Learning for Science: How to Stop Worrying and Love Your Black Box. (2024).\n\n\n[3] Horn F. A Practitioner‚Äôs Guide to Machine Learning. (2021).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#footnotes",
    "href": "01_purpose.html#footnotes",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "For example, currently, a lot of research aims to replace traditional mechanistic models with data-driven machine learning models, as these enable significantly faster simulations. A notable example is the AlphaFold model, which predicts protein folding from amino acid sequences‚Äîa breakthrough so impactful it was recognized with a Nobel Prize [1].‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "02_data.html",
    "href": "02_data.html",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Data Types\nIn the previous chapter, we‚Äôve gained clarity on the problem you‚Äôre trying to solve and how to quantify the improvements your research generates. Now it‚Äôs time to dive deeper into what these results might actually look like and the data on which they are built.\nIn one form or another, you‚Äôre research will rely on data, both collected or generated by yourself and possibly others.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-types",
    "href": "02_data.html#data-types",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Structured vs.¬†Unstructured Data\nData can take many forms, but one key distinction is between structured and unstructured data (Figure¬†2.1).\n\n\n\n\n\n\nFigure¬†2.1: Structured and unstructured data.\n\n\n\nStructured data is organized in rows and columns, like in Excel spreadsheets, CSV files, or relational databases. Each row represents a sample or observation (a data point), while each column corresponds to a variable or measurement (e.g., temperature, pressure, household income, number of children).\nUnstructured data, in contrast, lacks a predefined structure. Examples include images, text, audio recordings, and videos, typically stored as separate files on a computer or in the cloud. While these files might include structured metadata (e.g., timestamps, camera settings), the data content itself can vary widely‚Äîfor instance, audio recordings can range from seconds to hours in length.\nStructured data is often heterogeneous, meaning it includes variables representing different kinds of information with distinct units or scales (e.g., temperature in ¬∞C and pressure in kPa). Unstructured data tends to be homogeneous; for example, there‚Äôs no inherent difference between one pixel and the next in an image.\n\n\n\n\n\n\nThis book focuses on structured data\n\n\n\nEven though unstructured data is common in science (e.g., microscopy images), for simplicity, this book focuses on structured data. Furthermore, for now we‚Äôll assume that your data is stored in an Excel or CSV file, i.e., a spreadsheet with rows (samples) and columns (variables), on your computer. Later in Chapter 6, we‚Äôll discuss more advanced options for storing and accessing data, such as databases and APIs.\n\n\n\n\nProgramming Data Types\nEach variable in your dataset (i.e., each column in your spreadsheet) is represented as a specific data type, such as:\n\nNumbers (integers for whole numbers or floats for decimals)\nStrings (text)\nBoolean values (true/false)\n\nIn programming, these are so-called primitive data types (as opposed to composite types, like arrays or dictionaries containing multiple values, or user-defined objects) and define how information is stored in computer memory.\n\n\n\n\n\n\nData types in Python\n\n\n\n\n\n# integer\ni = 42\n# float\nx = 4.1083\n# string\ns = \"hello world!\"\n# boolean\nb = False\n\n\n\n\n\nStatistical Data Types\nEven more important than how your data is stored, is understanding what your data means. Variables fall into two main categories:\n\nContinuous (numerical) variables represent measurable values (e.g., temperature, height). These are usually stored as floats or integers.\nDiscrete (categorical) variables represent distinct options or groups (e.g., nationality, product type). These are often stored as strings, booleans, or sometimes integers.\n\n\n\n\n\n\n\nMisleading data types\n\n\n\nBe cautious: a variable that looks numerical (e.g., 1, 2, 3) may actually represent categories. For example, a material_type column with values 1, 2, and 3 might correspond to aluminum, copper, and steel, respectively. In this case, the numbers are IDs, not quantities.\n\n\nRecognizing whether a variable is continuous or discrete is crucial for creating meaningful visualizations and using appropriate statistical models.\n\n\nTime Series Data\nAnother consideration is whether your data points are linked by time. Time series data often refers to numerical data collected over time, like temperature readings or sales numbers. These datasets are usually expected to exhibit seasonal patterns or trends over time.\nHowever, nearly all datasets involve some element of time. For example, if your dataset consists of photos, timestamps might seem unimportant, but they could reveal trends‚Äîlike changes in image quality due to new equipment.\n\n\n\n\n\n\nAlways record timestamps\n\n\n\nAlways include timestamps in your data or metadata to help identify potential correlations or unexpected trends over time.\n\n\nSometimes, you may be able to collect truly time-independent data (e.g., sending a survey to 1,000 people simultaneously and they all answer within the next 10 minutes). But usually, your data collection will take longer and external factors‚Äîlike an election during a longer survey period‚Äîmight unintentionally affect your results. By tracking time, you can assess and adjust for such influences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-analysis-results",
    "href": "02_data.html#data-analysis-results",
    "title": "2¬† Data & Results",
    "section": "Data Analysis Results",
    "text": "Data Analysis Results\nWhen analyzing data, the process is typically divided into two phases:\n\nExploratory Analysis: This involves generating a variety of plots to gain a deeper understanding of your data, such as identifying correlations between variables. It‚Äôs often a quick and dirty process to help you familiarize yourself with the dataset.\nExplanatory Analysis: This focuses on creating refined, polished plots intended for communicating your findings, such as in a publication or presentation. These visuals are designed to clearly convey your results to an audience that may not be familiar with your data.\n\n\nExploratory Analysis\nIn this initial analysis, the goal is to get acquainted with the data, check if the trends and relationships you anticipated are present, and uncover any unexpected patterns or insights.\n\nExamine the raw data:\n\nIs the dataset complete, i.e., does it contain all the variables and samples you expected?\n\nExamine summary statistics (e.g., mean, standard deviation (std), min/max values, missing value count, etc.):\n\nWhat does each variable mean? Given your understanding of the variable, are its values in a reasonable range?\nAre missing values encoded as NaN (Not a Number) or as ‚Äòunrealistic‚Äô numeric values (e.g., -1 while normal values are between 0 and 100)?\nAre missing values random or systematic (e.g., in a survey rich people are less likely to answer questions about their income or specific measurements are only collected under certain conditions)? This can influence how missing values should be handled, e.g., whether it makes sense to impute them with the mean or some other specific value (e.g., zero).\n\nExamine the distributions of individual (continuous) variables:\n\n\n\nHistogram, strip plot, violin plot, box plot, and summary statistics of the same values.\n\n\n\nAre there any outliers? Are these genuine edge cases or can they be ignored (e.g., due to measurement errors or wrongly encoded data)?\nIs the data normally distributed or does the plot show multiple peaks? Is this expected?\n\nExamine trends over time (by plotting variables over time, even if you don‚Äôt think your data has a meaningful time component, e.g., by lining up representative images according to their timestamps to see if there is a pattern):\n\n\n\nWhat caused these trends and what are their implications for the future? This plot shows fictitious data of the pressure in a pipe affected by fouling‚Äîthat is, a buildup of unwanted material on the pipe‚Äôs surface, leading to increased pressure. The pipe is cleaned at regular intervals, causing a drop in pressure. However, because the cleaning process is imperfect, the baseline pressure gradually shifts upward over time.\n\n\n\nAre there time periods where the data was sampled irregularly or samples are missing? Why?\nAre there any (gradual or sudden) data drifts over time? Are these genuine changes (e.g., due to changes in the raw materials used in the process) or artifacts (e.g., due to a malfunctioning sensor recording wrong values)?\n\nExamine relationships between two variables:\n\n\n\nDepending on the variables‚Äô types (continuous or discrete), relationships can be shown in scatter plots, box plots, or a table. Please note that not all interesting relations between the two variables can be detected through a high correlation coefficient, so you should always check the scatter plot for details.\n\n\n\nAre the observed correlations between variables expected?\n\nExamine patterns in multidimensional data (using a parallel coordinate plot):\n\n\n\nEach line in a parallel coordinate plot represents one data point, with the corresponding values for the different variables marked at the respective y-axis. The screenshot here shows an interactive plot created using the Python plotly library. By selecting value ranges for the different dimensions (indicated by the pink stripes), it is possible to spot interesting patterns resulting from a combination of values across multiple variables.\n\n\n\nDo the observed patterns in the data match your understanding of the problem and dataset?\n\n\n\n\nExplanatory Analysis\nMost of the plots you create during an exploratory analysis are likely for your eyes only. Any plots you do choose to share with a broader audience‚Äîsuch as in a paper or presentation‚Äîshould be refined to clearly communicate your findings. Since your audience is much less familiar with the data and likely lacks the time or interest to explore it in depth, it‚Äôs essential to make your results more accessible. This process is often referred to as explanatory analysis [1].\n\n\n\n\n\n\nDon‚Äôt force an exploratory analysis onto your audience\n\n\n\nDon‚Äôt ‚Äújust show all the data‚Äù and hope that your audience will make something of it‚Äîunderstand what they need to answer the questions they have.\n\n\n\nStep 1: Choose the right plot type\n\nGet inspired by visualization libraries (e.g., here or here), but avoid the urge to create fancy graphics; sticking with common visualizations makes it easier for the audience to correctly decode the presented information.\nDon‚Äôt use 3D effects!\nAvoid pie or donut charts (angles are hard to interpret).\nUse line plots for time series data.\nUse horizontal instead of vertical bar charts for audiences that read left to right.\nStart the y-axis at 0 for area & bar charts.\nConsider using small multiples or sparklines instead of cramming too much into a single chart.\n\n\n\n\nLeft: Bar charts (especially in 3D) make it hard to compare numbers over a longer period of time. Right: Trends over time can be more easily detected in line charts. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 2: Cut clutter / maximize data-to-ink ratio\n\nRemove border.\nRemove gridlines.\nRemove data markers.\nClean up axis labels.\nLabel data directly.\n\n\n\n\nCut clutter! [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 3: Focus attention\n\nStart with gray, i.e., push everything in the background.\nUse pre-attentive attributes like color strategically to highlight what‚Äôs most important.\nUse data labels sparingly.\n\n\n\n\nStart with gray and use pre-attentive attributes strategically to focus the audience‚Äôs attention. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 4: Make data accessible\n\nAdd context: Which values are good (goal state), which are bad (alert threshold)? Should the value be compared to another variable (e.g., actual vs.¬†forecast)?\nLeverage consistent colors when information is spread across multiple plots (e.g., data from a certain country is always drawn in the same color).\nAnnotate the plot with text explaining the main takeaways (if this is not possible, e.g., in interactive dashboards where the data keeps changing, the title can instead include the question that the plot should answer, e.g., ‚ÄúIs the material quality on target?‚Äù).\n\n\n\n\nTell a story. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#draw-your-what",
    "href": "02_data.html#draw-your-what",
    "title": "2¬† Data & Results",
    "section": "Draw Your What",
    "text": "Draw Your What\nYou may not have looked at your data yet‚Äîor maybe you haven‚Äôt even collected it‚Äîbut it‚Äôs important to start with the end in mind.\nIn software development, a UX designer typically creates mockups of a user interface (like the screens of a mobile app) before developers begin coding. Similarly, in our case, we want to start with a clear picture of what the output of our program should look like. The difference is that, instead of users interacting with the software themselves, they‚Äôll only see the plots or tables that your program generated, maybe in a journal article.1\nBased on your takeaways from the previous chapter‚Äîabout the problem you‚Äôre solving and the metrics you should use to evaluate your solution‚Äîtry sketching what your final results might look like. Ask yourself: What figures or tables would best communicate the advantages of my approach?\nDepending on your research goals, your results might be as simple as a single number, such as a p-value or the total number of people surveyed. However, if you‚Äôre reading this, you‚Äôre likely tackling something that requires a more complex analysis. For example, you might compare your solution‚Äôs overall performance to several baseline approaches or illustrate how your solution converges over time (Figure¬†2.2).\n\n\n\n\n\n\nFigure¬†2.2: Exemplary envisioned results: The plots show the outcome of a multi-agent simulation, where ‚Äòmy approach‚Äô clearly outperforms two baseline methods. In this simulation, a group of agents is tasked with locating a food source and transporting the food back to their home base piece by piece. The ideal algorithm identifies the shortest path to the food source quickly to maximize food collection. Each algorithmic approach is tested 10 times using different random seeds to evaluate reliability. The plots display the mean and standard deviations across these runs. Left: How quickly each algorithm converges to the shortest path (resulting in the highest number of agents delivering food back to the home base per step). Right: Cumulative food collected by the end of the simulation.2\n\n\n\nIt‚Äôs important to remember that your actual results might look very different from your initial sketches‚Äîthey might even show that your solution performs worse than the baseline. This is completely normal. The scientific method is inherently iterative, and unexpected results are often a stepping stone to deeper understanding. By starting with a clear plan, you can generate results more efficiently and quickly pivot to a new hypothesis if needed. When your results deviate from your expectations, analyzing those differences can sharpen your intuition about the data and help you form better hypotheses in the future.\nOnce you‚Äôve visualized the results you want, work backward to figure out what data you need to create them. This is especially important when you‚Äôre generating the data yourself, such as through simulations. For instance, if you plan to plot how values change over time, you‚Äôll need to record variables at every time step rather than just saving the final outcome of a simulation (duh!). Similarly, if you want to report your model‚Äôs accuracy (Figure¬†2.3), you‚Äôll need:\n\nInput variables for each data point to generate predictions (= model output).\nThe actual (true) values for each data point.\nA way to compute the overall deviation between predictions and true values, such as using an evaluation metric like \\(R^2\\).\n\n\n\n\n\n\n\nFigure¬†2.3: Work backward from the desired results to determine what data is necessary to create them.\n\n\n\nBy working backward from your desired results to the required data, you can design your code and analysis pipeline to ensure your program delivers exactly what you need.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe specific results (tables and figures) you want to create to show how your solution outperforms existing approaches (e.g., in terms of accuracy, speed, etc.).\nThe underlying data needed to produce these results (e.g., what rows and columns should be in your spreadsheet).\n\n\n\n\n\n\n\n[1] Knaflic CN. Storytelling with Data: A Data Visualization Guide for Business Professionals. John Wiley & Sons (2015).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#footnotes",
    "href": "02_data.html#footnotes",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "A former master‚Äôs student that I mentored humorously called this approach ‚Äúplot-driven development,‚Äù a nod to test-driven development (TDD) in software engineering, where you write a test for your function first and then implement the function to pass the test. You could even use these sketches of your results as placeholders if you‚Äôre already drafting a paper or presentation.‚Ü©Ô∏é\nThese plots and the next were generated with Python using matplotlib‚Äôs plt.xkcd() setting and the xkcd script font. A pen and paper sketch will be sufficient for your case.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "03_tools.html",
    "href": "03_tools.html",
    "title": "3¬† Tools",
    "section": "",
    "text": "Programming Languages\nBefore we continue with creating your results‚Äîi.e., actually start developing software‚Äîlet‚Äôs take a quick tour of some tools that can make your software engineering journey smoother.\nAlthough the code examples in this book use Python, the general principles discussed here apply to most programming languages.\nDifferent programming languages suit different needs. Here‚Äôs a quick overview of some popular ones used in science and engineering:\nDue to its broad applicability and popularity in industry, Python is used for the examples in this book. However, you should choose the programming language that is most popular in your field as this will make it easier for you to find relevant resources (e.g., tailored libraries) and collaborate with colleagues.\nThere are plenty of great books and other resources available to teach you programming fundamentals, which is why this book focuses on higher level concepts. Going forward we‚Äôll assume that you‚Äôre familiar with the basic syntax and functionality of your programming language of choice (incl.¬†key scientific libraries). For example, to learn Python essentials, you can work through this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#programming-languages",
    "href": "03_tools.html#programming-languages",
    "title": "3¬† Tools",
    "section": "",
    "text": "R: Commonly used for statistics, with rich functionality to create data visualizations, fit statistical models (like different types of regression), and conduct advanced statistical tests (like ANOVA). The poplar Shiny framework also makes it possible to create interactive dashboards that run as web applications.\nMATLAB: Once dominant in engineering, used for simulations. But due to its high licensing costs, MATLAB is being replaced more and more by Python and Julia.\nJulia: Gaining traction in scientific computing for its speed and modern syntax.\nPython: A versatile language with strong support for data science, AI, web development, and more. Its active open source community has created many popular libraries for scientific computing (numpy, scipy), machine learning (scikit-learn, TensorFlow, PyTorch), and web development (FastAPI, streamlit).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#version-control",
    "href": "03_tools.html#version-control",
    "title": "3¬† Tools",
    "section": "Version Control",
    "text": "Version Control\nVersion control is essential in software development to keep track of code changes and collaborate effectively. Think of it as a time machine that lets you revert to any version of your code or examine how it evolved.\n\nWhy Use Version Control?\n\nTrack changes: See what you‚Äôve modified and when, with the ability to revert if necessary.\nReview collaborators‚Äô changes: When working with others, reviewing their changes before they are merged with the main version of the code (in so-called pull or merge requests) ensures quality and provides opportunities to teach each other better ways of doing things.\nNot just for code: Version control can be used for any kind of file. While it‚Äôs less effective for binary formats like images or Microsoft Word where you can‚Äôt create a clean ‚Äúdiff‚Äù between two versions, you should definitely give it a try when writing your next paper in a text-based format like LaTeX.\n\n\n\nGit\nThe go-to tool for version control is Git. While desktop clients exist, many professionals use git directly in the terminal as a command line tool.\nIf you‚Äôre new to Git, this beginner‚Äôs guide is a great place to start.\n\n\n\n\n\n\nEssential Git Commands\n\n\n\n\n\n\ngit init: Start a new repository in the current folder.\ngit status: View changes.\ngit diff: View differences between file versions before committing.\ngit add [file]: Stage files for a commit.\ngit commit -m \"message\": Save staged changes.\ngit push: Upload changes to a remote repository (e.g., on GitHub).\ngit pull: Download changes from a remote repository.\ngit branch: Create or list branches.\ngit checkout [branch]: Switch branches.\ngit merge [branch]: Combine branches.\n\n\n\n\nBy default, your repository‚Äôs files are on the main branch. Creating a new branch is like stepping into an alternate universe where you can experiment without affecting the main timeline. When making a major change or adding a new feature, it‚Äôs good practice to create a new branch, like new-feature, and implement your changes there. Once you‚Äôre satisfied with the result, you can merge the changes back into the main branch.\nThis approach keeps the main branch stable and ensures you always have a working version of your code. If you decide against your new feature, you can simply abandon the branch and start fresh from main. By creating a merge request (MR) once your new-feature branch is ready, you or a collaborator can review the changes thoroughly before merging them into main.\nTo publish your code or collaborate with others, your repository (i.e., the folder under version control) can be hosted on a platform like:\n\nGitHub: Great for open-source projects and public personal repositories to show off your skills.\nGitLab: Supports self-hosting, making it ideal for organizational needs.\n\nWe strongly encourage you to publish any code related to your publications on one of these platforms to promote reproducibility of your results! üë©‚Äçüî¨\n\n\n\n\n\n\nData Versioning\n\n\n\nIn addition to the changes made to your code, you should also keep track of how your data is generated and transformed over time (data lineage). While small datasets can be included in your repository (e.g., in a separate data/ folder), there are also more tailored tools available specifically to version your data, like DVC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#development-environment",
    "href": "03_tools.html#development-environment",
    "title": "3¬† Tools",
    "section": "Development Environment",
    "text": "Development Environment\nThe program you choose for writing code directly impacts your productivity. While you can technically write code using a plain text editor (like Notepad on Windows or TextEdit on macOS), special-purpose text editors and integrated development environments (IDEs) provide a tailored experience that boosts productivity.\n\nText Editors\nDeveloper-focused text editors are lightweight tools with features like syntax highlighting and extensions for basic programming tasks.\nExamples include:\n\nSublime Text: Lightweight and fast, with excellent customization through lots of plugins.\nAtom: Open-source and backed by GitHub (though less popular than other tools).\nVim and Emacs: Some of the first code editors, often used as command line tools and beloved by keyboard shortcuts enthusiasts.\n\n\n\nFull IDEs\nFor more features, IDEs integrate tools like file browsers, Git support, and debuggers. They are ideal for larger projects and provide support for more complex tasks, like renaming variables across multiple files when you‚Äôre refactoring your code.\nExamples include:\n\nVS Code: Minimalist by default but highly customizable with plugins, making it suitable for everything from basic editing to full-scale development.\nJetBrains IDEs (e.g., PyCharm): IDEs tailored to the needs of specific programming languages with very advanced features. You need to purchase a license to use the full version, but for many IDEs there is also a free community edition available.\nJupyterLab: An extension of Jupyter notebooks (see below), popular for data science and exploratory coding.\nRStudio: Tailored for R programming, with excellent support for data visualization, markdown reporting, and reproducible research workflows.\nMATLAB: The MATLAB programming language and IDE are virtually synonymous. However, its rich feature set comes with steep licensing fees.\n\n\n\nJupyter Notebooks\nJupyter notebooks are a unique format that lets you mix code, output (like plots), and explanatory text in one document. The name Jupyter is derived from Julia, Python, and R, the programming languages for which the notebook format, and later the JupyterLab IDE, were created. The IDE itself runs inside your web browser.\nNotebooks are great for exploratory data analysis and to create reproducible reports. However, since the notebooks themselves are composed of individual interactive cells that can be executed in any order, developing in notebooks often becomes messy quickly. We recommend that you keep the main logic and reusable functions in separate scrips or libraries and primarily use notebooks to create plots and other results. It is also good practice to run your notebook again from top to bottom once you‚Äôre finished to make sure everything still works and you‚Äôre not relying on variables that were defined in now-deleted cells, for example.\n\n\n\n\n\n\nNotebooks as text files\n\n\n\nJupyter notebooks, stored as files ending in .ipynb, are internally represented as JSON documents. If you have your notebooks under version control (which you should üòâ), you‚Äôll notice that the diffs between versions look quite bloated. But do not despair! Tools like Jupytext can convert notebooks into plain text without loss of functionality.\n\n\n\n\n\n\n\n\nParameterize notebooks\n\n\n\nIf you want to execute the same notebook with multiple different parameter settings (e.g., create the same plots for different model configurations), have a look at papermill.\n\n\nIn addition to the original JupyterLab IDE and notebooks that you install on your computer, there are also free cloud-based options available, such as Google Colab, which even gives you free compute time on GPUs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#reproducible-setups",
    "href": "03_tools.html#reproducible-setups",
    "title": "3¬† Tools",
    "section": "Reproducible Setups",
    "text": "Reproducible Setups\n‚ÄúIt works on my machine‚Äù isn‚Äôt good enough for science. Reproducibility means your results can be replicated by others (and by you a few months later when the reviewers of your paper request changes to your experiments). The first step to achieve this is to manage your dependencies (i.e., external libraries used by your code) to ensure the environment in which your code is executed is identical for everyone that runs your code, every time. This can be done using virtual environments, or, if you want to go even further, containers like Docker, which will be discussed in Chapter 6.\n\n\n\n\n\n\nVirtual Environments in Python with poetry\n\n\n\n\n\nVirtual environments isolate your project‚Äôs dependencies, thereby ensuring consistency. For Python, a common tool to do this is poetry. It tracks the libraries and their versions in a pyproject.toml like this:\n[tool.poetry]\nname = \"example-project\"\nversion = \"0.1.0\"\ndescription = \"A sample Python project\"\nauthors = [\"Your Name &lt;youremail@example.com&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\nrequests = \"^2.26.0\"  # external libraries incl. versions\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nBasic commands:\n\npoetry new example-project: Create a new project (folder incl.¬†pyproject.toml file).\npoetry add [package]: Add a dependency (can also be done directly in the file).\npoetry install: Install all dependencies.\npoetry shell: Activate the virtual environment.\n\n\n\n\n\nHandling Randomness\nYour program will often depend on randomly sampled values, for example, when defining the initial conditions for a simulation or initializing a model before it is fitted to data (like a neural network). To ensure that your experiments can be reproduced, it is important that you always set a random seed at the beginning of your program so the random number generator starts from a consistent state.\n\n\n\n\n\n\nSetting Random Seeds in Python\n\n\n\n\n\nAt the beginning of your script, set a random seed (depending on the library that you‚Äôre using this can vary):\nimport random\nimport numpy as np\n\nrandom.seed(42)\nnp.random.seed(42)\n\n\n\nTo get a better idea of how much your results depend on the random initialization and therefore how robust they are, it is advisable to always run your code with multiple random seeds and compare the results (e.g., compute the mean and standard deviation of the outcomes of different runs like in Figure¬†2.2).\n\n\n\n\n\n\nRandom state at startup\n\n\n\nDepending on the programming language that you‚Äôre using, if you run a script without executing any other code before, the random number generator may or may not always start in the same state. This means, if you don‚Äôt set a random seed and, for example, run your script ten times from scratch, you may always receive the same result even though the results would differ if the code was run under different circumstances. To avoid surprises, you should always explicitly set the random seed to have more control over the results.\n\n\n\n\n\n\n\n\nHardware differences\n\n\n\nIf your code is run on very different hardware, e.g., a CPU vs.¬†a GPU (graphics card, used to train neural network models, for example), despite setting a random seed, your results might still differ slightly. This is due to how the different architectures internally represent float values, i.e., with what precision the numbers are stored in memory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#clean-and-consistent-code",
    "href": "03_tools.html#clean-and-consistent-code",
    "title": "3¬† Tools",
    "section": "Clean and Consistent Code",
    "text": "Clean and Consistent Code\nEspecially when working together with others, it can be helpful to follow to a style guide to produce clean and consistent code. Google published their style guides for multiple programming languages, which is a great resource and adhering to these rules will also help you to avoid common sources of bugs.\n\nFormatters & Linters\nSince programmers are often rather lazy, they developed tools that automatically fix your code to implement these rules where possible:\n\nFormatters rewrite code to follow a consistent style (e.g., add whitespace after commas).\nLinters analyze code for errors, inefficiencies, and deviations from best practices.\n\n\n\n\n\n\n\nFormatter & Linter in Python: ruff\n\n\n\n\n\nruff is a (super fast) formatter and linter for Python, written in Rust. You can install it via pip and configure it in the same pyproject.toml file that we also used for poetry. Then run it over you code like this:\nruff check        # see which errors the linter finds\nruff check --fix  # automatically fix errors where possible\nruff format       # automatically format the code\nYou‚Äôll probably want to add exceptions for some of the errors that the linter checks for in your pyproject.toml file as ruff is quite strict. üòâ\n\n\n\nIt is important to have the configuration for your formatter and linter under version control as well, so that all collaborators use the same settings and you avoid unnecessary changes (and bloated diffs in merge requests) when different people format the code.\n\n\nPre-commit Hooks\nIn the heat of the moment, you might forget to run the formatter and linter over your code before committing your changes. To avoid accidentally checking messy code into your repository, you can configure so-called ‚Äúpre-commit hooks‚Äù. Pre-commit hooks catch issues automatically by enforcing coding standards before committing or pushing code with git.\n\n\n\n\n\n\nSetting up pre-commit hooks\n\n\n\n\n\nFirst, you need to install pre-commit hooks through Python‚Äôs package manger pip:\npip install pre-commit\nThen configure it in a file named .pre-commit-config.yaml (here done for ruff):\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n    - id: check-yaml\n    - id: end-of-file-fixer\n    - id: trailing-whitespace\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.8.3\n  hooks:\n    # Run the linter.\n    - id: ruff\n      args: [ --fix ]\n    # Run the formatter.\n    - id: ruff-format\nThen install the git hook scripts from the config file:\npre-commit install\nNow the configured hooks will be run on all changed files when you try to commit them and you can only proceed if all checks pass.\n\n\n\nTo catch any style inconsistencies after the code was pushed to your remote repository (e.g., in case one of your collaborators has not installed the pre-commit hooks), you can also add these checks to your CI/CD pipeline (see Chapter 6).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#putting-it-all-together",
    "href": "03_tools.html#putting-it-all-together",
    "title": "3¬† Tools",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nWhen you set up all these tools, your repository should now look something like this (see here for more details; setup for programming languages other than Python will differ slightly):\nproject-name/\n‚îú‚îÄ‚îÄ .gitignore              # Exclude unnecessary files from version control\n‚îú‚îÄ‚îÄ README.md               # Describe the project purpose and usage\n‚îú‚îÄ‚îÄ pre-commit-config.yaml  # Pre-commit hook setup\n‚îú‚îÄ‚îÄ pyproject.toml          # Python dependencies and configs\n‚îú‚îÄ‚îÄ data/                   # Store (small) datasets\n‚îú‚îÄ‚îÄ notebooks/              # For exploratory analysis\n‚îú‚îÄ‚îÄ src/                    # Core source code\n‚îî‚îÄ‚îÄ tests/                  # Unit tests\nA clean project structure makes it easier to maintain your code.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to set up your development environment to code efficiently.\nHow to host your version-controlled repository on a platform like GitHub or GitLab, complete with pre-commit hooks to ensure well-formatted code.\nThe fundamental syntax of your programming language of choice (incl.¬†key scientific libraries) to get started.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_design.html",
    "href": "04_design.html",
    "title": "4¬† Software Design",
    "section": "",
    "text": "Avoid Complexity\nNow that your code repository is set up, are you itching to start programming? Hold on for a moment!\nOne of the most common missteps I‚Äôve seen junior developers take is jumping straight into coding without first thinking through what they actually want to build. Imagine trying to construct a house by just laying bricks without consulting an architect first‚Äîhalfway through you‚Äôd probably realize the walls don‚Äôt align, and you forgot the plumbing for the kitchen. You‚Äôd have to tear it down and start over! To avoid this fate for your software, it‚Äôs essential to make a plan and design the final outcome first. It doesn‚Äôt have to be perfect‚Äîa quick sketch on paper will do. And you‚Äôll likely need to adapt as you go (which is fine since we‚Äôll design with flexibility in mind!). But the more thought you put into planning, the smoother and faster execution will be.\nTo make sure your designs will be worthy of implementation, this chapter also covers key paradigms and best practices that will help you create clean, maintainable code that‚Äôs easy to extend and reuse in future projects.\nA common acronym in software engineering is KISS - ‚Äúkeep it simple, stupid!‚Äù. While this may be well-intentioned advice, it can only apply to individual parts of your code, as a full-fledged software is a system that consists of multiple components that interact with each other. Here, the best you can hope for is complicated, i.e., to avoid complexity (Figure¬†4.1).\nA complex system includes many elements that interact with each other in ways that are not easily traceable or for humans to comprehend. You might change something in one place and suddenly, something far on the other side breaks. That‚Äôs not good in software. We should always design for change, since inevitably, there is something we need to adapt or extend and when this happens, we want to be confident in what we‚Äôre doing and not afraid that our change will break something else or have unintended consequences that we‚Äôre not aware of. No one likes bugs.\nThis is why we want a complicated system: it still includes a lot of elements, but they are grouped in components, neat little subsystems, so the whole can be taken apart and each part can be understood on its own while the whole can also be understood without understanding each individual component in detail. Ideally, this means our system consists of a neat hierarchy of components at different levels of abstraction (Figure¬†4.2). To understand the code on one level of this hierarchy, we only need to understand what the components one level below are doing to get an idea of the purpose of the code. For example, to understand what is happening in plot_results, it is enough to know that there is a scatter plot created and we don‚Äôt need to know the details of how this plot is created, such as that it requires the computation of \\(R^2\\). By decomposing code in such a way, we reduce the cognitive load that is required to understand what the code is doing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#avoid-complexity",
    "href": "04_design.html#avoid-complexity",
    "title": "4¬† Software Design",
    "section": "",
    "text": "Figure¬†4.1: (Software) Systems can be of different complexity. A script or function that linearly executes a set of steps could be considered simple. But most software programs are (at least) complicated: they consist of multiple components that interact with each other. However, these can still be broken down into manageable subsystems, which makes it possible to understand the system as a whole. A complex system, in computer science referred to as ‚Äúspaghetti code‚Äù or a ‚Äúbig ball of mud‚Äù [1], contains many individual elements and interactions between them ‚Äì when you change something on one end it is unclear how this will affect the other pieces as it is difficult to understand how all the elements come together.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4.2: Complicated systems in software design can usually be represented as hierarchies that show different levels of abstraction, e.g., in this case for plotting the results of a predictive model, i.e., creating a scatter plot that shows the true vs.¬†predicted values together with the \\(R^2\\) value that indicates the overall performance of the model.\n\n\n\n\nElements of Software Systems\nBefore we continue, lets take a quick look at what the fundamental building blocks of our software systems are. To keep things simple, we distinguish between:\n\nVariables: Used to store data values, either primitive (like integers or strings, like we saw in Chapter 2) or composite, like lists and dictionaries, which can grow arbitrarily complex in some languages (e.g., a list can contain multiple dictionaries that themselves contain lists etc.).\n# primitive data type: float\nx = 4.1083\n# composite data type: list\nmy_list = [\"hello\", 42, x]\n# composite data type: dict\nmy_dict = {\n    \"key1\": \"hello\",\n    \"key2\": 42,\n    \"key3\": my_list,\n}\nFunctions: Used to calculate something and/or perform an action, usually given some input arguments. We distinguish between pure and impure functions. A pure function is similar to a mathematical function \\(f: x \\to y\\) that computes and returns \\(y\\) given some \\(x\\). Impure functions have so-called ‚Äúside effects‚Äù, i.e., they perform some action that has effects that are visible outside of the function itself, e.g., because they create or modify a file. They also don‚Äôt necessarily have a return value.\ndef add(a, b):\n    # pure: a simple calculation\n    return a + b\n\ndef create_plot(x, y):\n    # impure: create and save a plot\n    plt.plot(x, y)\n    plt.savefig(\"my_figure.png\")\nObjects: They are basically a combination of variables and functions, i.e., specific constructs that store data values and have functions that usually access and modify these values. This comes in handy if you want to group multiple variables in one place because they logically belong together (e.g., the parameters used to configure a simulation model) and you want to use them store the results from functions.\nclass MyModel:\n\n  def __init__(self, param1, param2):\n      # initialize values from input arguments\n      self.param1 = param1\n      self.param2 = param2\n      # create additional variables\n      self.results = []\n\n  def run_simulation():\n      # do something\n      self.result = [1, 2, 3]\n\n\n\nSystem Boundaries\nTo avoid creating a complex mess, it is very important that we are clear on our (sub)system‚Äôs boundaries, i.e., what is in and what is out of our code‚Äôs scope. Adhering to clear boundaries makes it very easy to change our code. Unfortunately, though, establishing clear boundaries is easier said than done.\nLet‚Äôs start with a very simple example to illustrate the concept of scope:\ndef n_times_x(x, n):\n    result = 0\n    for i in range(n):\n        result += x\n    return result\n\nif __name__ == '__main__':\n    new_x = 3\n    new_n = 5\n    # call our function with some values\n    my_result = n_times_x(new_x, new_n)\n    print(my_result)  # should show 15\nOur code of interest (i.e., the subsystem we‚Äôre looking at) is the function n_times_x. Inside the scope of this function with have the variables x, n, i, and result, i.e., when we can refer to them by name and when we execute the code it is clear what we‚Äôre referring to. Outside of the scope of the function, where it is called in the program‚Äôs __main__ function, we have the variables new_x, new_n, and my_result. If we tried to access any of the internal variables from n_times_x here, we would get an error, since these variables are hidden inside the scope of n_times_x.\n\n\n\n\n\n\nNote\n\n\n\nWhile under some circumstances, depending on how you structure your code, the ‚Äúinside code‚Äù could access the ‚Äúoutside code‚Äù‚Äôs variables, it is best to avoid this, i.e., try to have a clean separation between what is going on inside and outside of your code. Ideally, your code should only be concerned about what is going on inside your code and not depend on anything that exists outside of it. Therefore, if it needs access to any variables, you should just pass them as input arguments explicitly to make it clear what values your code uses.\n\n\nThis brings us to the boundaries, i.e., where ‚Äúinside‚Äù and ‚Äúoutside‚Äù code meet. In software development, the boundary of your ‚Äúinside code‚Äù is also referred to as its interface. This defines how you interact with the code and it can be seen as a contract with the outside world for how to use your code. For a pure function, like in our example, the interface is the function‚Äôs signature, i.e.,\n\nThe function name (n_times_x).\nThe input arguments (x and n).\nThe return values (result).\n\nAs long as we keep this interface the same, i.e., the function still calculates the same thing, we‚Äôre free to change whatever we want inside the function itself. For example, we could change the ridiculously inefficient implementation to use a proper multiplication:\ndef n_times_x(x, n):\n    result = n * x\n    return result\nAnd we don‚Äôt have to tell anyone about this change, since we kept the interface of the function the same and none of the outside code was aware or depended upon what was going on inside our function. This is the beauty of clear boundaries.\nOn the other hand, if you decide that you don‚Äôt like your function‚Äôs name, changing this means that everywhere else where your function is used you also need to change the name. When you use an IDE to code, it probably has a feature to refactor your code where you can indicate that you want to change the name and then it checks all the files in your project for where this function is called and then changes the name there as well. However, if you‚Äôre writing a library that is used in multiple place outside your current project, it will be very difficult to identify all the places where this function is used and notify the respective people to change the name. In practice this requires a deprecation process where the old interface is slowly phased out. This is why it really pays of to define good interfaces that remain stable for a long time.\n\n\n\n\n\n\nGo deep\n\n\n\nPowerful code has narrow interfaces with a deep implementation, i.e., the code does meaningful computations without exposing too much of its internals. For example, a narrow and deep function would maybe have two input arguments but extend over ten lines of code to do a complex calculation. When you split your code up into reusable functions, make sure that you don‚Äôt split it up so much that you end up with a function that takes six input arguments but then only computes something on one line. Instead, try to create meaningful units of code that help you to hide complicated logic behind a simple interface and thereby make your code easier to change.\n\n\nPure functions with clean boundaries help us to write easily understandable code without any surprises. They are also easy to test, since they do not depend on anything besides what is passed as their input arguments, so no matter how many times you call a pure function with the same inputs, you‚Äôre always going to get the same output. Your code gets more complex with impure functions: by definition, they have side effects and therefore interact and rely on the outside world. For example, they could write results to a file (e.g., create a plot) or read values from a database. This also means that if you run the code twice, you might get different results because something in the outside world has changed, e.g., your code crashes because the results file already exist or the output is different because someone has changed the values in the database. This can make it harder to predict how changes to your code or somewhere else will affect the system as a whole, for example, because you don‚Äôt know who else is accessing the same resources that your code uses.\nAs a best practice, you should always try to encapsulate as much critical logic as possible in pure functions, as this makes your code easier to understand and test. For example:\ndef pure_function(inputs):\n    # do something without side effects\n    output = ...\n    return output\n\ndef impure_function():\n    # read from outside file/database\n    data = ...\n    # do the main calculations\n    result = pure_function(data)\n    # write to outside file/database\n    result.to_csv(\"result.csv\")\n\n\n\n\n\n\nCall-By-Value vs.¬†Call-By-Reference\n\n\n\nDepending on the programming language that you use, input arguments that are passed to a function can either be passed as their literal values (i.e., like getting a copy of the contents of the variable) or they can be passed as a reference, i.e., the function gets the location where the values reside in memory and can access (and manipulate!) them there. In the spirit of clean boundaries, we want to code by the principle ‚Äúwhat happens inside a function stays inside the function (except for the return values)‚Äù. However, when we pass a value by reference, this can result in unintended side effects (i.e., increase complexity), where we change the variables that was given to us from outside the function; a change which is then also visible outside of our code. In Python this can happen with complex data types like lists and dictionaries:\ndef change_list(a_list):\n    a_list[0] = 42\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    print(my_list)  # [1, 2, 3]\n    change_list(a_list)\n    print(my_list)  # [42, 2, 3] üò±\nTo be safe, you can create a copy of a variable before you modify it to make sure the original stays the same and thereby avoid these side effects that could otherwise result in confusing bugs.\nfrom copy import deepcopy\n\ndef change_list(a_list):\n    a_list = deepcopy(a_list)\n    a_list[0] = 42\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    change_list(a_list)\n    print(my_list)  # [1, 2, 3] üòä\n\n\n\nExtend your scope with objects\nSometimes, your code might depend on so many variables that you don‚Äôt want to pass all of these to a function. Especially when all of these variables and functionality are logically related, they should be grouped together into a class. A class, or an object, with is one concrete instantiation of a class, is great to create some extra scope for your functions.\nSince we want clean boundaries and ideally a narrow interface, it is important to distinguish between public and private attributes and methods of a class. Everything about a class that is public is part of its interface, i.e., the contract with the outside world, which means changing these later can cause extra work or issues elsewhere. Private attributes and methods are only for internal use and can therefore be changed more easily.\n\n\n\n\n\n\nPublic and Private in different programming languages\n\n\n\nDepending on the programming language that you use, there might be more access levels than public and private. For example, in Java there also exists protected and package. For our purposes it is sufficient to distinguish between what should be accessible internally (for your code only) and externally (part of the contract with the outside world).\nIn Python, nothing is really private as it is assumed that the programmer knows what she is doing when she accesses what she wants. By convention, variables and functions that are prefixed with _ or __ are for internal use only and you should only use them at your own risk. Therefore, it also makes sense to prefix all attributes and methods that you don‚Äôt want anyone else to mess with with underscores. While this does not provide any access guarantees, at least who ever uses these variables will be warned that they may change without notice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#draw-your-how",
    "href": "04_design.html#draw-your-how",
    "title": "4¬† Software Design",
    "section": "Draw Your How",
    "text": "Draw Your How\nNow it‚Äôs time for you to draw your design.\nBut keep it simple‚Äîyou don‚Äôt have to overengineer your design to account for every possibility. If you want to build a one family home, design a one family home. Of course, it can‚Äôt hurt to think ahead a little bit if you already know that some changes are likely to come (‚ÄúHow will the rooms change as the kids move out?‚Äù), but there is no value in trying to plan ahead for everything (‚ÄúWhat if the daughter wants to take over the house and transform it into an office for her 100+ people company?‚Äù).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#make-a-plan",
    "href": "04_design.html#make-a-plan",
    "title": "4¬† Software Design",
    "section": "Make a Plan",
    "text": "Make a Plan\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to design your program by building upon loosely coupled, reusable components.\nWhat steps your code entails.\n\n\n\n\n\n\n\n[1] Foote B, Yoder J. Big Ball of Mud. Pattern languages of program design 4, 654‚Äì692 (1997).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "05_implementation.html",
    "href": "05_implementation.html",
    "title": "5¬† Implementation",
    "section": "",
    "text": "From Design to Code: Fill in the Blanks\nNow that you have a plan, it‚Äôs finally time to get started with the implementation.\nArmed with your design from the last chapter, you can now translate your sketch into a code skeleton. Start by outlining the functions, place calls to them where needed, and add comments for any steps you‚Äôll figure out later. For example, the design from Figure X could result in the following draft:\nOnce your skeleton stands, you ‚Äúonly‚Äù need to fill in the details, which is a lot less intimidating than facing a blank page. Plus, since you started with a thoughtful design, your final program is more likely to be well-structured and easy to understand. Compare this to writing code on the fly, where decisions about functions are often made haphazardly‚Äîyou‚Äôll appreciate the difference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#from-design-to-code-fill-in-the-blanks",
    "href": "05_implementation.html#from-design-to-code-fill-in-the-blanks",
    "title": "5¬† Implementation",
    "section": "",
    "text": "Order of functions\n\n\n\nYour script likely includes multiple functions, so you‚Äôll need to decide their order from top to bottom. Since scripts typically start with imports (e.g., of libraries like numpy) and end with a main function, personally I prefer to put general functions that only rely on external dependencies at the top (i.e., the ones that are at the lower levels of abstraction in your call hierarchy). This ensures that, as you read the script from top to bottom, each function depends only on what was defined before it. Maintaining this order avoids circular dependencies and encourages you to write reusable, modular functions that serve as building blocks for the code that follows.\n\n\n\n\n\n\n\n\n\nUsing AI Code Generators\n\n\n\nAI assistants like ChatGPT or GitHub Copilot can be helpful tools when writing code, especially at the level of individual functions. However, remember that these tools only reproduce patterns from their training data, which includes both good and bad code. As a result, the code they generate may not always be optimal. For instance, they might use inefficient for-loops instead of more elegant matrix operations. Similarly, support for less popular programming languages may be subpar.\nTo get better results, consider crafting prompts like: ‚ÄúYou are a senior Python developer with 10 years of experience writing efficient, edge-case-aware code. Write a function ‚Ä¶‚Äù\n\n\n\nMinimum Viable Results\nIn product development, there‚Äôs a concept called the Minimum Viable Product (MVP). This refers to the simplest version of a product that still provides value to users. The MVP serves as a prototype to gather feedback on whether the product meets user needs and to identify which features are truly essential. By iterating quickly and testing hypotheses, teams can increase the odds of creating a successful product that people will actually pay for.\nThis approach also has motivational benefits. Seeing something functional‚Äîeven if basic‚Äîearly on makes it easier to stay engaged. It‚Äôs far better than toiling for months without tangible results. We recommend applying this mindset to your research software development by starting with a script that generates ‚ÄúMinimum Viable Results.‚Äù\nThis means creating a program that produces outputs resembling your final results, like plots or tables, but using placeholder data instead of actual values. For instance:\n\nIf your goal is to build a prediction model, start with one that simply predicts the mean of the observed data.\nIf you‚Äôre developing a simulation, begin with random outputs, such as a random walk.\n\nThis approach also serves as a ‚Äústupid baseline‚Äù‚Äîa simple, easy-to-beat reference point for your final method. It‚Äôs a sanity check: if your sophisticated solution can‚Äôt outperform this baseline, something‚Äôs off.\nBy starting with Minimum Viable Results, you can test your code end-to-end early on, see tangible progress, and iteratively improve from there.\n\n\nBreaking Code into Components\nStarting a new project often begins with all your code in a single script or notebook. This is fine for quick and small tasks, but as your project grows, keeping everything in one file becomes messy and overwhelming. To keep your code organized and easier to understand, it‚Äôs a good idea to move functionality into separate files, also called (sub)modules. Separating code into modules makes your project easier to navigate, test, and reuse.\nA typical first step is splitting the main logic of your analysis (main.py) from general-purpose helper functions (utils.py). Over time, as utils.py expands, you‚Äôll notice clusters of related functionality that can be moved into their own files, such as preprocessing.py, models.py, or plot_results.py. This modular approach naturally leads to a clean directory structure, which might look like this for a larger Python project:1\nsrc/\n‚îî‚îÄ‚îÄ my-package/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ main.py\n    ‚îú‚îÄ‚îÄ models/\n    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_a.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_b.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ my_model.py\n    ‚îî‚îÄ‚îÄ utils/\n        ‚îú‚îÄ‚îÄ __init__.py\n        ‚îú‚îÄ‚îÄ preprocessing.py\n        ‚îî‚îÄ‚îÄ plot_results.py\nIn main.py, you can import the relevant classes and functions from these modules to keep the main script clean and focused:\nfrom models.my_model import MyModel\nfrom utils import preprocessing\n\nif __name__ == '__main__':\n    # steps that will be executed when running `python main.py`\n    model = MyModel()\n\n\n\n\n\n\nKeep helper functions separate\n\n\n\nAlways separate reusable helper functions from the main executable code. This also means that files like utils/preprocessing.py should not include a main function, as they are not standalone scripts. Instead, these modules provide functionality that can be imported by other scripts‚Äîjust like external dependencies such as numpy.\nAs you tackle more projects, you may develop a set of functions that are so versatile and useful that you find yourself reusing them across multiple projects. At that point, you might consider packaging them as your own open-source library, allowing others to install and use it just like any other external library.\n\n\n\n\nKeep It Compact\nWhen writing code, aim to achieve your goals while using as little screen space as possible‚Äîthis applies to both the number of lines and their length.\n\n\n\n\n\n\nTips to create compact, reusable code\n\n\n\n\nAvoid duplication: Instead of copying and pasting code in multiple places, consolidate it into a reusable function to save lines.\nPrefer ‚Äòdeep‚Äô functions: Avoid extracting very short code fragments (1-2 lines) into a separate function, especially if this function would require many arguments. Such shallow functions with wide interfaces increase complexity without meaningfully reducing line count. Instead, strive for deep functions (spanning multiple lines) with narrow interfaces (e.g., only 1-3 input arguments, i.e., fewer arguments than the function has lines of code), which tend to be more general and reusable [3].\nAddress nesting: If your code becomes overly nested, this can be a sign that parts of the code should be moved into a separate function. This simplifies logic and shortens lines.\nUse Guard Clauses: Deeply nested if-statements can make code harder to read. Instead, use guard clauses [1] to handle preconditions (e.g., checking for wrong user input) early, leaving the ‚Äúhappy path‚Äù clear and concise. For example:\nif condition:\n    if not other_condition:\n        # do something\n        return result\nelse:\n    return None\nCan be refactored into:\nif not condition:\n    return None\nif other_condition:\n    return None\n# do something\nreturn result\nThis approach reduces nesting and improves readability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "href": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "title": "5¬† Implementation",
    "section": "Documentation & Comments: A Note to Your Future Self",
    "text": "Documentation & Comments: A Note to Your Future Self\nWhile you write it, everything seems obvious. However, when revisiting your code a few months later (e.g., to address reviewer feedback), you‚Äôre often left wondering what the heck you were doing. This is especially true when some external constraint (like a library quirk) forced you to create a workaround instead of opting for the straightforward solution. When returning to such code, you might be tempted to replace the awkward implementation with something more elegant, only to rediscover why you chose that approach in the first place. This is where comments can save you some trouble. And they are even more important when collaborating with others who need to understand your code.\nWe distinguish between documentation and comments: Documentation provides the general description of when and how to use your code, such as function docstrings explaining what the function computes, its input parameters, and return values. This is particularly important for open source libraries where you can‚Äôt personally explain the code‚Äôs purpose and usage to others. Comments help developers understand why your code was written in a certain way, like explaining that unintuitive workaround. Additionally, for scientific code, you may also need to document the origin of certain values or equations by referencing the corresponding paper in the comments.\n\n\n\n\n\n\nCode should be self-documenting\n\n\n\nIdeally, your code should be written so clearly that it‚Äôs self-explanatory. Comments shouldn‚Äôt explain what the code does, only why it does that (when not obvious). Comments and documentation, like code, need to be maintained‚Äîif you modify code, update the corresponding comments, or they become misleading and harmful rather than helpful. Using comments sparingly minimizes the risk of confusing, outdated comments.\nInformative variable and function names are essential for self-explanatory code. When you‚Äôre tempted to write a comment that summarizes what the following block of code does (e.g., # preprocess data), consider moving these lines into a separate function with an informative name, especially if they contain significant, reusable logic.\n\n\n\nNaming is hard\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n‚Äì Phil Karlton2\n\nFinding informative names for variables, functions, and classes can be challenging, but good names are crucial to make the code easier to understand for you and your collaborators.\n\n\n\n\n\n\nTips for effective naming\n\n\n\n\nNames should reveal intent. Longer names (consisting of multiple words in snake_case or camelCase, depending on the conventions of your chosen programming language) are usually better. However, stick to domain conventions‚Äîif everyone understands X and y as feature matrix and target vector, use these despite common advice denouncing single letter names.\nBe consistent: similar names should indicate similar things.\nAvoid reserved keywords (i.e., words your code editor colors differently, like Python‚Äôs input function).\nUse verbs for functions, nouns for classes.\nUse affirmative phrases for booleans (e.g., is_visible instead of is_invisible).\nUse plurals for collections (e.g., cats instead of list_of_cats).\nAvoid encoding types in names (e.g., color_dict), since if you decide to change the data type later, you either need to rename the variable everywhere or the name is now misleading.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#tests-protect-what-you-love",
    "href": "05_implementation.html#tests-protect-what-you-love",
    "title": "5¬† Implementation",
    "section": "Tests: Protect What You Love",
    "text": "Tests: Protect What You Love\nWe all want our code to be correct. During development, we often verify this manually by running the code with example inputs to check if the output matches our expectations. While this approach helps ensure correctness initially, it becomes cumbersome to recreate these test cases later when the code needs changes. The simple solution? Package your manual tests into a reusable test suite that you can run anytime to check your code for errors.\nTests typically use assert statements to confirm that the actual output matches the expected output. For example:\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    # verify correctness with examples, including edge cases\n    # syntax: assert (expression that should evaluate to True), \"error message\"\n    assert add(2, 2) == 4, \"2 + 2 should equal 4\"\n    assert add(5, -6) == -1, \"5 - 6 should equal -1\"\n    assert add(-2, 10.6) == 8.6, \"-2 + 10.6 should equal 8.6\"\n    assert add(0, 0) == 0, \"0 + 0 should equal 0\"\nPure functions‚Äîthose without side effects like reading or writing external files‚Äîare especially easy to test because you can directly supply the necessary inputs. Placing your main logic into pure functions therefore simplifies testing the critical parts of your code. For impure functions, such as those interacting with databases or APIs, you can use techniques like mocking to simulate external dependencies.\n\n\n\n\n\n\nTesting in Python with pytest\n\n\n\nConsider using the pytest framework for your Python tests. Organize all your test scripts in a dedicated tests/ folder to keep them separate from the main source code.\n\n\nWhen designing your tests, focus on edge cases‚Äîunusual or extreme scenarios like values outside the normal range or invalid inputs (e.g., dividing by zero or passing an empty list). The more thorough your tests, the more confident you can be in your code. Each time you make significant changes, run all your tests to ensure the code still behaves as expected.\nSome developers even adopt Test-Driven Development (TDD), where they write tests before the actual code. The process begins with writing tests that fail, then creating the code to make them pass. TDD can be highly motivating as it provides clear goals, but it requires discipline and may not always be practical in the early stages of development when function definitions are still evolving.\n\n\n\n\n\n\nTesting at different levels\n\n\n\nIdeally, you‚Äôll test your software at all levels:\n\nUnit Tests: Test individual components (e.g., single functions) to verify basic logic.\nIntegration/System Tests: Check that different parts of the system work together as expected. These often require more complex setups, like running multiple services at the same time.\nManual Testing: Identify unexpected behavior or overlooked edge cases. Whenever a bug is found, create an automated test to reproduce it and prevent regression.\nUser Testing: Evaluate the user interface (UI) with real users to ensure clarity and usability. UX designers often perform these tests using design mockups before coding begins.\n\n\n\n\nDebugging\nWhen your code doesn‚Äôt work as intended, you‚Äôll need to debug‚Äîsystematically identify and fix the problem. Debugging becomes easier if your code is organized into small, testable functions covered by unit tests. These tests often help narrow down the source of the issue. If none of your tests caught the bug, write a new test to reproduce it and ensure this case is covered in the future.\nTo isolate the exact line causing the error:\n\nUse print statements to log variable values at key points and understand the program‚Äôs flow.\nAdd assert statements to verify intermediate results.\nUse a debugger, often integrated into your IDE, to set breakpoints where execution will pause, allowing you to step through the program manually and inspect variables.\n\nDebugging is an essential skill that not only fixes bugs but also improves your understanding of the code and its behavior.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#make-it-fast",
    "href": "05_implementation.html#make-it-fast",
    "title": "5¬† Implementation",
    "section": "Make It Fast",
    "text": "Make It Fast\n\nMake it run, make it right, make it fast.\n‚Äì Kent Beck (or rather this dad, Douglas Kent Beck3)\n\nNow that your code works and produces the right results (as you‚Äôve dutifully confirmed with thorough testing), it‚Äôs time to think about performance.\n\n\n\n\n\n\nReadability over performance\n\n\n\nAlways prioritize writing code that‚Äôs easy to understand. Performance optimizations should never come at the cost of readability. More time is spent by humans reading and maintaining code than machines executing it.\n\n\n\nFind and fix the bottlenecks\nInstead of randomly trying to speed up everything, focus on the parts of your code that are actually slow. A quick way to find bottlenecks is to manually interrupt your code during a long run; if it always stops in the same place, that‚Äôs likely the issue. For a more systematic approach, use a profiler. Profilers analyze your code and show you how much time each part takes, helping you decide where to focus your efforts.\n\n\n\n\n\n\nRun it in the cloud\n\n\n\nWorking with large datasets may trigger Out of Memory errors as your computer runs out of RAM. While optimizing your code can help, sometimes the quickest solution is to run it on a larger machine in the cloud. Platforms like AWS, Google Cloud, Azure, or your institution‚Äôs own compute cluster make this cost-effective and accessible. That said, always look for simple performance improvements first!\n\n\n\n\nThink About Big O\nSome computations have unavoidable limits. For example, finding the maximum value in an unsorted list requires checking every item‚Äîthere is no way around this. The ‚ÄúBig O‚Äù notation is used to describe these limits, helping you understand how your code scales as data grows (both in terms of execution time and required memory).\n\nConstant time (\\(\\mathcal{O}(1)\\)): Independent of dataset size (e.g., looking up a key in a dictionary).\nLinear time (\\(\\mathcal{O}(n)\\)): Grows proportionally to data size (e.g., finding the maximum in a list).\nProblematic growth (e.g., \\(\\mathcal{O}(n^3)\\) or \\(\\mathcal{O}(2^n)\\)): Polynomial or exponential scaling can make algorithms impractical for large datasets.\n\nWhen developing a novel algorithm, you should examine its scaling behavior both theoretically (e.g., using proofs) and empirically (e.g., timing it on datasets of different sizes). Designing a more efficient algorithm is a major achievement in computational research!\n\n\nDivide & Conquer\nIf your code is too slow or your dataset too large, try splitting the work into smaller, independent chunks and combining the results. This ‚Äúdivide and conquer‚Äù approach is used in many algorithms, like the merge sort algorithm, and in big data frameworks like MapReduce.\n\nExample: MapReduce\nMapReduce [2] was one of the first frameworks developed to work with ‚Äòbig data‚Äô that does not fit on a single computer anymore. The data is split into chunks and distributed across multiple machines, where each chunk is processed in parallel (map step), and then the results are combined into the final output (reduce step).\nFor instance, if you‚Äôre training a machine learning model on a very large dataset, you could train separate models on subsets of the data and then aggregate their predictions (e.g., by averaging them), thereby creating an ensemble model.\n\n\n\n\n\n\nReplace For-Loops with Map/Filter/Reduce\n\n\n\nSequential for loops can often be replaced with map, filter, and reduce operations for better readability and potential parallelism:\n\nmap: Transforms each element in a sequence.\nfilter: Keeps elements that meet a condition.\nreduce: Aggregates elements recursively (e.g., summing values).\n\nFor example:\nfrom functools import reduce\n\n### Simplify this loop:\ncurrent_sum = 0\ncurrent_max = -float('inf')\nfor i in range(10000):\n    new_i = i**0.5\n    # the modulo operator x % y gives the remainder when diving x by y\n    # i.e., we're checking for even numbers, where the rest is == 0\n    if (round(new_i) % 2) == 0:\n        current_sum += new_i\n        current_max = max(current_max, new_i)\n\n### Using map/filter/reduce:\n# map(function to apply, list of elements)\nnew_i_all = map(lambda x: x**0.5, range(10000))\n# filter(function that returns true or false, list of elements)\nnew_i_filtered = filter(lambda x: (round(x) % 2) == 0, new_i_all)\n# reduce(function to combine current result with next element, list of elements, initial value)\ncurrent_sum = reduce(lambda acc, x: acc + x, new_i_filtered, 0)\ncurrent_max = reduce(lambda acc, x: max(acc, x), new_i_filtered, -float('inf'))\n# (of course, for these simple cases you could just use sum() and max() on the list directly)\nIn Python, list comprehensions also offer concise alternatives:\nnew_i_filtered = [i**0.5 for i in range(10000) if (round(i**0.5) % 2) == 0]\n\n\n\n\nExploit Parallelism\nMany scientific computations are ‚Äúembarrassingly parallelizable,‚Äù meaning tasks can run independently. For example, running simulations with different model configurations, initial conditions, or random seeds. Each of these experiments can be submitted as a separate job and run in parallel on a compute cluster. By identifying parts of your code that can be parallelized, you can save time and make full use of available resources.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#refactoring-make-change-easy",
    "href": "05_implementation.html#refactoring-make-change-easy",
    "title": "5¬† Implementation",
    "section": "Refactoring: Make Change Easy",
    "text": "Refactoring: Make Change Easy\nRefactoring is the process of modifying existing code without altering its external behavior. In other words, it preserves the ‚Äúcontract‚Äù (interface) between your code and its users while improving its internal structure.\nCommon refactoring tasks include:\n\nRenaming: Giving variables, functions, or classes more meaningful and descriptive names.\nExtracting Functions: Breaking large functions into smaller, more focused ones (\\(\\to\\) one function should do one thing).\nEliminating Duplication: Consolidating repeated code into reusable functions.\nSimplifying Logic: Reducing deeply nested code structures or introducing guard clauses for clarity.\nReorganizing Code: Grouping related functions or classes into appropriate files or modules.\n\n\nWhy refactor?\nRefactoring is typically done for two main reasons:\n\nAddressing Technical Debt:\nWhen code is written quickly‚Äîoften to meet deadlines‚Äîit may include shortcuts that make future changes harder. This accumulation of compromises is called ‚Äútechnical debt.‚Äù Refactoring cleans up this debt, improving code quality and making the code easier to understand.\n\nExample: Revisiting old code can be like tidying up a messy campsite. Just as a good scout leaves the campground cleaner than they found it, a responsible developer leaves the codebase better for the next person (or themselves in the future).\n\nMaking Change Easier:\nSometimes, implementing a new feature in your existing code feels like forcing a square peg into a round hole. Instead of struggling with awkward workarounds, you should first refactor your code to align with the new requirements. The goal of software design isn‚Äôt to predict every possible future change (which is impossible) but to adapt gracefully when those changes arise.\n\nBefore adding a new feature, clean up your code so that the change feels natural and seamless. This not only simplifies the task at hand but also results in a more general, reusable functions and classes.\n\n\n\n\nRefactorings to simplify changes\n\nFor each desired change, make the change easy (warning: this may be hard), then make the easy change.\n‚Äì Kent Beck4\n\n\nReplace Magic Numbers with Constants: Magic numbers‚Äîvalues with unclear meaning‚Äîcan make code harder to understand and maintain. By replacing them with constants, you create a single source of truth that‚Äôs easy to modify.\n# Before:\nif status == 404:\n    ...\n\n# After:\nERROR_NOT_FOUND = 404\nif status == ERROR_NOT_FOUND:\n    ...\nDon‚Äôt Repeat Yourself (DRY): Copying and pasting code may seem like a quick fix, but it leads to problems later. If the logic changes, you‚Äôll need to update it everywhere it‚Äôs duplicated, which is error-prone. Instead, move the logic into a reusable function or method.\n# Before:\nif (model.a &gt; 5) and (model.b == 3) and (model.c &lt; 8):\n    ...\n\n# After:\nclass MyModel:\n    def is_ready(self):\n        return (self.a &gt; 5) and (self.b == 3) and (self.c &lt; 8)\n\nif model.is_ready():\n    ...\nOrganize for Coherence: Keep code elements that need to change together in the same file or module. Conversely, separate unrelated parts of your code to prevent unnecessary entanglement. This way, changes are localized, which reduces cognitive load.\nIn larger codebases shared by multiple teams, this is even more critical. When changes require excessive communication and coordination, it signals a need to reorganize the code. Clear ownership and reduced dependencies help teams work independently while keeping the system coherent through agreed upon interfaces.\n\n\n\n\n\n\n\nAdditional tips\n\n\n\n\nTest as you refactor: Always run tests before and after refactoring to ensure no functionality is accidentally broken. Writing or expanding automated tests is often part of the process to safeguard against regressions.\nLeverage IDE support: Modern IDEs like PyCharm or Visual Studio Code provide tools for automated refactoring, such as renaming, extracting functions, or moving files. These can save time and reduce errors.\nAvoid over-refactoring: While cleaning up code is valuable, avoid making unnecessary changes that don‚Äôt improve functionality or clarity. Over-refactoring wastes time and can confuse collaborators.\n\n\n\nBy refactoring regularly and following these practices, you‚Äôll create a cleaner, more maintainable codebase that is adaptable to future needs and fun to work with.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to transform your ideas into code.\nSome best practices to write code that is easy to understand and maintain.\n\n\n\n\n\n\n\n[1] Beck K. Tidy First? O‚ÄôReilly Media, Inc. (2023).\n\n\n[2] Dean J, Ghemawat S. MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM 51(1), 107‚Äì113 (2008).\n\n\n[3] Ousterhout JK. A Philosophy of Software Design. Yaknyam Press Palo Alto, CA, USA (2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#footnotes",
    "href": "05_implementation.html#footnotes",
    "title": "5¬† Implementation",
    "section": "",
    "text": "The __init__.py file is needed to turn a directory into a package from which other scripts can import functionality. Usually, the file is completely empty.‚Ü©Ô∏é\nhttps://martinfowler.com/bliki/TwoHardThings.html‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/704385198301904896‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/250733358307500032‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "06_production.html",
    "href": "06_production.html",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Components of Software Products",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_production.html#components-of-software-products",
    "href": "06_production.html#components-of-software-products",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Graphical User Interface (GUI)\n\n\nDatabases\n\n\nAPIs\n\n\nEvents\n\n\nBatch Jobs\n\n\nSoftware Design Revisited",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_production.html#delivery-deployment",
    "href": "06_production.html#delivery-deployment",
    "title": "6¬† From Research to Production",
    "section": "Delivery & Deployment",
    "text": "Delivery & Deployment\n\nContinuous Integration\n\n\nContainers in the Cloud\nContainers like Docker capture the entire computing environment, making it portable and consistent across machines. A simple example Dockerfile for a Python project:\nFROM python:3.9\nWORKDIR /app\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"main.py\"]\n\n\nTesting & Production Environments\n\n\nScalability\n\n\nObservability\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nWhich additional steps you could take to make your research project production ready.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] Beck K. Tidy First? O‚ÄôReilly Media,\nInc. (2023).\n\n\n[2] Callaway E. Chemistry Nobel Goes to Developers\nof AlphaFold AI That Predicts Protein Structures. Nature\n634(8034), 525‚Äì526 (2024).\n\n\n[3] Dean J, Ghemawat S. MapReduce: Simplified Data\nProcessing on Large Clusters. Communications of the ACM 51(1),\n107‚Äì113 (2008).\n\n\n[4] Foote B, Yoder J. Big Ball of Mud. Pattern\nlanguages of program design 4, 654‚Äì692 (1997).\n\n\n[5] Freiesleben T, Molnar C. Supervised Machine Learning for\nScience: How to Stop Worrying and Love Your Black Box.\n(2024).\n\n\n[6] Horn F. A Practitioner‚Äôs Guide to\nMachine Learning. (2021).\n\n\n[7] Knaflic CN. Storytelling with Data: A Data\nVisualization Guide for Business Professionals. John Wiley &\nSons (2015).\n\n\n[8] Ousterhout JK. A Philosophy of Software\nDesign. Yaknyam Press Palo Alto, CA, USA (2018).",
    "crumbs": [
      "References"
    ]
  }
]
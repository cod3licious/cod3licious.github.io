[
  {
    "objectID": "interaction_flow_mapping.html",
    "href": "interaction_flow_mapping.html",
    "title": "InterAction Flow Mapping",
    "section": "",
    "text": "Organizations run on processes. Most of them are messy, span multiple roles, combine human judgment with software automation, and exist partly in people‚Äôs heads. Improving these processes‚Äîby simplifying them, automating parts of them, or supporting steps with software‚Äîrequires a shared understanding of how they actually work today and how they should work in the future.\nThis is especially important when building software. Software does not exist in isolation: it is embedded in human workflows, exchanges data with other systems, depends on user inputs, and introduces new constraints. When developers, domain experts, and stakeholders carry different mental models of the same process, the result is friction, rework, and the classic ‚Äúthat‚Äôs not what I meant‚Äù conversation.\nSo this is where visual models come in: A diagram that depicts the sequence of steps over time and shows the interactions and handoffs between different actors provides far more clarity than a verbal description of a process.\nThe effect is even stronger when that diagram is created collaboratively. In a collaborative modeling (CoMo) session, the relevant people jointly build a visual model of the process, discuss it, and question it. The diagram becomes a shared thinking aid: assumptions are made explicit, gaps become visible, and the group gains a concrete reference point for reasoning about change.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#creating-shared-understanding",
    "href": "interaction_flow_mapping.html#creating-shared-understanding",
    "title": "InterAction Flow Mapping",
    "section": "",
    "text": "Organizations run on processes. Most of them are messy, span multiple roles, combine human judgment with software automation, and exist partly in people‚Äôs heads. Improving these processes‚Äîby simplifying them, automating parts of them, or supporting steps with software‚Äîrequires a shared understanding of how they actually work today and how they should work in the future.\nThis is especially important when building software. Software does not exist in isolation: it is embedded in human workflows, exchanges data with other systems, depends on user inputs, and introduces new constraints. When developers, domain experts, and stakeholders carry different mental models of the same process, the result is friction, rework, and the classic ‚Äúthat‚Äôs not what I meant‚Äù conversation.\nSo this is where visual models come in: A diagram that depicts the sequence of steps over time and shows the interactions and handoffs between different actors provides far more clarity than a verbal description of a process.\nThe effect is even stronger when that diagram is created collaboratively. In a collaborative modeling (CoMo) session, the relevant people jointly build a visual model of the process, discuss it, and question it. The diagram becomes a shared thinking aid: assumptions are made explicit, gaps become visible, and the group gains a concrete reference point for reasoning about change.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#why-another-model",
    "href": "interaction_flow_mapping.html#why-another-model",
    "title": "InterAction Flow Mapping",
    "section": "Why Another Model?",
    "text": "Why Another Model?\nYes, there already exist plenty of modeling notations and workshop formats that have proven their value in many contexts (check out the book Visual Collaboration Tools for an overview). This article is not an argument against any of these tools.\nStill, despite being a great source of inspiration, somehow none of these methods felt exactly right for what I was trying to accomplish when creating the diagrams for my latest book. So, just in case I‚Äôm not the only one out there yearning for an alternative approach, I wanted to share my notation and the ideas behind it in more detail.\n\nModeling from Different Perspectives\nLet‚Äôs start by acknowledging that systems can be modeled from different perspectives (Figure¬†1).\nOne perspective focuses on a system‚Äôs structure: how individual elements are organized and composed into larger subsystems (often forming bottom-up in natural systems). Examples include:\n\nCells ‚Üí Organs ‚Üí Human body\nEmployees ‚Üí Teams ‚Üí Departments ‚Üí Company\nLines of code ‚Üí Functions and classes ‚Üí Modules (files and folders) ‚Üí Repository or deployable service\n\nTools such as Context Mapping and the C4 Model help describe the structural perspective of (software) systems.\nAnother perspective focuses on behavior: what the system does over time. Examples include:\n\nDigesting food\nOnboarding a new employee\nA user interacting with a mobile app to accomplish a task\n\nThese behaviors can be described as processes, which are the focus of this article.\nProcesses themselves can be examined at different levels of granularity. Here, we deliberately stay at a higher level of abstraction.\nTechniques such as Example Mapping are better suited for zooming in further and uncovering the detailed business rules that govern individual steps.\n\n\n\n\n\n\nFigure¬†1: A system has a structure, defined by how individual elements interact to form subsystems, and exhibits behaviors that can be described as processes. Zooming into a process step reveals business rules and logic, where individual operations, decision points, and loops detail how each step is executed.\n\n\n\n\n\nProcess Modeling\nExisting tools to model processes all optimize for slightly different goals, for example:\n\nUML and BPMN are well-established standards for modeling software behavior and business processes. They allow for very detailed and precise diagrams, but their specialized notation often makes the resulting models difficult to grasp for non-experts. They were also primarily intended for documentation rather than collaborative exploration.\nEventStorming is a popular collaborative modeling technique that emphasizes accessibility by using plain-language domain events. It works particularly well for discovering domain concepts and behavior, especially in unfamiliar problem spaces. However, the initial chaotic exploration phase can quickly become visually overwhelming, and as more structure and technical details are added, the resulting model often becomes difficult to follow. Additionally, expressing events in the past tense is unintuitive for many participants.\nDomain Storytelling focuses on interactions between actors and objects and produces diagrams that are easy to understand and discuss. While effective for creating a shared overview, the temporal sequence of a process is represented indirectly through numbered arrows, which can make it harder to perceive the overall flow at a glance in more complex scenarios.\n\nInterAction Flow Mapping is not supposed to replace these approaches. Instead, it fills a specific gap by combining the simplicity of Domain Storytelling with the precision of UML:\n\nA simple diagram that is easy to understand, even for non-technical domain experts.\nA clear view of the process over time, with explicit separation of the different actors involved.\nA notation that supports direct translation of the modeled process into a software implementation.\n\nThe technique that comes closest to InterAction Flow Mapping is Event Modeling. It visualizes wireframes alongside read and write operations that aggregate event-based data from multiple sources. The resulting diagrams are easy for domain experts to follow while still providing a rough implementation blueprint for developers. In practice, however, I found these models map more naturally to event sourcing architectures and often require an extra translation step when implementing a process with a simpler CRUD-based design.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#the-interaction-flow-notation",
    "href": "interaction_flow_mapping.html#the-interaction-flow-notation",
    "title": "InterAction Flow Mapping",
    "section": "The InterAction Flow Notation",
    "text": "The InterAction Flow Notation\nAn InterAction Flow models a process as a sequence of interactions between actors. It can be used to visualize both analog processes as well as software behavior.\nThe modeling canvas is simple:\n\nThe horizontal axis represents time.\nThe vertical axis represents actors in swimlanes‚Äîtypically starting with a user or customer at the top (who initiates the process), followed by software modules or organizational units, and ending with data stores or records at the bottom.\n\nThe visual elements are intentionally minimal (Figure¬†2):\n\nAction boxes describe what an actor does, labeled with a short phrase in present tense starting with a verb (e.g., verify credentials). These actions should be meaningful at the business level and plausibly map to a function, API endpoint, or responsibility in software.\nStart and end markers indicate where the process begins and terminates. These borrow from BPMN but are optional‚Äîfeel free to use normal stickies in a CoMo session.\nArrows connect actions across swimlanes and are labeled with the information being exchanged. In software terms, these often correspond to input parameters and return values.\nResource icons indicate where data is read or written during an action.1\n\n\n\n\n\n\n\nFigure¬†2: Visual elements used in an InterAction Flow diagram. Essentially, it‚Äôs all about boxes (actions) and arrows (information).\n\n\n\n\nInterAction Flows in Action\nA simple example of an analog process‚Äîreserving a hotel room by phone‚Äîmodeled as an InterAction Flow is shown in Figure¬†3, while the corresponding software-enabled process is shown in Figure¬†4.\n\n\n\n\n\n\nFigure¬†3: Analog process to reserve a hotel room: the guest calls the hotel, tells the manager the dates and number of guests, the hotel manager checks which types of rooms are available during this time, and the guest decides which room to reserve.\n\n\n\n\n\n\n\n\n\nFigure¬†4: Software-enabled process to reserve a hotel room: the user opens the hotel‚Äôs website, enters the dates and number of guests, sees which types of rooms are available during this time, and then proceeds to reserve one of them. Some actions happen in parallel (for example, the user entering data while the website is being displayed). Not all arrows are annotated with exchanged information: some operations require no additional input (e.g., a GET request without parameters), while other information is transmitted indirectly, such as via the rendered web page.\n\n\n\nIt is also possible to model multiple subactions in sequence for a single actor when a particular step is especially complex and decomposing it adds clarity (Figure¬†5).\n\n\n\n\n\n\nFigure¬†5: Process excerpt showing a conversation with an AI agent, with the goal of finding a hotel room: the user submits a prompt; the Large Language Model (LLM) powering the agent analyzes it, determines which tools to use to gather the required information, calls those tools (in this case, an external API for hotel search available via MCP), and then synthesizes a response from the results. Since this process step is more complex, it makes sense to spell out the individual subactions explicitly‚Äîwhile avoiding too much detail, so the overall process still remains understandable at a high level. In addition to the final output, the LLM also provides updates (explaining its ‚Äúthinking‚Äù), which do not result in a user action.\n\n\n\n\n\n\n\n\n\nTipTime-triggered processes\n\n\n\nIf your process is not triggered by an external event but rather runs on a schedule, simply put the corresponding time in the circle denoting the start of the process.\n\n\n\n\n\n\n\n\nNoteWhat about decision points and branches?\n\n\n\nIf you feel compelled to add conditionals or loops to your flow, this is often a signal to pause and reconsider the level of abstraction. A lot of branching behavior happens inside an action and does not need to be visible at the overview level.\nWhen branching paths are central to the discussion, start by modeling the happy path and add annotations where alternatives exist, or create a separate diagram that starts at the branching point to explore the alternative flow. In some cases, it is also possible to describe different paths using separate swimlanes (Figure¬†6).\n\n\n\n\n\n\nFigure¬†6: Using separate swimlanes (and color coding) to represent branching at a decision point: a smartwatch analyzes collected sensor measurements every five minutes to detect movement patterns, stress, and other conditions. When an internet connection is available, the data is sent to a cloud-based machine learning (ML) model for analysis; otherwise, a less accurate model running on the device is used.\n\n\n\n\n\n\n\nFrom Diagram to Code\nInterAction Flows translate naturally into software design:\n\nAction boxes often map directly to function calls or API endpoints.\nArrow labels make the required data contracts explicit.\nSwimlanes highlight ownership and boundaries between systems or teams.\nResource icons encourage early discussion about persistence and data ownership.\n\nWhen co-creating an InterAction Flow in a collaborative InterAction Flow Mapping session, it often makes sense to keep the diagram more abstract, as shown in the previous figures. That said, if you want to use an InterAction Flow as a blueprint for implementation, you can easily include additional technical details and wireframes to make the intended software behavior more explicit (Figure¬†7).\n\n\n\n\n\n\nFigure¬†7: Software-enabled process to book a hotel room with more details, including wireframes, the API endpoints that are called, and an external API used to process the payment.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#interaction-flow-mapping-workshops",
    "href": "interaction_flow_mapping.html#interaction-flow-mapping-workshops",
    "title": "InterAction Flow Mapping",
    "section": "InterAction Flow Mapping Workshops",
    "text": "InterAction Flow Mapping Workshops\nI propose two formats for InterAction Flow Mapping workshops, tailored to two common goals:\n\ndesigning a new (to-be) process, or\nimproving an existing (as-is) process.\n\nBoth workshops can be run in-person or remotely2. Feel free to adapt these workshop formats to your needs!\nFor more tips on how to conduct successful collaborative modeling sessions, please refer to the excellent book Collaborative Software Design.\n\nDesigning a New (To-Be) Process\nThis workshop format emphasizes shared design and exploration, like a typical CoMo session.\n\nPreparation\nPrepare the space and ensure participants know which process will be designed.\n\n\nRunning the Workshop\nDepending on how complex the process is, this workshop might take a full day or you can split it over multiple sessions.\nMain workshop steps:\n\nIntroduce goal and notation: Clarify (again) which process will be modeled and briefly explain the InterAction Flow elements if participants are unfamiliar with them.\nBraindrawing (together, alone): Give participants 5‚Äì10 minutes to independently sketch the process from their perspective, focusing on actors and their actions.\n\n\nFor in-person sessions, provide paper and pencils; for remote sessions, prepare an individual whiteboard frame with example elements for each participant.\nA useful approach is to start with user interactions and then move on to refining the required data, its sources, and persistence.\nWhen redesigning an existing process, participants should try to ignore current constraints and design an ideal target process from scratch.\n\nGallery walk: Allow time for participants to review each other‚Äôs diagrams.\nSelect a starting point: Choose one diagram as the basis for the shared InterAction Flow‚Äîpreferably via dot voting during the gallery walk, or by facilitator choice if time is limited.\nIncremental refinement: Starting with the author of the selected diagram, participants take turns explaining their view while the group incrementally refines the shared InterAction Flow based on the new perspective.\n[Optional] Compare with the as-is process: When redesigning an existing process, compare the new flow with the current-state diagram (including any identified pain points) to ensure no key steps are missing and major issues are addressed.\nFor software-based processes, proceed by defining a refactoring plan with iterative improvements that gradually move the current process toward the target state, rather than attempting to implement a new flow from scratch while maintaining the existing one in parallel until a full replacement is possible.\n\nFor software projects, the InterAction Flow can be refined further to aid the implementation (as in Figure¬†7) by splitting into two groups:\n\nGroup 1 (incl.¬†a UX Designer / Product Owner, Frontend Developer): Refines user interactions, adds wireframes, and identifies data needed to render the UI.\nGroup 2 (incl.¬†a Backend Developer): Refines API and data store layers, including database schemas or ORMs.\n\nThis can be done in breakout sessions or asynchronously. The groups then reconvene to align their results and ensure the overall flow remains consistent and data needs are met end to end. At this point it can also be helpful to perform a Data Completeness Check.\n\n\n\n\n\n\nTipData Completeness Check\n\n\n\nAn interesting concept in Event Modeling is the Data Completeness Check: Each dynamic element in a wireframe is highlighted in a specific color to indicate whether the data shown there needs to be retrieved from (green) or persisted in (blue) a data store. By then linking these elements to our database schema, we can check whether we are storing all data required to enable the user experience we want (Figure¬†8).\n\n\n\n\n\n\nFigure¬†8: Wireframes with colored elements to indicate whether the shown data needs to be retrieved from (green) or persisted in (blue) a data store, as well as the corresponding fields in our database schema that contain these values. This helps us notice in advance whether our data model is incomplete.\n\n\n\n\n\n\n\n\nImproving an Existing (As-Is) Process\nThis workshop is ideal for analyzing and improving an existing process, surfacing problems, and identifying solutions. It draws inspiration from the book Flow Engineering.\n\nPreparation\nSince here the focus is on identifying problems rather than creating a new process, preparing the InterAction Flow is less of a creative exercise and can often be done in advance. Talk to key stakeholders to get different perspectives and draft a diagram that can serve as a conversation starter in the workshop. Consider sharing the diagram with participants in the invitation so they can review it beforehand.\n\n\nRunning the Workshop\nPlan for 1‚Äì3 hours depending on process complexity. The workshop typically follows six steps (Figure¬†9):\n\nAlign on the InterAction Flow: Walk participants through the diagram and refine it collaboratively.\nBrainstorm pain points: Participants add pink stickies for any problem they notice, from minor UI issues to tedious manual steps.\nPrioritize pain points: Rank the pink stickies by dimensions relevant to your context, like severity (‚Äúhow annoying is this?‚Äù) and frequency (‚Äúhow often does it happen?‚Äù).\nBrainstorm solutions: Participants suggest solutions for the prioritized pain points, adding green stickies to corresponding problems.\nPrioritize solutions: Rank solution ideas by impact (‚Äúby how much could this improve the overall situation?‚Äù) and feasibility (‚Äúhow easy would it be to implement?‚Äù).\nMake a plan: Use dot voting to select 3‚Äì5 solutions to implement, assign owners and deadlines, and track progress in the team‚Äôs project management tool.\n\n\n\n\n\n\n\nFigure¬†9: Overview of the workshop steps.\n\n\n\nFollow up on assigned tasks and revisit the board for further improvement ideas as needed.",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "interaction_flow_mapping.html#footnotes",
    "href": "interaction_flow_mapping.html#footnotes",
    "title": "InterAction Flow Mapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne reason for modeling these resources separately is to make it easier to distinguish between pure and impure functions in a software flow.‚Ü©Ô∏é\nIn terms of digital whiteboarding tools, I recommend using Miro (or their WebWhiteboard to get started without an account) or Excalidraw.‚Ü©Ô∏é",
    "crumbs": [
      "InterAction Flow Mapping"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html",
    "href": "2026-01-cdd-humans-ai.html",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "",
    "text": "This article is still very much work in progress; more details on the practical example will be added over time.\nWhen I attended my first computer science class in 2009, programming‚Äîwriting text that makes a computer do something‚Äîfelt like a magical superpower. But over the years, as I shifted towards designing larger systems, typing out the necessary code to materialize these systems started to feel more and more like a chore. That‚Äôs why I‚Äôve been really excited when, around Nov/Dec 2025, AI coding agents finally reached a point where they produced code I didn‚Äôt feel I had to refactor from top to bottom‚Äîand suddenly software development became fun again.\nIn this article, I want to give a glimpse into how I personally use AI to develop software (as of January 2026). But more importantly, I want to explain what I don‚Äôt let AI do: the conceptual work that happens before I start coding‚Äîwith or without AI‚Äîand which gives me the confidence that the software is worth building at all. I call this approach Clarity-Driven Development (CDD), which you can also read more about in my latest book, Clarity-Driven Development of Scientific Software.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#where-humans-are-still-in-the-loop",
    "href": "2026-01-cdd-humans-ai.html#where-humans-are-still-in-the-loop",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Where Humans Are Still in the Loop",
    "text": "Where Humans Are Still in the Loop\nAI‚Äîmore specifically, generative AI in the form of large language models (LLMs)‚Äîis now available both through web-based chat tools like ChatGPT and through more autonomous agents like Claude Code. These systems can support us in different ways, with varying levels of oversight (Figure¬†1).\n\n\n\n\n\n\nFigure¬†1: Interactions with AI lie on a spectrum. How much oversight you need to provide depends on your goal, use case, and personal preference.\n\n\n\nWhile vibe coding may be appropriate for throwaway prototypes, so far I still lean toward the ‚Äúhuman control‚Äù end of the spectrum. That said, as models improve, I‚Äôm increasingly surprised by the quality of AI-generated code and find myself manually verifying less and less‚Äîwhile still having guardrails in place: tests, type checking, linting, and similar feedback loops that also help AI agents validate their own work before calling a task done.\nIn that respect, I found that working with an AI agent feels a lot like working with a newly hired colleague. At first, you watch their output closely‚Äîwho knows, maybe they oversold themselves in the interview, just like marketing promises and flashy AI hype posts don‚Äôt exactly mirror reality. But over time, as they repeatedly deliver, trust grows and the need for detailed checks drops (assuming you‚Äôre not a micromanaging control freak who already scared away their best people).\nBut even your most capable colleagues can only deliver high-quality work if you provide them with enough context and clearly describe the outcome you want. By that I don‚Äôt mean prescribing specific steps they need to follow (‚Äúfirst do X, then Y, then Z‚Äù)‚Äîhumans value autonomy over how they accomplish their tasks, and modern AIs are capable of figuring out these steps themselves. But you do need to provide a clear definition of done. And this is exactly what CDD is about: describing the result you envision so your colleague‚Äîhuman or AI‚Äîhas a real chance of delivering something you‚Äôll be happy with.\nThis also means the spectrum in Figure¬†1 is too one-dimensional. Human control is exerted in two steps (Figure¬†2): when providing the input to the AI (by writing more specific prompts) and when reviewing and manually correcting the AI‚Äôs output.\n\n\n\n\n\n\nFigure¬†2: Humans are in the loop in two places: when providing inputs and when scrutinizing the AI‚Äôs output. I prefer investing effort on the input side and would happily stop verifying outputs one day.\n\n\n\nMy hope is that the second step eventually disappears‚Äîthat I can some day trust AI-generated code the way I trust compiler output. I‚Äôm perfectly happy, however, to keep full control on the input side. I still want to think through the problem and gain clarity about what it is I actually want to build. Now, I‚Äôm not saying this process won‚Äôt also involve AI‚Äîit can actually be a great sounding board for your ideas, similar to how brainstorming together with colleagues provides valuable perspectives. But I believe human thinking is still essential for guiding AI, not least because the software we build is ultimately for human users.\n\nMy Goal\nWhat the future of autonomous AI agents will look like‚Äîwhether it will be common practice to have fleets of agents implementing tickets, opening pull requests, and reviewing each other‚Äôs work to produce working software‚Äîis still an open question. But assuming we can one day trust AI outputs, let‚Äôs focus now on the task that will remain essential for humans: defining its inputs.\nFor this, I‚Äôd argue we should strive for a format that‚Äôs useful not just for AI, but for human developers as well: detailed enough that a capable engineer with good judgment can implement it, but not so detailed that it becomes exhausting to write or review. Most importantly, I want the act of writing these documents to help me think‚Äîto better understand the problem and the kind of solution I want. So that‚Äôs where Clarity-Driven Development comes in.\n\n\n\n\n\n\nCautionWhat about the next generation of developers?\n\n\n\nA common concern is that using AI to produce code will prevent junior developers from acquiring the skills needed to become seniors. But those skills were never primarily about coding. The biggest difference I‚Äôve observed between junior and senior developers is that seniors think deeply about the problem and the trade-offs between possible solutions, while juniors tend to jump straight into implementation.\nCDD explicitly pushes you to think before writing code. It helps you practice core skills: decomposing complex problems into smaller steps, designing data flow and structure, abstracting details behind clean interfaces, and deciding what the right thing to build actually is.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#gaining-clarity",
    "href": "2026-01-cdd-humans-ai.html#gaining-clarity",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Gaining Clarity",
    "text": "Gaining Clarity\nI didn‚Äôt create Clarity-Driven Development (CDD) as a method for coding with AI. After more than a decade of professional software development, I simply noticed that I kept going through the same steps at the start of every major project. Over time, I refined those steps into a more explicit approach. And since the resulting documents are genuinely useful to me as a human developer, it‚Äôs no surprise they also turned out to be effective at guiding AI agents‚Äîa win on both sides.\nAt its core, CDD asks you to answer three questions before you start coding (Figure¬†3):\n\nWhy is your idea worth implementing?\nWhat should your implementation look like to users?\nHow could you best implement this?\n\n\n\n\n\n\n\nFigure¬†3: Before writing code‚Äîyourself or via an AI‚Äîit‚Äôs worth gaining clarity on why the solution should exist, what users should experience, and how it might be implemented.\n\n\n\nI refer to the documents that capture these answers as sketches, as they provide a rough but intentional outline of the system to be built. They are far lighter than full requirements or formal specifications and still require a generous dose of sound judgment from a capable engineer to turn them into working software.\nIn the following sections, I‚Äôll walk through what answering these questions looks like in practice. I‚Äôll use an example from the domain of predictive quality‚Äîmore complex than some toy To-Do app, but familiar enough for me to confidently evaluate the results. The example draws on my prior work at BASF (the chemical company) and at alcemy, a climate tech startup applying machine learning to optimize cement recipes and reduce CO‚ÇÇ emissions. Since this example is illustrative rather than commercial, I‚Äôll deliberately skip some details, which I‚Äôll return to at the end of the article.\n\n\n\n\n\n\nNoteClarity- vs.¬†Spec-Driven Development\n\n\n\nA related approach that has gained attention recently is Spec-Driven Development (SDD). SDD focuses on producing detailed specifications, plans, and task descriptions‚Äîoften with AI support. The goals is to then review these artifacts before implementation and treat the specs as the source of truth. However, I‚Äôm not convinced that reviewing detailed plans is inherently better than reviewing working code‚Äîand I‚Äôd certainly choose a lightweight set of user stories over hundreds of pages of requirements.\nCDD instead emphasizes creating just enough documentation for you and your collaborators‚Äîhuman and AI‚Äîto understand the problem and the conceptual solution. The goal isn‚Äôt to offload thinking onto an AI, but to create sketches that help you gain clarity about what you actually want to build. And many coding agents nowadays then generate plans and task lists on their own in the background anyway. Personally, I‚Äôd also keep the code as the source of truth, since we all know how tempting it is to quickly tweak a feature and forget to update the documentation accordingly.\n\n\n\nThe Why\nThe first question we need to answer is why our idea is worth implementing in the first place. This comes down to why users would be better off with our software than with the alternatives‚Äîwhether those alternatives are competing products, spreadsheets, or no software whatsoever. Answering this requires identifying who our target users are (e.g., by creating user personas) and what is important to them. Their priorities might be related to specific functionality, usability, aesthetics, or anything else that helps them perform their job better and improve the metrics by which their work is evaluated. If your team includes a product manager / owner, they should be able to help you with these points.\n\nPredictive Quality Example\nTODO\n\n\n\nThe What\nIn the next step, we need to examine what our implementation should look like to users. This typically means creating wireframes and user interface mockups, and‚Äîideally‚Äîtesting them with potential users to see where they struggle. I often start with simple pencil-and-paper sketches, but tools like Google‚Äôs Stitch can generate polished mockups in seconds using AI. That said, input from a UX designer is invaluable here. And when designs already exist, it‚Äôs worth understanding why they make sense and how they address user needs.\n\nPredictive Quality Example\nTODO\n\n\n\nThe How\nFinally, we need to ponder how we could best implement this solution. This includes designing interfaces, data flows, and data structures. Specifically, this means defining the inputs and outputs of major endpoints or scripts and outlining the database schema. If possible, reviewing these sketches with a more experienced software engineer or architect is time well spent.\nI begin by identifying the endpoints required to support the user experience, based on the data needs revealed by each screen‚Äôs mockup. Next, I determine any additional processes needed to support them, such as batch jobs for data processing. For each relevant interface, I note down:\n- script name or endpoint path\n- purpose (high-level description)\n- input arguments (names, types, brief description)\n- return values\n- side effects: how external resources such as databases, message queues, or other services are accessed\n- implementation notes, especially key domain logic, invariants, and edge cases\nIn larger projects, these interface sketches can be organized into files and folders that mirror the intended module structure. I usually limit these sketches to interfaces that define the system‚Äôs external behavior. For core functionality, however, it may be useful to additionally outline key functions or classes in this way. Everything else‚Äîhow the internals are decomposed‚Äîcan be left to the implementation and entrusted to our colleague.\nTo define database schemas, I like using drawDB. It lets you model the schema visually and export the corresponding SQL, which is excellent input for an AI when generating ORMs or other database-related code.\n\nPredictive Quality Example\nTODO\n\nAt this stage, our results are independent of any specific programming language or technology choices. That makes them cheap in the sense that no hard-to-reverse decisions have been made yet. Which also means now is a good opportunity‚Äîif you haven‚Äôt already‚Äîto share your idea and sketches with others (human and/or AI) and gather feedback before committing to an implementation built on a potentially flawed design.\n\nThinking through a problem and solution in this way has always been the fun part for me. In the past, I‚Äôd often stop here‚Äîonce the application ‚Äúworked in my head,‚Äù the challenge was essentially over, and turning my sketches into code just felt like a chore. Fortunately, that tedious work can now be offloaded to an AI that won‚Äôt get bored so easily.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#implementing-with-ai",
    "href": "2026-01-cdd-humans-ai.html#implementing-with-ai",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Implementing with AI",
    "text": "Implementing with AI\nNow that we have clarity on our desired destination, we‚Äôre ready to let AI take the driver‚Äôs seat.\n\n\n\n\n\n\nImportantThis is not a prompt engineering tutorial\n\n\n\nThe workflow descriptions below are kept deliberately high-level. Concrete tips tend to become obsolete almost as soon as they‚Äôre written, given how fast AI models evolve. If you want detailed, tactical advice, just search for ‚Äúprompt engineering.‚Äù For example, the best practices provided by Anthropic are a good starting point.\n\n\n\nUp-front Decisions\nEvery project starts with a set of structural decisions: codebase layout, deployment strategy, framework choices, and so on. I often chat with an AI to think these through, get initial suggestions, and weigh trade-offs.\nFor example, I usually work in Python, so for a web app I would normally default to using Streamlit (despite all its flaws) to get something working quickly. For this project, though, I wanted something more production-ready and to push both myself and the AI a bit further, so I decided to build a proper frontend using a JavaScript framework. So I asked an AI whether React or Angular would be more appropriate for my use case (Figure¬†4).\n\n\n\n\n\n\nFigure¬†4: How Claude helped me pick a frontend framework.\n\n\n\nThe arguments for React sound reasonable, though of course I don‚Äôt have enough frontend experience to fully validate them. However, had I asked an experienced frontend developer for their recommendation, their answer might also be biased, shaped by personal framework preferences and not necessarily tuned to my specific constraints.\nOf course, identifying those constraints in the first place, and deciding which trade-offs matter, is a skill that comes with experience. If you‚Äôre not sure which factors might be relevant for your use case, you can also tell the AI to first ask you some questions before making a recommendation. In any case, using an AI to help with architectural decisions is almost certainly better than simply adopting whatever technology happens to be the latest shiny thing.\nBut please note: since LLMs are probabilistic models, your results may vary. I‚Äôve found the following system prompt1 very helpful to improve answer quality:\n\nTell it like it is; don‚Äôt sugar-coat responses. Take a forward-thinking view. Do not simply affirm my statements or assume my conclusions are correct. Your goal is to be an intellectual sparring partner, not just an agreeable assistant. Where necessary, question my assumptions and provide counterpoints / alternative perspectives to make sure my reasoning holds up under scrutiny and there are now gaps or flaws in my argument. Correct me where I‚Äôm wrong.\n\n\n\nPartitioning the Work\nAI context windows are still limited, so dumping all sketches into a single chat and expecting a working system isn‚Äôt realistic. The work needs to be broken into manageable steps‚Äîeither manually or by asking the AI to do it.\nI typically start with the database schema and an initial set of endpoints to get a backend running, then add the corresponding frontend components, and repeat this to incrementally extend the system with additional features.\nTODO: add recommendation from AI how to implement the features step by step\n\n\nCoding\nNow for the actual implementation, my AI-assisted coding setup is actually really basic. I‚Äôm not running a swarm of sub-agents overnight to parallelize tasks, review pull requests, and hand me a finished app the next morning. Partly that‚Äôs because I still have enough trust issues and would rather supervise the development step by step. But if we‚Äôre being honest, it‚Äôs also because I haven‚Äôt yet had the patience to dive this deep into the fast-moving ecosystem of AI agents and tooling (how many VS Code clones with AI integration do we have now?!) and prefer to maintain a reasonably predictable workflow.\nSo what I actually use is the Zed IDE, built from the ground up for AI integration and driving recent standards like the agent client protocol, together with Claude Code. I have Claude Code running inside the IDE rather than as a CLI, since I really appreciate a thoughtful GUI, for example, to review the agent‚Äôs code changes. I selected Claude Code based on some preliminary experiments comparing different AI models and because Anthropic has set the standard for innovation in coding agents, while other companies mainly scrambled to keep up.\nTODO: add screenshot of IDE\nAs with web-based chats, I find global instructions essential for getting useful results from a local coding agent. For that, I use an AGENTS.md file.2 It contains\n\na project overview derived from the CDD ‚Äúwhy‚Äù sketches,\ninstructions on how the agent should interact with the codebase (for example, using the ty type checker instead of the former default mypy for Python),\nand, most importantly, a set of coding guidelines reflecting what I consider good code.\n\nYou can find my current version in the project‚Äôs GitHub repository (TODO: add link), but it‚Äôs very much a living document. Whenever the AI does something stupid, I‚Äôll revise the instructions to improve the likelihood of better results in future runs.\nAfter all these preparations‚Äîreasonably detailed sketches, plus a clear set of instructions and constraints‚Äîthe implementation itself becomes fairly straightforward. As Bent Flyvbjerg argues in How Big Things Get Done, a solid, well-thought-out plan gets you halfway to smooth execution and delivery.\nBut of course, the process is still iterative. Early sketches inevitably have gaps that only surface during implementation. In principle, you could then go back, update the sketch documents, and generate a new prompt from the diff. In practice, I usually find it quicker to ask the AI to fix the issue directly and archive the sketches once they‚Äôve been implemented.",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#the-result",
    "href": "2026-01-cdd-humans-ai.html#the-result",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "The Result",
    "text": "The Result\nTODO! The implementation took roughly X hours and consumed $X in Claude Code tokens. You can check out the code on GitHub and interact with the live application on Render.\nTODO: add screenshot of application\nKeep in mind this is still a toy project: achieving the first 80% was relatively straightforward, but‚Äîas always‚Äîthe remaining 20% (or likely more in this case) is where the real challenges lie. From my experience at alcemy, just a few of the critical components missing for a production-ready system include:\n\nUser management, authentication, and other security considerations\nIntegrating data with automated exports from last-century data management systems\nBuilding an ML model that delivers reliable predictions despite measurement errors, which usually requires integrating deep domain knowledge through feature engineering or hybrid models that combine data-driven approaches with a mechanistic understanding of the underlying processes\nRunning the system in production, including provisioning infrastructure, handling ongoing data issues, retraining models regularly, and adapting to new edge cases\nChange management: ensuring users adopt and ideally enjoy using your product\n\nAll of these require substantial human creativity, domain expertise, and empathy for users.\n\nWhere to Go From Here\nThe most important advice that I can give you for AI-assisted coding‚Äîbeyond clearly defining what you want to build‚Äîis to keep experimenting. It takes practice to discover which prompts work and when the AI goes off the rails. And then you‚Äôll need to relearn this again every few months as new models and tools appear. But it‚Äôs also fun to keep being positively surprised by your new AI colleague and see how it can amplify your creativity and handle the tedious tasks, letting you focus on the ideas that matter.\n\nAcknowledgements\nThe creation of this article was inspired by some great conversations with my wonderful colleagues at WPS (but any potentially controversial opinions expressed here are my own üòâ).",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "2026-01-cdd-humans-ai.html#footnotes",
    "href": "2026-01-cdd-humans-ai.html#footnotes",
    "title": "Clarity-Driven Development: For Humans & AI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA system prompt is a set of instructions automatically included in all conversations. Depending on how you use an AI, this can live in personal preferences (for web interfaces) or in a configuration file such as an AGENTS.md.‚Ü©Ô∏é\nAt the time of writing, AGENTS.md is unfortunately not yet a universally accepted standard across IDEs and coding agents, which is problematic when collaborating with people using different setups. In my case, I work around this by referencing @AGENTS.md from my CLAUDE.md file so Claude Code reliably picks up the instructions. Ideally, this won‚Äôt be necessary in the future.‚Ü©Ô∏é",
    "crumbs": [
      "CDD for Humans & AI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Thanks for reading!\nI‚Äôm always looking for feedback or suggestions ‚Äì don‚Äôt hesitate to contact me at hey@franziskahorn.de. üòä\nYou might also be interested in my books:\n\nClarity-Driven Development of Scientific Software (2025) ‚Äî also available for purchase on Leanpub\nA Practitioner‚Äôs Guide to Machine Learning (2021)",
    "crumbs": [
      "Home"
    ]
  }
]
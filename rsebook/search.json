[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Software Engineering: A Primer",
    "section": "",
    "text": "Preface\nThis book is meant to empower researchers to code with confidence and clarity.\nIf you studied something other than computer science‚Äîespecially in the natural sciences like physics, chemistry, or biology‚Äîit‚Äôs likely you were never taught how to properly develop software. Yet, you‚Äôre often still expected to write code as part of your daily work. Maybe you‚Äôve taken a programming course like Python for Biologists and can put together functional scripts through trial and error (with a little help from ChatGPT). But chances are, no one ever showed you how to write well-structured, maintainable, and reusable code that could make your life‚Äîand collaborating with your colleagues‚Äîso much easier.\nThis book is for you if you want to:\n\nWrite functional software more quickly\nUse a structured approach to design better programs\nReuse your code in future projects\nFeel confident about what your scripts are doing\nPrepare your research code for production\nShare your work with pride.\n\nWhether you‚Äôre just beginning your scientific journey‚Äîperhaps working on your first major project like a master‚Äôs thesis or your first paper‚Äîor you‚Äôre contemplating a move from academia to industry, the practical advice in this book can guide you along the way. We will approach software design from first principles and tackle research questions with a product mindset. While the book contains some example code in Python to illustrate the concepts, the general ideas are independent of any programming language.\nSoftware development is a craft that‚Äôs best learned with the guidance of a senior colleague‚Äîsomeone who can show you the right tools and provide feedback through code reviews. Unfortunately, mentors with industry experience are rare in academia. While a book can‚Äôt replace an apprenticeship, I hope this one gives you a head start. It‚Äôs the book I wish I could have read at university and the one I always wanted to recommend to the students and junior developers I‚Äôve mentored.\nThis is still a draft version! Please write me an email, if you have any suggestions for how this book could be improved!\nEnjoy! üòä\n\nAcknowledgments\nI would like to thank Marcel Lengert for his thoughtful feedback.\nThe texts in this book were partly edited and refined with the help of ChatGPT, however, all original content is my own.\n\n\nHow to cite\n@book{horn2025rseprimer,\n  author = {Horn, Franziska},\n  title = {Research Software Engineering: A Primer},\n  year = {2025},\n  url = {https://franziskahorn.de/rsebook/},\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_purpose.html",
    "href": "01_purpose.html",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Types of Research Questions\nBefore writing your first line of code, it‚Äôs crucial to have a clear understanding of what you‚Äôre trying to achieve‚Äîspecifically, the purpose of your research. This clarity will not only help you reach your desired outcomes more efficiently but will also be invaluable when collaborating with others. Being able to explain your goals effectively ensures everyone is aligned and working toward the same objective.\nWe‚Äôll begin with an overview of common research goals and the types of data analysis needed to achieve them. Then, we‚Äôll discuss how to quantify the outcomes you‚Äôre trying to achieve. Finally, we‚Äôll explore how to visually communicate your research purpose, as visual representations are often the most effective way to convey complex ideas.\nIn research, your goal is to improve the status quo, whether by filling a knowledge gap or developing a new method, material, or process with better properties. Most research questions can be categorized into four broad groups, each associated with a specific type of analytics approach (Figure¬†1.1).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#types-of-research-questions",
    "href": "01_purpose.html#types-of-research-questions",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "Figure¬†1.1: Descriptive, diagnostic, predictive, and prescriptive analytics, with increasing computational complexity and need to write custom code.\n\n\n\n\nDescriptive Analytics\nThis approach focuses on observing and describing phenomena to establish baseline measurements or track changes over time.\nExamples include:\n\nIdentifying animal and plant species in unexplored regions of the deep ocean.\nMeasuring the physical properties of a newly discovered material.\nSurveying the political views of the next generation of teenagers.\n\nMethodology:\n\nCollect a large amount of data (e.g., samples or observations).\nCalculate summary statistics like averages, ranges, or standard deviations, typically using standard software tools.\n\n\n\nDiagnostic Analytics\nHere, the goal is to understand relationships between variables and uncover causal chains to explain why phenomena occur.\nExamples include:\n\nInvestigating how CO2 emissions from burning fossil fuels drive global warming.\nEvaluating whether a new drug reduces symptoms and under what conditions it works best.\nExploring how economic and social factors influence shifts toward right-wing political parties.\n\nMethodology:\n\nPerform exploratory data analysis, such as looking for correlations between variables.\nConduct statistical tests to support or refute hypotheses (e.g., comparing treatment and placebo groups).\nDesign of experiments to control for external factors (e.g., randomized clinical trials).\nBuild predictive models to simulate relationships. If these models match real-world observations, it suggests their assumptions correctly represent causal effects.\n\n\n\nPredictive Analytics\nThis method involves building models to describe and predict relationships between independent variables (inputs) and dependent variables (outputs). These models often rely on insights from diagnostic analytics, such as which variables to include in the model and how they might interact (e.g., linear or nonlinear dependence). Despite its name, this approach is not just about predicting the future, but used to estimate unknown values in general (e.g., variables that are difficult or expensive to measure). It also includes any kind of simulation model to describe a process virtually (i.e., to conduct in silico experiments).\nExamples include:\n\nWeather forecasting models.\nDigital twin of a wind turbine to simulate how much energy is generated under different conditions.\nPredicting protein folding based on amino acid sequences.\n\nMethodology:\nThe key difference lies in how much domain knowledge informs the model:\n\nWhite-box (mechanistic) models: Based entirely on known principles, such as physical laws or experimental findings. These models are often manually designed, with parameters fitted to observed data.\nBlack-box (data-driven) models: Derived primarily from observational data. Researchers usually test different model types (e.g., neural networks or Gaussian processes) and choose the one with the highest accuracy.\nGray-box (hybrid) models: These combine mechanistic and data-driven approaches. For example, the output of a mechanistic model may serve as an input to a data-driven model, or the data-driven model may predict residuals (i.e., prediction errors) from the mechanistic model, where both outputs combined yield the final prediction.\n\n\n\n\n\n\nResources to learn more about data-driven models\n\n\n\n\n\nIf you want to learn more about how to create data-driven models and the machine learning (ML) algorithms behind them, these two free online books are highly recommended:\n\n[2] Supervised Machine Learning for Science by Christoph Molnar & Timo Freiesleben; A fantastic introduction focused on applying black-box models in scientific research.\n[3] A Practitioner‚Äôs Guide to Machine Learning by me; A broader overview of ML methods for a variety of use cases.\n\n\n\n\n\nAfter developing an accurate model, researchers can analyze its behavior (e.g., through a sensitivity analysis, which examines how outputs change with varying inputs) to gain further insights about the system (to feed back into diagnostic analytics).\n\n\nPrescriptive Analytics\nThis approach focuses on decision-making and optimization, often using predictive models.\nExamples include:\n\nScreening thousands of drug candidates to find those most likely to bind with a target protein.\nOptimizing reactor conditions to maximize yield while minimizing energy consumption.\n\nMethodology:\n\nDecision support: Use models for ‚Äúwhat-if‚Äù analyses to predict outcomes of different scenarios. For example, models can estimate the effects of limiting global warming to 2¬∞C versus exceeding that threshold, thereby informing policy decisions.\nDecision automation: Use models in optimization loops to systematically test input conditions, evaluate outcomes (e.g., resulting predicted material quality), and identify the best conditions automatically.\n\n\n\n\n\n\nModel accuracy is crucial\n\n\n\nThese recommendations are only as good as the underlying models. Models must accurately capture causal relationships and often need to extrapolate beyond the data used to build them (e.g., for disaster simulations). Data-driven models are typically better at interpolation (predicting within known data ranges), so results should ideally be validated through additional experiments, such as testing the recommended new materials in the lab.\n\n\n\nTogether, these four types of analytics form a powerful toolkit for tackling real-world challenges: descriptive analytics provides a foundation for understanding, diagnostic analytics uncovers the causes behind observed phenomena, predictive analytics models future scenarios based on this understanding, and prescriptive analytics turns these insights into actionable solutions. Each step builds on the previous one, creating a systematic approach to answering complex questions and making informed decisions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#evaluation-metrics",
    "href": "01_purpose.html#evaluation-metrics",
    "title": "1¬† Research Purpose",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nTo demonstrate the impact of your work and compare your solution against existing approaches, it‚Äôs crucial to define what success looks like quantitatively. Consider these common evaluation metrics to measure the outcome of your research and generate compelling results:\n\nNumber of samples: This refers to the amount of data you‚Äôve collected, such as whether you surveyed 100 or 10,000 people. Larger sample sizes can provide more robust and reliable results. But you also need to make sure your sample is representative of the population as a whole, i.e., to avoid sampling bias.\nReliability of measurements: This evaluates the consistency of your data. For example, how much variation occurs if you repeat the same measurement, e.g., run a simulation with different random seeds. This is important as others need to be able to reproduce your results.\nStatistical significance: The outcome of a statistical hypothesis test, such as a p-value that indicates whether the difference in symptom reduction between the treatment and placebo groups is significant.\nModel accuracy: For predictive models, this includes:\n\nStandard metrics like \\(R^2\\) to measure how closely the model‚Äôs predictions align with observational data.\nCross-validation scores to assess performance on new data.\nUncertainty estimates to understand how confident the model is in its predictions.\n\nAlgorithm performance: This includes metrics like memory usage and the time required to fit a model or make predictions, and how these values change as the dataset size increases. Efficient algorithms are crucial when scaling to large datasets or handling complex simulations.\nKey Performance Indicators (KPIs): These are the practical measures that matter in your field. For example:\n\nFor a chemical process: yield, purity, energy efficiency\nFor a new material: strength, durability, cost\nFor an optimization task: convergence time, solution quality\n\n\nYour evaluation typically involves multiple metrics. For example, in prescriptive analytics, you need to demonstrate both the accuracy of your model and that the recommendations generated with it led to a genuinely optimized process or product. Before starting your research, review similar work in your field to understand which metrics are standard in your community.\n\n\n\n\n\n\nFigure¬†1.2: The metrics we‚Äôre interested in often represent trade-offs. For example, we want a high quality product, but it should also be cheap. Or a good model accuracy, but at the same time not use excessive compute resources. Your approach might not outperform existing baselines on all metrics, but its trade-off could still be preferable.\n\n\n\nIdeally, you should already have an idea of how existing solutions perform on these metrics (e.g., based on findings from other publications) to establish the baseline your solution should outperform. You‚Äôll likely need to replicate at least some of these baseline results (e.g., by reimplementing existing models) to ensure your comparisons are not influenced by external factors. But understanding where the ‚Äúcompetition‚Äù stands can also help you identify secondary metrics where your solution could excel. For example, even if there‚Äôs little room to improve model accuracy, existing solutions might be too slow to handle large datasets efficiently (Figure¬†1.2).1\nThese results are central to your research (and publications), and much of your code will be devoted to generating them, along with the models and simulations behind them. Clearly defining the key metrics needed to demonstrate your research‚Äôs impact will help you focus your programming efforts effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#draw-your-why",
    "href": "01_purpose.html#draw-your-why",
    "title": "1¬† Research Purpose",
    "section": "Draw Your Why",
    "text": "Draw Your Why\nWhether you‚Äôre collaborating with colleagues, presenting at a conference, or writing a paper‚Äîclearly communicating the problem you‚Äôre solving and your proposed solution is essential.\nVisual representations are particularly powerful for conveying complex ideas. One effective approach is creating ‚Äúbefore and after‚Äù visuals that contrast the current state of the field with your proposed improvements (Figure¬†1.3).\nThe ‚Äúbefore‚Äù scenario might show a lack of data, an incomplete understanding of a phenomenon, poor model performance, or an inefficient process or material. The ‚Äúafter‚Äù scenario highlights how your research addresses these issues and improves on the current state, such as refining a predictive model or enhancing the properties of a new material.\n\n\n\n\n\n\nFigure¬†1.3: Exemplary research goals and corresponding ‚Äúbefore and after‚Äù visuals for descriptive, diagnostic, predictive, and prescriptive analytics tasks.\n\n\n\nAt this point, your ‚Äúafter‚Äù scenario might be based on a hypothesis or an educated guess about what your results will look like‚Äîand that‚Äôs totally fine! The purpose of visualizing your goal is to guide your development process. Later, you can update the picture with actual results if you decide to include it in a journal publication, for example.\nOf course, not all research goals are tied directly to analytics. Sometimes the main improvement is more qualitative, for example, focusing on design or functionality (Figure¬†1.4). Even in these cases, however, you‚Äôll often need to demonstrate that your new approach meets or exceeds existing solutions in terms of other key performance indicators (KPIs), such as energy efficiency, speed, or quality parameters like strength or durability.\n\n\n\n\n\n\nFigure¬†1.4: This example illustrates a task where a robot must reach its target (represented by money) as efficiently as possible. Original approach (left): The robot relied on information encoded in the environment as expected rewards. To determine the shortest path to the target, the robot required a large sensor (shown as a yellow circle) capable of scanning nearby fields to locate the highest reward. New approach (right): Instead of relying on reward values scattered across the environment, the optimal direction is now encoded directly in the current field. This eliminates the need for large sensors, as the robot only needs to read the value of its current position, enabling it to operate with a much smaller sensor and thereby reducing hardware costs. Additional quantitative evaluation: It still needs to be demonstrated that with the new approach, the robot reaches its target at least as quickly as with the original approach.\n\n\n\nGive it a try‚Äîdoes the sketch help you explain your research to your family?\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe problem you‚Äôre trying to solve.\nExisting solutions to this problem, i.e., the baseline you‚Äôre competing against.\nWhich metrics should be used to quantify your improvement on the current state.\n\n\n\n\n\n\n\n[1] Callaway E. Chemistry Nobel Goes to Developers of AlphaFold AI That Predicts Protein Structures. Nature 634(8034), 525‚Äì526 (2024).\n\n\n[2] Freiesleben T, Molnar C. Supervised Machine Learning for Science: How to Stop Worrying and Love Your Black Box. (2024).\n\n\n[3] Horn F. A Practitioner‚Äôs Guide to Machine Learning. (2021).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "01_purpose.html#footnotes",
    "href": "01_purpose.html#footnotes",
    "title": "1¬† Research Purpose",
    "section": "",
    "text": "For example, currently, a lot of research aims to replace traditional mechanistic models with data-driven machine learning models, as these enable significantly faster simulations. A notable example is the AlphaFold model, which predicts protein folding from amino acid sequences‚Äîa breakthrough so impactful it was recognized with a Nobel Prize [1].‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Research Purpose</span>"
    ]
  },
  {
    "objectID": "02_data.html",
    "href": "02_data.html",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Data Types\nIn the previous chapter, we‚Äôve gained clarity on the problem you‚Äôre trying to solve and how to quantify the improvements your research generates. Now it‚Äôs time to dive deeper into what these results might actually look like and the data on which they are built.\nIn one form or another, you‚Äôre research will rely on data, both collected or generated by yourself and possibly others.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-types",
    "href": "02_data.html#data-types",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "Structured vs.¬†Unstructured Data\nData can take many forms, but one key distinction is between structured and unstructured data (Figure¬†2.1).\n\n\n\n\n\n\nFigure¬†2.1: Structured and unstructured data.\n\n\n\nStructured data is organized in rows and columns, like in Excel spreadsheets, CSV files, or relational databases. Each row represents a sample or observation (a data point), while each column corresponds to a variable or measurement (e.g., temperature, pressure, household income, number of children).\nUnstructured data, in contrast, lacks a predefined structure. Examples include images, text, audio recordings, and videos, typically stored as separate files on a computer or in the cloud. While these files might include structured metadata (e.g., timestamps, camera settings), the data content itself can vary widely‚Äîfor instance, audio recordings can range from seconds to hours in length.\nStructured data is often heterogeneous, meaning it includes variables representing different kinds of information with distinct units or scales (e.g., temperature in ¬∞C and pressure in kPa). Unstructured data tends to be homogeneous; for example, there‚Äôs no inherent difference between one pixel and the next in an image.\n\n\n\n\n\n\nThis book focuses on structured data\n\n\n\nEven though unstructured data is common in science (e.g., microscopy images), for simplicity, this book focuses on structured data. Furthermore, for now we‚Äôll assume that your data is stored in an Excel or CSV file, i.e., a spreadsheet with rows (samples) and columns (variables), on your computer. Later in Chapter 6, we‚Äôll discuss more advanced options for storing and accessing data, such as databases and APIs.\n\n\n\n\nProgramming Data Types\nEach variable in your dataset (i.e., each column in your spreadsheet) is represented as a specific data type, such as:\n\nNumbers (integers for whole numbers or floats for decimals)\nStrings (text)\nBoolean values (true/false)\n\nIn programming, these are so-called primitive data types (as opposed to composite types, like arrays or dictionaries containing multiple values, or user-defined objects) and define how information is stored in computer memory.\n\n\n\n\n\n\nData types in Python\n\n\n\n\n\n# integer\ni = 42\n# float\nx = 4.1083\n# string\ns = \"hello world!\"\n# boolean\nb = False\n\n\n\n\n\nStatistical Data Types\nEven more important than how your data is stored, is understanding what your data means. Variables fall into two main categories:\n\nContinuous (numerical) variables represent measurable values (e.g., temperature, height). These are usually stored as floats or integers.\nDiscrete (categorical) variables represent distinct options or groups (e.g., nationality, product type). These are often stored as strings, booleans, or sometimes integers.\n\n\n\n\n\n\n\nMisleading data types\n\n\n\nBe cautious: a variable that looks numerical (e.g., 1, 2, 3) may actually represent categories. For example, a material_type column with values 1, 2, and 3 might correspond to aluminum, copper, and steel, respectively. In this case, the numbers are IDs, not quantities.\n\n\nRecognizing whether a variable is continuous or discrete is crucial for creating meaningful visualizations and using appropriate statistical models.\n\n\nTime Series Data\nAnother consideration is whether your data points are linked by time. Time series data often refers to numerical data collected over time, like temperature readings or sales numbers. These datasets are usually expected to exhibit seasonal patterns or trends over time.\nHowever, nearly all datasets involve some element of time. For example, if your dataset consists of photos, timestamps might seem unimportant, but they could reveal trends‚Äîlike changes in image quality due to new equipment.\n\n\n\n\n\n\nAlways record timestamps\n\n\n\nAlways include timestamps in your data or metadata to help identify potential correlations or unexpected trends over time.\n\n\nSometimes, you may be able to collect truly time-independent data (e.g., sending a survey to 1,000 people simultaneously and they all answer within the next 10 minutes). But usually, your data collection will take longer and external factors‚Äîlike an election during a longer survey period‚Äîmight unintentionally affect your results. By tracking time, you can assess and adjust for such influences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#data-analysis-results",
    "href": "02_data.html#data-analysis-results",
    "title": "2¬† Data & Results",
    "section": "Data Analysis Results",
    "text": "Data Analysis Results\nWhen analyzing data, the process is typically divided into two phases:\n\nExploratory Analysis: This involves generating a variety of plots to gain a deeper understanding of your data, such as identifying correlations between variables. It‚Äôs often a quick and dirty process to help you familiarize yourself with the dataset.\nExplanatory Analysis: This focuses on creating refined, polished plots intended for communicating your findings, such as in a publication or presentation. These visuals are designed to clearly convey your results to an audience that may not be familiar with your data.\n\n\nExploratory Analysis\nIn this initial analysis, the goal is to get acquainted with the data, check if the trends and relationships you anticipated are present, and uncover any unexpected patterns or insights.\n\nExamine the raw data:\n\nIs the dataset complete, i.e., does it contain all the variables and samples you expected?\n\nExamine summary statistics (e.g., mean, standard deviation (std), min/max values, missing value count, etc.):\n\nWhat does each variable mean? Given your understanding of the variable, are its values in a reasonable range?\nAre missing values encoded as NaN (Not a Number) or as ‚Äòunrealistic‚Äô numeric values (e.g., -1 while normal values are between 0 and 100)?\nAre missing values random or systematic (e.g., in a survey rich people are less likely to answer questions about their income or specific measurements are only collected under certain conditions)? This can influence how missing values should be handled, e.g., whether it makes sense to impute them with the mean or some other specific value (e.g., zero).\n\nExamine the distributions of individual (continuous) variables:\n\n\n\nHistogram, strip plot, violin plot, box plot, and summary statistics of the same values.\n\n\n\nAre there any outliers? Are these genuine edge cases or can they be ignored (e.g., due to measurement errors or wrongly encoded data)?\nIs the data normally distributed or does the plot show multiple peaks? Is this expected?\n\nExamine trends over time (by plotting variables over time, even if you don‚Äôt think your data has a meaningful time component, e.g., by lining up representative images according to their timestamps to see if there is a pattern):\n\n\n\nWhat caused these trends and what are their implications for the future? This plot shows fictitious data of the pressure in a pipe affected by fouling‚Äîthat is, a buildup of unwanted material on the pipe‚Äôs surface, leading to increased pressure. The pipe is cleaned at regular intervals, causing a drop in pressure. However, because the cleaning process is imperfect, the baseline pressure gradually shifts upward over time.\n\n\n\nAre there time periods where the data was sampled irregularly or samples are missing? Why?\nAre there any (gradual or sudden) data drifts over time? Are these genuine changes (e.g., due to changes in the raw materials used in the process) or artifacts (e.g., due to a malfunctioning sensor recording wrong values)?\n\nExamine relationships between two variables:\n\n\n\nDepending on the variables‚Äô types (continuous or discrete), relationships can be shown in scatter plots, box plots, or a table. Please note that not all interesting relations between the two variables can be detected through a high correlation coefficient, so you should always check the scatter plot for details.\n\n\n\nAre the observed correlations between variables expected?\n\nExamine patterns in multidimensional data (using a parallel coordinate plot):\n\n\n\nEach line in a parallel coordinate plot represents one data point, with the corresponding values for the different variables marked at the respective y-axis. The screenshot here shows an interactive plot created using the Python plotly library. By selecting value ranges for the different dimensions (indicated by the pink stripes), it is possible to spot interesting patterns resulting from a combination of values across multiple variables.\n\n\n\nDo the observed patterns in the data match your understanding of the problem and dataset?\n\n\n\n\nExplanatory Analysis\nMost of the plots you create during an exploratory analysis are likely for your eyes only. Any plots you do choose to share with a broader audience‚Äîsuch as in a paper or presentation‚Äîshould be refined to clearly communicate your findings. Since your audience is much less familiar with the data and likely lacks the time or interest to explore it in depth, it‚Äôs essential to make your results more accessible. This process is often referred to as explanatory analysis [1].\n\n\n\n\n\n\nDon‚Äôt force an exploratory analysis onto your audience\n\n\n\nDon‚Äôt ‚Äújust show all the data‚Äù and hope that your audience will make something of it‚Äîunderstand what they need to answer the questions they have.\n\n\n\nStep 1: Choose the right plot type\n\nGet inspired by visualization libraries (e.g., here or here), but avoid the urge to create fancy graphics; sticking with common visualizations makes it easier for the audience to correctly decode the presented information.\nDon‚Äôt use 3D effects!\nAvoid pie or donut charts (angles are hard to interpret).\nUse line plots for time series data.\nUse horizontal instead of vertical bar charts for audiences that read left to right.\nStart the y-axis at 0 for area & bar charts.\nConsider using small multiples or sparklines instead of cramming too much into a single chart.\n\n\n\n\nLeft: Bar charts (especially in 3D) make it hard to compare numbers over a longer period of time. Right: Trends over time can be more easily detected in line charts. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 2: Cut clutter / maximize data-to-ink ratio\n\nRemove border.\nRemove gridlines.\nRemove data markers.\nClean up axis labels.\nLabel data directly.\n\n\n\n\nCut clutter! [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 3: Focus attention\n\nStart with gray, i.e., push everything in the background.\nUse pre-attentive attributes like color strategically to highlight what‚Äôs most important.\nUse data labels sparingly.\n\n\n\n\nStart with gray and use pre-attentive attributes strategically to focus the audience‚Äôs attention. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]\n\n\n\n\nStep 4: Make data accessible\n\nAdd context: Which values are good (goal state), which are bad (alert threshold)? Should the value be compared to another variable (e.g., actual vs.¬†forecast)?\nLeverage consistent colors when information is spread across multiple plots (e.g., data from a certain country is always drawn in the same color).\nAnnotate the plot with text explaining the main takeaways (if this is not possible, e.g., in interactive dashboards where the data keeps changing, the title can instead include the question that the plot should answer, e.g., ‚ÄúIs the material quality on target?‚Äù).\n\n\n\n\nTell a story. [Example adapted from: Storytelling with Data by Cole Nussbaum Knaflic]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#draw-your-what",
    "href": "02_data.html#draw-your-what",
    "title": "2¬† Data & Results",
    "section": "Draw Your What",
    "text": "Draw Your What\nYou may not have looked at your data yet‚Äîor maybe you haven‚Äôt even collected it‚Äîbut it‚Äôs important to start with the end in mind.\nIn software development, a UX designer typically creates mockups of a user interface (like the screens of a mobile app) before developers begin coding. Similarly, in our case, we want to start with a clear picture of what the output of our program should look like. The difference is that, instead of users interacting with the software themselves, they‚Äôll only see the plots or tables that your program generated, maybe in a journal article.1\nBased on your takeaways from the previous chapter‚Äîabout the problem you‚Äôre solving and the metrics you should use to evaluate your solution‚Äîtry sketching what your final results might look like. Ask yourself: What figures or tables would best communicate the advantages of my approach?\nDepending on your research goals, your results might be as simple as a single number, such as a p-value or the total number of people surveyed. However, if you‚Äôre reading this, you‚Äôre likely tackling something that requires a more complex analysis. For example, you might compare your solution‚Äôs overall performance to several baseline approaches or illustrate how your solution converges over time (Figure¬†2.2).\n\n\n\n\n\n\nFigure¬†2.2: Exemplary envisioned results: The plots show the outcome of a multi-agent simulation, where ‚Äòmy approach‚Äô clearly outperforms two baseline methods. In this simulation, a group of agents is tasked with locating a food source and transporting the food back to their home base piece by piece. The ideal algorithm identifies the shortest path to the food source quickly to maximize food collection. Each algorithmic approach is tested 10 times using different random seeds to evaluate reliability. The plots display the mean and standard deviations across these runs. Left: How quickly each algorithm converges to the shortest path (resulting in the highest number of agents delivering food back to the home base per step). Right: Cumulative food collected by the end of the simulation.2\n\n\n\nIt‚Äôs important to remember that your actual results might look very different from your initial sketches‚Äîthey might even show that your solution performs worse than the baseline. This is completely normal. The scientific method is inherently iterative, and unexpected results are often a stepping stone to deeper understanding. By starting with a clear plan, you can generate results more efficiently and quickly pivot to a new hypothesis if needed. When your results deviate from your expectations, analyzing those differences can sharpen your intuition about the data and help you form better hypotheses in the future.\nOnce you‚Äôve visualized the results you want, work backward to figure out what data you need to create them. This is especially important when you‚Äôre generating the data yourself, such as through simulations. For instance, if you plan to plot how values change over time, you‚Äôll need to record variables at every time step rather than just saving the final outcome of a simulation (duh!). Similarly, if you want to report your model‚Äôs accuracy (Figure¬†2.3), you‚Äôll need:\n\nInput variables for each data point to generate predictions (= model output).\nThe actual (true) values for each data point.\nA way to compute the overall deviation between predictions and true values, such as using an evaluation metric like \\(R^2\\).\n\n\n\n\n\n\n\nFigure¬†2.3: Work backward from the desired results to determine what data is necessary to create them.\n\n\n\nBy working backward from your desired results to the required data, you can design your code and analysis pipeline to ensure your program delivers exactly what you need.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nThe specific results (tables and figures) you want to create to show how your solution outperforms existing approaches (e.g., in terms of accuracy, speed, etc.).\nThe underlying data needed to produce these results (e.g., what rows and columns should be in your spreadsheet).\n\n\n\n\n\n\n\n[1] Knaflic CN. Storytelling with Data: A Data Visualization Guide for Business Professionals. John Wiley & Sons (2015).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "02_data.html#footnotes",
    "href": "02_data.html#footnotes",
    "title": "2¬† Data & Results",
    "section": "",
    "text": "A former master‚Äôs student that I mentored humorously called this approach ‚Äúplot-driven development,‚Äù a nod to test-driven development (TDD) in software engineering, where you write a test for your function first and then implement the function to pass the test. You could even use these sketches of your results as placeholders if you‚Äôre already drafting a paper or presentation.‚Ü©Ô∏é\nThese plots and the next were generated with Python using matplotlib‚Äôs plt.xkcd() setting and the xkcd script font. A pen and paper sketch will be sufficient for your case.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Results</span>"
    ]
  },
  {
    "objectID": "03_tools.html",
    "href": "03_tools.html",
    "title": "3¬† Tools",
    "section": "",
    "text": "Programming Languages\nBefore we continue with creating your results‚Äîi.e., actually start developing software‚Äîlet‚Äôs take a quick tour of some tools that can make your software engineering journey smoother.\nAlthough the code examples in this book use Python, the general principles discussed here apply to most programming languages.\nDifferent programming languages suit different needs. Here‚Äôs a quick overview of some popular ones used in science and engineering:\nDue to its broad applicability and popularity in industry, Python is used for the examples in this book. However, you should choose the programming language that is most popular in your field as this will make it easier for you to find relevant resources (e.g., tailored libraries) and collaborate with colleagues.\nThere are plenty of great books and other resources available to teach you programming fundamentals, which is why this book focuses on higher level concepts. Going forward we‚Äôll assume that you‚Äôre familiar with the basic syntax and functionality of your programming language of choice (incl.¬†key scientific libraries). For example, to learn Python essentials, you can work through this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#programming-languages",
    "href": "03_tools.html#programming-languages",
    "title": "3¬† Tools",
    "section": "",
    "text": "R: Commonly used for statistics, with rich functionality to create data visualizations, fit statistical models (like different types of regression), and conduct advanced statistical tests (like ANOVA). The poplar Shiny framework also makes it possible to create interactive dashboards that run as web applications.\nMATLAB: Once dominant in engineering, used for simulations. But due to its high licensing costs, MATLAB is being replaced more and more by Python and Julia.\nJulia: Gaining traction in scientific computing for its speed and modern syntax.\nPython: A versatile language with strong support for data science, AI, web development, and more. Its active open source community has created many popular libraries for scientific computing (numpy, scipy), machine learning (scikit-learn, TensorFlow, PyTorch), and web development (FastAPI, streamlit).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#version-control",
    "href": "03_tools.html#version-control",
    "title": "3¬† Tools",
    "section": "Version Control",
    "text": "Version Control\nVersion control is essential in software development to keep track of code changes and collaborate effectively. Think of it as a time machine that lets you revert to any version of your code or examine how it evolved.\n\nWhy Use Version Control?\n\nTrack changes: See what you‚Äôve modified and when, with the ability to revert if necessary.\nReview collaborators‚Äô changes: When working with others, reviewing their changes before they are merged with the main version of the code (in so-called pull or merge requests) ensures quality and provides opportunities to teach each other better ways of doing things.\nNot just for code: Version control can be used for any kind of file. While it‚Äôs less effective for binary formats like images or Microsoft Word documents where you can‚Äôt create a clean ‚Äúdiff‚Äù between two versions, you should definitely give it a try when writing your next paper in a text-based format like LaTeX.\n\n\n\nGit\nThe go-to tool for version control is Git. While desktop clients exist, you can also use git directly in the terminal as a command line tool.\nIf you‚Äôre new to Git, this beginner‚Äôs guide is a great place to start.\n\n\n\n\n\n\nEssential Git Commands\n\n\n\n\n\n\ngit init: Start a new repository in the current folder.\ngit status: View changes.\ngit diff: View differences between file versions before committing.\ngit add [file]: Stage files for a commit.\ngit commit -m \"message\": Save staged changes.\ngit push: Upload changes to a remote repository (e.g., on GitHub).\ngit pull: Download changes from a remote repository.\ngit branch: Create or list branches.\ngit checkout [branch]: Switch branches.\ngit merge [branch]: Combine branches.\n\n\n\n\nBy default, your repository‚Äôs files are on the main branch. Creating a new branch is like stepping into an alternate universe where you can experiment without affecting the main timeline. When making a major change or adding a new feature, it‚Äôs good practice to create a new branch, like new-feature, and implement your changes there. Once you‚Äôre satisfied with the result, you can merge the changes back into the main branch.\nThis approach keeps the main branch stable and ensures you always have a working version of your code. If you decide against your new feature, you can simply abandon the branch and start fresh from main. By creating a merge request (MR) once your new-feature branch is ready, you or a collaborator can review the changes thoroughly before merging them into main.\nTo publish your code or collaborate with others, your repository (i.e., the folder under version control) can be hosted on a platform like:\n\nGitHub: Great for open-source projects and public personal repositories to show off your skills.\nGitLab: Supports self-hosting, making it ideal for organizational needs.\n\nWe strongly encourage you to publish any code related to your publications on one of these platforms to promote reproducibility of your results! üë©‚Äçüî¨\n\n\n\n\n\n\nData Versioning\n\n\n\nIn addition to the changes made to your code, you should also keep track of how your data is generated and transformed over time (data lineage). While small datasets can be included in your repository (e.g., in a separate data/ folder), there are also more tailored tools available specifically to version your data, like DVC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#development-environment",
    "href": "03_tools.html#development-environment",
    "title": "3¬† Tools",
    "section": "Development Environment",
    "text": "Development Environment\nThe program you choose for writing code directly impacts your productivity. While you can technically write code using a plain text editor (like Notepad on Windows or TextEdit on macOS), special-purpose text editors and integrated development environments (IDEs) provide a tailored experience that boosts productivity.\n\nText Editors\nDeveloper-focused text editors are lightweight tools with features like syntax highlighting and extensions for basic programming tasks.\nExamples include:\n\nSublime Text: Lightweight and fast, with excellent customization through lots of plugins.\nAtom: Open-source and backed by GitHub (though less popular than other tools).\nVim and Emacs: Some of the first code editors, often used as command line tools and beloved by keyboard shortcuts enthusiasts.\n\n\n\nTerminal\nWhen you write code in a text editor, you need a way to execute it. This is where the terminal comes in. A terminal, or console, lets you interact with your computer through the command line, using text-based commands. Think of it like stepping back to the 1970s‚Äîor like being one of those cool hackers you see on TV.\nOn macOS and Linux, a terminal app is already preinstalled. On Windows, different options exist to install a Unix-like terminal, like the Windows Terminal. Inside the terminal, there‚Äôs a shell: the actual program that processes the commands you type. The most common shells on Unix systems are bash and zsh, which are quite similar. For this book, we‚Äôll assume you‚Äôre using one of these.\nWith the shell, you can navigate your computer‚Äôs file system and run programs through their command-line interface (CLI). Try it out!\n\n\n\n\n\n\nBasic Shell Commands\n\n\n\nFollow along by typing these commands into your terminal. In parallel, you can watch your normal file browser to see files and folders appear or disappear as you go.\n\npwd: Print the current working directory‚Äîthis shows the path to where you opened the terminal.\nls: List files and directories in the current location. Use ls -la for more details, including hidden files (like .gitignore).\ncd path/to/folder: Change directory to the specified path. Tips: Use tab to autocomplete names. If the path starts with /, it‚Äôs absolute (from the file system‚Äôs root). If it starts with ~/, it‚Äôs relative to your home directory. Use .. to move up one folder.\nmkdir new_folder: Create a new directory named new_folder.\ntouch new_file.txt: Create an empty file named new_file.txt.\ncp new_file.txt copied_file.txt: Copy new_file.txt to copied_file.txt. Use mv instead of cp to move or rename files.\nrm new_file.txt: Delete new_file.txt. Add -r to delete directories. But be careful: files deleted this way bypass the trash and are gone for good, so double-check before hitting enter!\n\nYou can also run other CLI programs in the terminal, like using the git commands described earlier.\nA Python script can be executed with python script.py (assuming the script is in your current directory).\n\n\nNot all CLI programs mentioned in this book will be preinstalled on your machine. Linux systems already come with a command-line package manager (like apt on Ubuntu), which can be used to install other tools. A popular package manager for macOS is brew, while for Windows you can use winget.\nOnce you get comfortable with your shell, you can also create shell scripts (files with a .sh extension) to automate tasks and handle more complex workflows. These scripts can include conditionals, loops, and other programming constructs. For more information on bash scripting, check out this resource.\n\n\nFull IDEs\nIntegrated Development Environments (IDEs) combine all the tools you need in one place‚Äîfile browser, editor, terminal, Git support, debugger, and more. They are ideal for larger projects and provide support for more complex tasks, like renaming variables across multiple files when you‚Äôre refactoring your code.\nExamples include:\n\nVS Code: Minimalist by default but highly customizable with plugins, making it suitable for everything from basic editing to full-scale development.\nJetBrains IDEs (e.g., PyCharm): IDEs tailored to the needs of specific programming languages with very advanced features. You need to purchase a license to use the full version, but for many IDEs there is also a free community edition available.\nJupyterLab: An extension of Jupyter notebooks (see below), popular for data science and exploratory coding.\nRStudio: Tailored for R programming, with excellent support for data visualization, markdown reporting, and reproducible research workflows.\nMATLAB: The MATLAB programming language and IDE are virtually synonymous. However, its rich feature set comes with steep licensing fees.\n\n\n\nJupyter Notebooks\nJupyter notebooks are a unique format that lets you mix code, output (like plots), and explanatory text in one document. The name Jupyter is derived from Julia, Python, and R, the programming languages for which the notebook format, and later the JupyterLab IDE, were created. The IDE itself runs inside your web browser.\nNotebooks are great for exploratory data analysis and to create reproducible reports. However, since the notebooks themselves are composed of individual interactive cells that can be executed in any order, developing in notebooks often becomes messy quickly. We recommend that you keep the main logic and reusable functions in separate scrips or libraries and primarily use notebooks to create plots and other results. It is also good practice once you‚Äôre finished to restart the kernel and run your notebook again from top to bottom to make sure everything still works and you‚Äôre not relying on variables that were defined in now-deleted cells, for example.\n\n\n\n\n\n\nNotebooks as text files\n\n\n\nJupyter notebooks, stored as files ending in .ipynb, are internally represented as JSON documents. If you have your notebooks under version control (which you should üòâ), you‚Äôll notice that the diffs between versions look quite bloated. But do not despair! Tools like Jupytext can convert notebooks into plain text without loss of functionality.\n\n\n\n\n\n\n\n\nParameterize notebooks\n\n\n\nIf you want to execute the same notebook with multiple different parameter settings (e.g., create the same plots for different model configurations), have a look at papermill.\n\n\nIn addition to the original JupyterLab IDE and notebooks that you install on your computer, there are also free cloud-based options available, such as Google Colab, which even gives you free compute time on GPUs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#reproducible-setups",
    "href": "03_tools.html#reproducible-setups",
    "title": "3¬† Tools",
    "section": "Reproducible Setups",
    "text": "Reproducible Setups\n‚ÄúIt works on my machine‚Äù isn‚Äôt good enough for science. Reproducibility means your results can be replicated by others (and by you a few months later when the reviewers of your paper request changes to your experiments). The first step to achieve this is to manage your dependencies (i.e., external libraries used by your code) to ensure the environment in which your code is executed is identical for everyone that runs your code, every time. This can be done using virtual environments, or, if you want to go even further, containers like Docker, which will be discussed in Chapter 6.\n\n\n\n\n\n\nVirtual Environments in Python with poetry\n\n\n\n\n\nVirtual environments isolate your project‚Äôs dependencies, thereby ensuring consistency. For Python, a common tool to do this is poetry. It tracks the libraries and their versions in a pyproject.toml like this:\n[tool.poetry]\nname = \"example-project\"\nversion = \"0.1.0\"\ndescription = \"A sample Python project\"\nauthors = [\"Your Name &lt;youremail@example.com&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\nrequests = \"^2.26.0\"  # external libraries incl. versions\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nBasic commands:\n\npoetry new example-project: Create a new project (folder incl.¬†pyproject.toml file).\npoetry add [package]: Add a dependency (can also be done directly in the file).\npoetry install: Install all dependencies.\npoetry shell: Activate the virtual environment.\n\n\n\n\n\nHandling Randomness\nYour program will often depend on randomly sampled values, for example, when defining the initial conditions for a simulation or initializing a model before it is fitted to data (like a neural network). To ensure that your experiments can be reproduced, it is important that you always set a random seed at the beginning of your program so the random number generator starts from a consistent state.\n\n\n\n\n\n\nSetting Random Seeds in Python\n\n\n\n\n\nAt the beginning of your script, set a random seed (depending on the library that you‚Äôre using this can vary):\nimport random\nimport numpy as np\n\nrandom.seed(42)\nnp.random.seed(42)\n\n\n\nTo get a better idea of how much your results depend on the random initialization and therefore how robust they are, it is advisable to always run your code with multiple random seeds and compare the results (e.g., compute the mean and standard deviation of the outcomes of different runs like in Figure¬†2.2).\n\n\n\n\n\n\nRandom state at startup\n\n\n\nDepending on the programming language that you‚Äôre using, if you run a script without executing any other code before, the random number generator may or may not always start in the same state. This means, if you don‚Äôt set a random seed and, for example, run your script ten times from scratch, you may always receive the same result even though the results would differ if the code was run under different circumstances. To avoid surprises, you should always explicitly set the random seed to have more control over the results.\n\n\n\n\n\n\n\n\nHardware differences\n\n\n\nIf your code is run on very different hardware, e.g., a CPU vs.¬†a GPU (graphics card, used to train neural network models, for example), despite setting a random seed, your results might still differ slightly. This is due to how the different architectures internally represent float values, i.e., with what precision the numbers are stored in memory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#clean-and-consistent-code",
    "href": "03_tools.html#clean-and-consistent-code",
    "title": "3¬† Tools",
    "section": "Clean and Consistent Code",
    "text": "Clean and Consistent Code\nEspecially when working together with others, it can be helpful to follow to a style guide to produce clean and consistent code. Google published their style guides for multiple programming languages, which is a great resource and adhering to these rules will also help you to avoid common sources of bugs.\n\nFormatters & Linters\nSince programmers are often rather lazy, they developed tools that automatically fix your code to implement these rules where possible:\n\nFormatters rewrite code to follow a consistent style (e.g., add whitespace after commas).\nLinters analyze code for errors, inefficiencies, and deviations from best practices.\n\n\n\n\n\n\n\nFormatter & Linter in Python: ruff\n\n\n\n\n\nruff is a (super fast) formatter and linter for Python, written in Rust. You can install it via pip and configure it in the same pyproject.toml file that we also used for poetry. Then run it over you code like this:\nruff check        # see which errors the linter finds\nruff check --fix  # automatically fix errors where possible\nruff format       # automatically format the code\nYou‚Äôll probably want to add exceptions for some of the errors that the linter checks for in your pyproject.toml file as ruff is quite strict. üòâ\n\n\n\nIt is important to have the configuration for your formatter and linter under version control as well, so that all collaborators use the same settings and you avoid unnecessary changes (and bloated diffs in merge requests) when different people format the code.\n\n\nPre-commit Hooks\nIn the heat of the moment, you might forget to run the formatter and linter over your code before committing your changes. To avoid accidentally checking messy code into your repository, you can configure so-called ‚Äúpre-commit hooks‚Äù. Pre-commit hooks catch issues automatically by enforcing coding standards before committing or pushing code with git.\n\n\n\n\n\n\nSetting up pre-commit hooks\n\n\n\n\n\nFirst, you need to install pre-commit hooks, e.g., through Python‚Äôs package manger pip:\npip install pre-commit\nThen configure it in a file named .pre-commit-config.yaml (here done for ruff):\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n    - id: check-yaml\n    - id: end-of-file-fixer\n    - id: trailing-whitespace\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.8.3\n  hooks:\n    # Run the linter.\n    - id: ruff\n      args: [ --fix ]\n    # Run the formatter.\n    - id: ruff-format\nThen install the git hook scripts from the config file:\npre-commit install\nNow the configured hooks will be run on all changed files when you try to commit them and you can only proceed if all checks pass.\n\n\n\nTo catch any style inconsistencies after the code was pushed to your remote repository (e.g., in case one of your collaborators has not installed the pre-commit hooks), you can also add these checks to your CI/CD pipeline (see Chapter 6).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "03_tools.html#putting-it-all-together",
    "href": "03_tools.html#putting-it-all-together",
    "title": "3¬† Tools",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nWhen you set up all these tools, your repository should now look something like this (see here for more details; setup for programming languages other than Python will differ slightly):\nproject-name/\n‚îú‚îÄ‚îÄ .gitignore              # Exclude unnecessary files from version control\n‚îú‚îÄ‚îÄ README.md               # Describe the project purpose and usage\n‚îú‚îÄ‚îÄ pre-commit-config.yaml  # Pre-commit hook setup\n‚îú‚îÄ‚îÄ pyproject.toml          # Python dependencies and configs\n‚îú‚îÄ‚îÄ data/                   # Store (small) datasets\n‚îú‚îÄ‚îÄ notebooks/              # For exploratory analysis\n‚îú‚îÄ‚îÄ src/                    # Core source code\n‚îî‚îÄ‚îÄ tests/                  # Unit tests\nA clean project structure makes it easier to maintain your code.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to set up your development environment to code efficiently.\nHow to host your version-controlled repository on a platform like GitHub or GitLab, complete with pre-commit hooks to ensure well-formatted code.\nThe fundamental syntax of your programming language of choice (incl.¬†key scientific libraries) to get started.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "04_design.html",
    "href": "04_design.html",
    "title": "4¬† Software Design",
    "section": "",
    "text": "Avoid Complexity\nNow that your code repository is set up, are you itching to start programming? Hold on for a moment!\nOne of the most common missteps I‚Äôve seen junior developers take is jumping straight into coding without first thinking through what they actually want to build. Imagine trying to construct a house by just laying bricks without consulting an architect first‚Äîhalfway through you‚Äôd probably realize the walls don‚Äôt align, and you forgot the plumbing for the kitchen. You‚Äôd have to tear it down and start over! To avoid this fate for your software, it‚Äôs essential to make a plan and design the final outcome first. It doesn‚Äôt have to be perfect‚Äîa quick sketch on paper will do. And you‚Äôll likely need to adapt as you go (which is fine since we‚Äôll design with flexibility in mind!). But the more thought you put into planning, the smoother and faster execution will be.\nTo make sure your designs will be worthy of implementation, this chapter also covers key paradigms and best practices that will help you create clean, maintainable code that‚Äôs easy to extend and reuse in future projects.\nA common acronym in software engineering is KISS - ‚Äúkeep it simple, stupid!‚Äù. While this may be well-intentioned advice, it can only apply to individual parts of your code, as a full-fledged software is a system that consists of multiple components that interact with each other. Here, the best you can hope for is complicated, i.e., to avoid complexity (Figure¬†4.1).\nA complex system includes many elements that interact with each other in ways that are not easily traceable or for humans to comprehend. You might change something in one place and suddenly, something far on the other side breaks. That‚Äôs not good in software. We should always design for change, since inevitably, there is something we need to adapt or extend and when this happens, we want to be confident in what we‚Äôre doing and not afraid that our change will break something else or have unintended consequences that we‚Äôre not aware of. No one likes bugs.\nThis is why we want a complicated system: it still includes a lot of elements, but they are grouped in components, neat little subsystems, so the whole can be taken apart and each part can be understood on its own while the whole can also be understood without understanding each individual component in detail. Ideally, this means our system consists of a neat hierarchy of components at different levels of abstraction (Figure¬†4.2). To understand the code on one level of this hierarchy, we only need to understand what the components one level below are doing to get an idea of the purpose of the code. For example, to understand what is happening in plot_results, it is enough to know that there is a scatter plot created and we don‚Äôt need to know the details of how this plot is created, such as that it requires the computation of \\(R^2\\). By decomposing code in such a way, we reduce the cognitive load that is required to understand what the code is doing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#avoid-complexity",
    "href": "04_design.html#avoid-complexity",
    "title": "4¬† Software Design",
    "section": "",
    "text": "Figure¬†4.1: (Software) Systems can be of different complexity. A script or function that linearly executes a set of steps could be considered simple. But most software programs are (at least) complicated: they consist of multiple components that interact with each other. However, these can still be broken down into manageable subsystems, which makes it possible to understand the system as a whole. A complex system, in computer science referred to as ‚Äúspaghetti code‚Äù or a ‚Äúbig ball of mud‚Äù [1], contains many individual elements and interactions between them ‚Äì when you change something on one end it is unclear how this will affect the other pieces as it is difficult to understand how all the elements come together. (Figure adapted from [2])\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4.2: Complicated systems in software design can usually be represented as hierarchies that show different levels of abstraction, e.g., in this case for plotting the results of a predictive model, i.e., creating a scatter plot that shows the true vs.¬†predicted values together with the \\(R^2\\) value that indicates the overall performance of the model.\n\n\n\n\nElements of Software Systems\nBefore we continue, lets take a quick look at what the fundamental building blocks of our software systems are. To keep things simple, we distinguish between:\n\nVariables: Used to store data values, either primitive (like integers or strings, like we saw in Chapter 2) or composite, like lists and dictionaries, which can grow arbitrarily complex in some languages (e.g., a list can contain multiple dictionaries that themselves contain lists etc.).\n# primitive data type: float\nx = 4.1083\n# composite data type: list\nmy_list = [\"hello\", 42, x]\n# composite data type: dict\nmy_dict = {\n    \"key1\": \"hello\",\n    \"key2\": 42,\n    \"key3\": my_list,\n}\nFunctions: Used to calculate something and/or perform an action, usually given some input arguments. We distinguish between pure and impure functions [3]. A pure function is similar to a mathematical function \\(f: x \\to y\\) that computes and returns \\(y\\) given some \\(x\\). Impure functions have so-called ‚Äúside effects‚Äù, i.e., they perform some action that has effects that are visible outside of the function itself, e.g., because they create or modify a file. They also don‚Äôt necessarily have a return value.\ndef add(a, b):\n    # pure: a simple calculation\n    return a + b\n\ndef create_plot(x, y):\n    # impure: create and save a plot\n    plt.plot(x, y)\n    plt.savefig(\"my_figure.png\")\nObjects: They are basically a combination of variables and functions, i.e., specific constructs that store data values and have functions that usually access and modify these values. This comes in handy if you want to group multiple variables in one place because they logically belong together (e.g., the parameters used to configure a simulation model) and you want to use them store the results from functions.\nclass MyModel:\n\n  def __init__(self, param1, param2):\n      # initialize values from input arguments\n      self.param1 = param1\n      self.param2 = param2\n      # create additional variables\n      self.results = []\n\n  def run_simulation():\n      # do something\n      self.result = [1, 2, 3]\n\n\n\nSystem Boundaries\nTo avoid creating a complex mess, it is very important that we are clear on our (sub)system‚Äôs boundaries, i.e., what is in and what is out of our code‚Äôs scope. Adhering to clear boundaries makes it very easy to change our code. Unfortunately, though, establishing clear boundaries is easier said than done.\nLet‚Äôs start with a very simple example to illustrate the concept of scope:\ndef n_times_x(x, n):\n    result = 0\n    for i in range(n):\n        result += x\n    return result\n\nif __name__ == '__main__':\n    new_x = 3\n    new_n = 5\n    # call our function with some values\n    my_result = n_times_x(new_x, new_n)\n    print(my_result)  # should show 15\nOur code of interest (i.e., the subsystem we‚Äôre looking at) is the function n_times_x. Inside the scope of this function with have the variables x, n, i, and result, i.e., when we can refer to them by name and when we execute the code it is clear what we‚Äôre referring to. Outside of the scope of the function, where it is called in the program‚Äôs __main__ function, we have the variables new_x, new_n, and my_result. If we tried to access any of the internal variables from n_times_x here, we would get an error, since these variables are hidden inside the scope of n_times_x.\n\n\n\n\n\n\nNote\n\n\n\nWhile under some circumstances, depending on how you structure your code, the ‚Äúinside code‚Äù could access the ‚Äúoutside code‚Äù‚Äôs variables, it is best to avoid this, i.e., try to have a clean separation between what is going on inside and outside of your code. Ideally, your code should only be concerned about what is going on inside your code and not depend on anything that exists outside of it. Therefore, if it needs access to any variables, you should just pass them as input arguments explicitly to make it clear what values your code uses.\n\n\nThis brings us to the boundaries, i.e., where ‚Äúinside‚Äù and ‚Äúoutside‚Äù code meet. In software development, the boundary of your ‚Äúinside code‚Äù is also referred to as its interface. This defines how you interact with the code and it can be seen as a contract with the outside world for how to use your code. For a pure function, like in our example, the interface is the function‚Äôs signature, i.e.,\n\nThe function name (n_times_x).\nThe input arguments (x and n).\nThe return values (result).\n\nAs long as we keep this interface the same, i.e., the function still calculates the same thing, we‚Äôre free to change whatever we want inside the function itself. For example, we could change the ridiculously inefficient implementation to use a proper multiplication:\ndef n_times_x(x, n):\n    result = n * x\n    return result\nAnd we don‚Äôt have to tell anyone about this change, since we kept the interface of the function the same and none of the outside code was aware or depended upon what was going on inside our function. This is the beauty of clear boundaries.\nOn the other hand, if you decide that you don‚Äôt like your function‚Äôs name, changing this means that everywhere else where your function is used you also need to change the name. When you use an IDE to code, it probably has a feature to refactor your code where you can indicate that you want to change the name and then it checks all the files in your project for where this function is called and then changes the name there as well. However, if you‚Äôre writing a library that is used in multiple place outside your current project, it will be very difficult to identify all the places where this function is used and notify the respective people to change the name. In practice this requires a deprecation process where the old interface is slowly phased out. This is why it really pays of to define good interfaces that remain stable for a long time.\n\n\n\n\n\n\nGo deep\n\n\n\nPowerful code has narrow interfaces with a deep implementation, i.e., the code does meaningful computations without exposing too much of its internals. For example, a narrow and deep function would maybe have two input arguments but extend over ten lines of code to do a complex calculation. When you split your code up into reusable functions, make sure that you don‚Äôt split it up so much that you end up with a function that takes six input arguments but then only computes something on one line. Instead, try to create meaningful units of code that help you to hide complicated logic behind a simple interface and thereby make your code easier to change.\n\n\nPure functions with clean boundaries help us to write easily understandable code without any surprises. They are also easy to test, since they do not depend on anything besides what is passed as their input arguments, so no matter how many times you call a pure function with the same inputs, you‚Äôre always going to get the same output. Your code gets more complex with impure functions: by definition, they have side effects and therefore interact and rely on the outside world. For example, they could write results to a file (e.g., create a plot) or read values from a database. This also means that if you run the code twice, you might get different results because something in the outside world has changed, e.g., your code crashes because the results file already exist or the output is different because someone has changed the values in the database. This can make it harder to predict how changes to your code or somewhere else will affect the system as a whole, for example, because you don‚Äôt know who else is accessing the same resources that your code uses.\nAs a best practice, you should always try to encapsulate as much critical logic as possible in pure functions, as this makes your code easier to understand and test. For example:\ndef pure_function(inputs):\n    # do something without side effects\n    output = ...\n    return output\n\ndef impure_function():\n    # read from outside file/database\n    data = ...\n    # do the main calculations\n    result = pure_function(data)\n    # write to outside file/database\n    result.to_csv(\"result.csv\")\n\nExtend your scope with objects\nSometimes, your code might depend on so many variables that you don‚Äôt want to pass all of these to a function. Especially when all of these variables and functionality are logically related, they should be grouped together into a class. A class, or an object, with is one concrete instantiation of a class, is great to create some extra scope for your functions.\nSince we want clean boundaries and ideally a narrow interface, it is important to distinguish between public and private attributes and methods of a class. Everything about a class that is public is part of its interface, i.e., the contract with the outside world, which means changing these later can cause extra work or issues elsewhere. Private attributes and methods are only for internal use and can therefore be changed more easily.\n\n\n\n\n\n\nPublic and Private in different programming languages\n\n\n\nDepending on the programming language that you use, there might be more access levels than public and private. For example, in Java there also exists protected and package. For our purposes it is sufficient to distinguish between what should be accessible internally (for your code only) and externally (part of the contract with the outside world).\nIn Python, nothing is really private as it is assumed that the programmer knows what she is doing when she accesses what she wants. By convention, variables and functions that are prefixed with _ or __ are for internal use only and you should only use them at your own risk. Therefore, it also makes sense to prefix all attributes and methods that you don‚Äôt want anyone else to mess with with underscores. While this does not provide any access guarantees, at least who ever uses these variables will be warned that they may change without notice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#draw-your-how",
    "href": "04_design.html#draw-your-how",
    "title": "4¬† Software Design",
    "section": "Draw Your How",
    "text": "Draw Your How\nNow it‚Äôs time for you to draw your design.\nBut keep it simple‚Äîyou don‚Äôt have to overengineer your design to account for every possibility. If you want to build a one family home, design a one family home. Of course, it can‚Äôt hurt to think ahead a little bit if you already know that some changes are likely to come (‚ÄúHow will the rooms change as the kids move out?‚Äù), but there is no value in trying to plan ahead for everything (‚ÄúWhat if the daughter wants to take over the house and transform it into an office for her 100+ people company?‚Äù).\n\n\n\n\n\n\nFigure¬†4.3: green boxes represent pure functions, blue boxes class methods, and purple boxes impure functions with side effects (reading or writing to external files)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "04_design.html#make-a-plan",
    "href": "04_design.html#make-a-plan",
    "title": "4¬† Software Design",
    "section": "Make a Plan",
    "text": "Make a Plan\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to design your program by building upon loosely coupled, reusable components.\nWhat steps your code entails.\n\n\n\n\n\n\n\n[1] Foote B, Yoder J. Big Ball of Mud. Pattern languages of program design 4, 654‚Äì692 (1997).\n\n\n[2] McChrystal GS, Collins T, Silverman D, Fussell C. Team of Teams: New Rules of Engagement for a Complex World. Penguin (2015).\n\n\n[3] Normand E. Grokking Simplicity: Taming Complex Software with Functional Thinking. Simon; Schuster (2021).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Software Design</span>"
    ]
  },
  {
    "objectID": "05_implementation.html",
    "href": "05_implementation.html",
    "title": "5¬† Implementation",
    "section": "",
    "text": "From Design to Code: Fill in the Blanks\nNow that you have a plan, it‚Äôs finally time to get started with the implementation.\nArmed with your design from the last chapter, you can now translate your sketch into a code skeleton. Start by outlining the functions, place calls to them where needed, and add comments for any steps you‚Äôll figure out later. For example, the design from Figure¬†4.3 could result in the following draft:\nOnce your skeleton stands, you ‚Äúonly‚Äù need to fill in the details, which is a lot less intimidating than facing a blank page. Plus, since you started with a thoughtful design, your final program is more likely to be well-structured and easy to understand. Compare this to writing code on the fly, where decisions about functions are often made haphazardly‚Äîyou‚Äôll appreciate the difference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#from-design-to-code-fill-in-the-blanks",
    "href": "05_implementation.html#from-design-to-code-fill-in-the-blanks",
    "title": "5¬† Implementation",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nclass Model:\n    def __init__(self, param1):\n        self.param1 = param1\n\n    def fit(self, x, y):\n        pass\n\n    def predict(self, x):\n        y = ...\n        return y\n\ndef preprocess(df):\n    df = ...\n    return df\n\ndef load_data(file_name):\n    df = pd.read_csv(file_name)\n    df = preprocess(df)\n    return df\n\ndef compute_R2(y, y_pred):\n    R2 = ...\n    return R2\n\ndef evaluate_and_plot(y, y_pred):\n    R2 = compute_R2(y, y_pred)\n    # ... create and save plot ...\n    return R2\n\nif __name__ == '__main__':\n    # script is called as `python script.py seed param1 [param2]`\n    seed, param1, param2 = ...\n    np.random.seed(seed)\n    model = Model(param1)\n    df_train = load_data(\"train.csv\")\n    model.fit(df_train.x, df_train.y)\n    df_test = load_data(\"test.csv\")\n    y_pred = model.predict(df_test.x)\n    R2 = evaluate_and_plot(df_test.y, y_pred)\n\n\n\n\n\n\nOrder of functions\n\n\n\nYour script likely includes multiple functions, so you‚Äôll need to decide their order from top to bottom. Since scripts typically start with imports (e.g., of libraries like numpy) and end with a main function, personally I prefer to put more general functions (i.e., the ones that are at the lower levels of abstraction in your call hierarchy and that only rely on external dependencies) towards the top of the file. This ensures that, as you read the script from top to bottom, each function depends only on what was defined before it. Maintaining this order avoids circular dependencies and encourages you to write reusable, modular functions that serve as building blocks for the code that follows.\n\n\n\n\n\n\n\n\n\nUsing AI Code Generators\n\n\n\nAI assistants like ChatGPT or GitHub Copilot can be helpful tools when writing code, especially at the level of individual functions. However, remember that these tools only reproduce patterns from their training data, which includes both good and bad code. As a result, the code they generate may not always be optimal. For instance, they might use inefficient for-loops instead of more elegant matrix operations. Similarly, support for less popular programming languages may be subpar.\nTo get better results, consider crafting prompts like: ‚ÄúYou are a senior Python developer with 10 years of experience writing efficient, edge-case-aware code. Write a function ‚Ä¶‚Äù\n\n\n\nMinimum Viable Results\nIn product development, there‚Äôs a concept called the Minimum Viable Product (MVP). This refers to the simplest version of a product that still provides value to users. The MVP serves as a prototype to gather feedback on whether the product meets user needs and to identify which features are truly essential. By iterating quickly and testing hypotheses, teams can increase the odds of creating a successful product that people will actually pay for.\nThis approach also has motivational benefits. Seeing something functional‚Äîeven if basic‚Äîearly on makes it easier to stay engaged. It‚Äôs far better than toiling for months without tangible results. We recommend applying this mindset to your research software development by starting with a script that generates ‚ÄúMinimum Viable Results.‚Äù\nThis means creating a program that produces outputs resembling your final results, like plots or tables, but using placeholder data instead of actual values. For instance:\n\nIf your goal is to build a prediction model, start with one that simply predicts the mean of the observed data.\nIf you‚Äôre developing a simulation, begin with random outputs, such as a random walk.\n\nBy starting with Minimum Viable Results, you can test your code end-to-end early on, see tangible progress, and iteratively improve from there.\nThis approach also serves as a ‚Äústupid baseline‚Äù‚Äîa simple, easy-to-beat reference point for your final method. It‚Äôs a sanity check: if your sophisticated solution can‚Äôt outperform this baseline, something‚Äôs off.\n\n\nBreaking Code into Modules\nStarting a new project often begins with all your code in a single script or notebook. This is fine for quick and small tasks, but as your project grows, keeping everything in one file becomes messy and overwhelming. To keep your code organized and easier to understand, it‚Äôs a good idea to move functionality into separate files, also called (sub)modules. Separating code into modules makes your project easier to navigate, test, and reuse.\nA typical first step is splitting the main logic of your analysis (main.py) from general-purpose helper functions (utils.py). Over time, as utils.py expands, you‚Äôll notice clusters of related functionality that can be moved into their own files, such as preprocessing.py, models.py, or plot_results.py. This modular approach naturally leads to a clean directory structure, which might look like this for a larger Python project:1\nsrc/\n‚îî‚îÄ‚îÄ my-package/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ main.py\n    ‚îú‚îÄ‚îÄ models/\n    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_a.py\n    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_b.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ my_model.py\n    ‚îî‚îÄ‚îÄ utils/\n        ‚îú‚îÄ‚îÄ __init__.py\n        ‚îú‚îÄ‚îÄ preprocessing.py\n        ‚îî‚îÄ‚îÄ plot_results.py\nIn main.py, you can import the relevant classes and functions from these modules to keep the main script clean and focused:\nfrom models.my_model import MyModel\nfrom utils import preprocessing\n\nif __name__ == '__main__':\n    # steps that will be executed when running `python main.py`\n    model = MyModel()\n\n\n\n\n\n\nKeep helper functions separate\n\n\n\nAlways separate reusable helper functions from the main executable code. This also means that files like utils/preprocessing.py should not include a main function, as they are not standalone scripts. Instead, these modules provide functionality that can be imported by other scripts‚Äîjust like external dependencies such as numpy.\nAs you tackle more projects, you may develop a set of functions that are so versatile and useful that you find yourself reusing them across multiple projects. At that point, you might consider packaging them as your own open-source library, allowing others to install and use it just like any other external library.\n\n\n\n\nKeep It Compact\nWhen writing code, aim to achieve your goals while using as little screen space as possible‚Äîthis applies to both the number of lines and their length.\n\n\n\n\n\n\nTips to create compact, reusable code\n\n\n\n\nAvoid duplication: Instead of copying and pasting code in multiple places, consolidate it into a reusable function to save lines.\nPrefer ‚Äòdeep‚Äô functions: Avoid extracting very short code fragments (1-2 lines) into a separate function, especially if this function would require many arguments. Such shallow functions with wide interfaces increase complexity without meaningfully reducing line count. Instead, strive for deep functions (spanning multiple lines) with narrow interfaces (e.g., only 1-3 input arguments, i.e., fewer arguments than the function has lines of code), which tend to be more general and reusable [5].\nAddress nesting: If your code becomes overly nested, this can be a sign that parts of the code should be moved into a separate function. This simplifies logic and shortens lines.\nUse Guard Clauses: Deeply nested if-statements can make code harder to read. Instead, use guard clauses [1] to handle preconditions (e.g., checking for wrong user input) early, leaving the ‚Äúhappy path‚Äù clear and concise. For example:\nif condition:\n    if not other_condition:\n        # do something\n        return result\nelse:\n    return None\nCan be refactored into:\nif not condition:\n    return None\nif other_condition:\n    return None\n# do something\nreturn result\nThis approach reduces nesting and improves readability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "href": "05_implementation.html#documentation-comments-a-note-to-your-future-self",
    "title": "5¬† Implementation",
    "section": "Documentation & Comments: A Note to Your Future Self",
    "text": "Documentation & Comments: A Note to Your Future Self\nWhile you write it, everything seems obvious. However, when revisiting your code a few months later (e.g., to try a different experiment), you‚Äôre often left wondering what the heck you were doing. This is especially true when some external constraint (like a library quirk) forced you to create a workaround instead of opting for the straightforward solution. When returning to such code, you might be tempted to replace the awkward implementation with something more elegant, only to rediscover why you chose that approach in the first place. This is where comments can save you some trouble. And they are even more important when collaborating with others who need to understand your code.\nWe distinguish between documentation and comments: Documentation provides the general description of when and how to use your code, such as function docstrings explaining what the function computes, its input parameters, and return values. This is particularly important for open source libraries where you can‚Äôt personally explain the code‚Äôs purpose and usage to others. Comments help developers understand why your code was written in a certain way, like explaining that unintuitive workaround. Additionally, for scientific code, you may also need to document the origin of certain values or equations by referencing the corresponding paper in the comments.\n\n\n\n\n\n\nCode should be self-documenting\n\n\n\nIdeally, your code should be written so clearly that it‚Äôs self-explanatory. Comments shouldn‚Äôt explain what the code does, only why it does that (when not obvious). Comments and documentation, like code, need to be maintained‚Äîif you modify code, update the corresponding comments, or they become misleading and harmful rather than helpful. Using comments sparingly minimizes the risk of confusing, outdated comments.\nInformative variable and function names are essential for self-explanatory code. When you‚Äôre tempted to write a comment that summarizes what the following block of code does (e.g., # preprocess data), consider moving these lines into a separate function with an informative name, especially if they contain significant, reusable logic.\n\n\n\nNaming is hard\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n‚Äì Phil Karlton2\n\nFinding informative names for variables, functions, and classes can be challenging, but good names are crucial to make the code easier to understand for you and your collaborators.\n\n\n\n\n\n\nTips for effective naming\n\n\n\n\nNames should reveal intent. Longer names (consisting of multiple words in snake_case or camelCase, depending on the conventions of your chosen programming language) are usually better. However, stick to domain conventions‚Äîif everyone understands X and y as feature matrix and target vector, use these despite common advice denouncing single letter names.\nBe consistent: similar names should indicate similar things.\nAvoid reserved keywords (i.e., words your code editor colors differently, like Python‚Äôs input function).\nUse verbs for functions, nouns for classes.\nUse affirmative phrases for booleans (e.g., is_visible instead of is_invisible).\nUse plurals for collections (e.g., cats instead of list_of_cats).\nAvoid encoding types in names (e.g., color_dict), since if you decide to change the data type later, you either need to rename the variable everywhere or the name is now misleading.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#tests-protect-what-you-love",
    "href": "05_implementation.html#tests-protect-what-you-love",
    "title": "5¬† Implementation",
    "section": "Tests: Protect What You Love",
    "text": "Tests: Protect What You Love\nWe all want our code to be correct. During development, we often verify this manually by running the code with example inputs to check if the output matches our expectations. While this approach helps ensure correctness initially, it becomes cumbersome to recreate these test cases later when the code needs changes. The simple solution? Package your manual tests into a reusable test suite that you can run anytime to check your code for errors.\nTests typically use assert statements to confirm that the actual output matches the expected output. For example:\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    # verify correctness with examples, including edge cases\n    # syntax: assert (expression that should evaluate to True), \"error message\"\n    assert add(2, 2) == 4, \"2 + 2 should equal 4\"\n    assert add(5, -6) == -1, \"5 - 6 should equal -1\"\n    assert add(-2, 10.6) == 8.6, \"-2 + 10.6 should equal 8.6\"\n    assert add(0, 0) == 0, \"0 + 0 should equal 0\"\n\n\n\n\n\n\nTesting in Python with pytest\n\n\n\nConsider using the pytest framework for your Python tests. Organize all your test scripts in a dedicated tests/ folder to keep them separate from the main source code.\n\n\nPure functions‚Äîthose without side effects like reading or writing external files‚Äîare especially easy to test because you can directly supply the necessary inputs. Placing your main logic into pure functions therefore simplifies testing the critical parts of your code. For impure functions, such as those interacting with databases or APIs, you can use techniques like mocking to simulate external dependencies.\n\n\n\n\n\n\nCall-By-Value vs.¬†Call-By-Reference\n\n\n\nDepending on your programming language, function arguments can be passed either by value (a copy of the variable‚Äôs content) or by reference (the function accesses the variable‚Äôs memory location, allowing direct manipulation). For clean code, we aim to follow the principle: ‚ÄúWhat happens inside a function stays inside the function (except for return values).‚Äù However, passing by reference can introduce unintended side effects, where changes made inside the function affect variables outside it, increasing complexity and leading to confusing bugs. In Python, this often occurs with mutable data types like lists and dictionaries:\ndef change_list(a_list):\n    a_list[0] = 42\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    print(my_list)  # [1, 2, 3]\n    change_list(a_list)\n    print(my_list)  # [42, 2, 3] üò±\nTo prevent such side effects, create a copy of the variable before modifying it. This ensures the original remains unchanged:\nfrom copy import deepcopy\n\ndef change_list(a_list):\n    a_list = deepcopy(a_list)\n    a_list[0] = 42\n\nif __name__ == '__main__':\n    my_list = [1, 2, 3]\n    change_list(a_list)\n    print(my_list)  # [1, 2, 3] üòä\nTo avoid sneaky bugs, include tests to verify:\n\nInputs remain unchanged after execution.\nThe function produces the same output when called twice with identical inputs.\n\n\n\nWhen designing your tests, focus on edge cases‚Äîunusual or extreme scenarios like values outside the normal range or invalid inputs (e.g., dividing by zero or passing an empty list). The more thorough your tests, the more confident you can be in your code. Each time you make significant changes, run all your tests to ensure the code still behaves as expected.\nSome developers even adopt Test-Driven Development (TDD), where they write tests before the actual code. The process begins with writing tests that fail, then creating the code to make them pass. TDD can be highly motivating as it provides clear goals, but it requires discipline and may not always be practical in the early stages of development when function definitions are still evolving.\n\n\n\n\n\n\nTesting at different levels\n\n\n\nIdeally, you‚Äôll test your software at all levels:\n\nUnit Tests: Test individual components (e.g., single functions) to verify basic logic.\nIntegration/System Tests: Check that different parts of the system work together as expected. These often require more complex setups, like running multiple services at the same time.\nManual Testing: Identify unexpected behavior or overlooked edge cases. Whenever a bug is found, create an automated test to reproduce it and prevent regression.\nUser Testing: Evaluate the user interface (UI) with real users to ensure clarity and usability. UX designers often perform these tests using design mockups before coding begins.\n\n\n\n\nDebugging\nWhen your code doesn‚Äôt work as intended, you‚Äôll need to debug‚Äîsystematically identify and fix the problem. Debugging becomes easier if your code is organized into small, testable functions covered by unit tests. These tests often help narrow down the source of the issue. If none of your tests caught the bug, write a new test to reproduce it and ensure this case is covered in the future.\nTo isolate the exact line causing the error:\n\nUse print statements to log variable values at key points and understand the program‚Äôs flow.\nAdd assert statements to verify intermediate results.\nUse a debugger, often integrated into your IDE, to set breakpoints where execution will pause, allowing you to step through the program manually and inspect variables.\n\nDebugging is an essential skill that not only fixes bugs but also improves your understanding of the code and its behavior.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#make-it-fast",
    "href": "05_implementation.html#make-it-fast",
    "title": "5¬† Implementation",
    "section": "Make It Fast",
    "text": "Make It Fast\n\nMake it run, make it right, make it fast.\n‚Äì Kent Beck (or rather this dad, Douglas Kent Beck3)\n\nNow that your code works and produces the right results (as you‚Äôve dutifully confirmed with thorough testing), it‚Äôs time to think about performance.\n\n\n\n\n\n\nReadability over performance\n\n\n\nAlways prioritize writing code that‚Äôs easy to understand. Performance optimizations should never come at the cost of readability. More time is spent by humans reading and maintaining code than machines executing it.\n\n\n\nFind and fix the bottlenecks\nInstead of randomly trying to speed up everything, focus on the parts of your code that are actually slow. A quick way to find bottlenecks is to manually interrupt your code during a long run; if it always stops in the same place, that‚Äôs likely the issue. For a more systematic approach, use a profiler. Profilers analyze your code and show you how much time each part takes, helping you decide where to focus your efforts.\nAccessing files on disk or fetching data over the network is one of the slowest operations in most programs. Whenever possible, cache the results by storing the loaded data in memory to avoid repeated access to external resources. Just be mindful of how frequently the external data changes and invalidate the cache when the information becomes outdated.\n\n\n\n\n\n\nRun it in the cloud\n\n\n\nWorking with large datasets may trigger Out of Memory errors as your computer runs out of RAM. While optimizing your code can help, sometimes the quickest solution is to run it on a larger machine in the cloud. Platforms like AWS, Google Cloud, Azure, or your institution‚Äôs own compute cluster make this cost-effective and accessible. That said, always look for simple performance improvements first!\n\n\n\n\nThink About Big O\nSome computations have unavoidable limits. For example, finding the maximum value in an unsorted list requires checking every item‚Äîthere is no way around this. The ‚ÄúBig O‚Äù notation is used to describe these limits, helping you understand how your code scales as data grows (both in terms of execution time and required memory).\n\nConstant time (\\(\\mathcal{O}(1)\\)): Independent of dataset size (e.g., looking up a key in a dictionary).\nLinear time (\\(\\mathcal{O}(n)\\)): Grows proportionally to data size (e.g., finding the maximum in a list).\nProblematic growth (e.g., \\(\\mathcal{O}(n^3)\\) or \\(\\mathcal{O}(2^n)\\)): Polynomial or exponential scaling can make algorithms impractical for large datasets.\n\nWhen developing a novel algorithm, you should examine its scaling behavior both theoretically (e.g., using proofs) and empirically (e.g., timing it on datasets of different sizes). Designing a more efficient algorithm is a major achievement in computational research!\n\n\nDivide & Conquer\nIf your code is too slow or your dataset too large, try splitting the work into smaller, independent chunks and combining the results. Such a ‚Äúdivide and conquer‚Äù approach is used in many algorithms, like the merge sort algorithm, and in big data frameworks like MapReduce.\n\nExample: MapReduce\nMapReduce [2] was one of the first frameworks developed to work with ‚Äòbig data‚Äô that does not fit on a single computer anymore. The data is split into chunks and distributed across multiple machines, where each chunk is processed in parallel (map step), and then the results are combined into the final output (reduce step).\nFor instance, if you‚Äôre training a machine learning model on a very large dataset, you could train separate models on subsets of the data and then aggregate their predictions (e.g., by averaging them), thereby creating an ensemble model.\n\n\n\n\n\n\nReplace For-Loops with Map/Filter/Reduce\n\n\n\nSequential for loops can often be replaced with map, filter, and reduce operations for better readability and potential parallelism:\n\nmap: Transform each element in a sequence.\nfilter: Keep elements that meet a condition.\nreduce: Aggregate elements recursively (e.g., summing values).\n\nFor example:\nfrom functools import reduce\n\n### Simplify this loop:\nresult_sum = 0\nresult_max = -float('inf')\nfor i in range(10000):\n    new_i = i**0.5\n    # the modulo operator x % y gives the remainder when diving x by y\n    # i.e., we're checking for even numbers, where the rest is == 0\n    if (round(new_i) % 2) == 0:\n        result_sum += new_i\n        result_max = max(result_max, new_i)\n\n### Using map/filter/reduce:\n# map(function to apply, list of elements)\nnew_i_all = map(lambda x: x**0.5, range(10000))\n# filter(function that returns true or false, list of elements)\nnew_i_filtered = filter(lambda x: (round(x) % 2) == 0, new_i_all)\n# reduce(function to combine current result with next element, list of elements, initial value)\nresult_sum = reduce(lambda acc, x: acc + x, new_i_filtered, 0)\nresult_max = reduce(lambda acc, x: max(acc, x), new_i_filtered, -float('inf'))\n# (of course, for these simple cases you could just use sum() and max() on the list directly)\nIn Python, list comprehensions also offer concise alternatives:\nnew_i_filtered = [i**0.5 for i in range(10000) if (round(i**0.5) % 2) == 0]\n\n\n\n\nExploit Parallelism\nMany scientific computations are ‚Äúembarrassingly parallelizable,‚Äù meaning tasks can run independently. For example, running simulations with different model configurations, initial conditions, or random seeds. Each of these experiments can be submitted as a separate job and run in parallel on a compute cluster. By identifying parts of your code that can be parallelized, you can save time and make full use of available resources.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#refactoring-make-change-easy",
    "href": "05_implementation.html#refactoring-make-change-easy",
    "title": "5¬† Implementation",
    "section": "Refactoring: Make Change Easy",
    "text": "Refactoring: Make Change Easy\nRefactoring is the process of modifying existing code without altering its external behavior [4]. In other words, it preserves the ‚Äúcontract‚Äù (interface) between your code and its users while improving its internal structure.\nCommon refactoring tasks include:\n\nRenaming: Giving (internally used) variables, functions, or classes more meaningful and descriptive names.\nExtracting Functions: Breaking large functions into smaller, more focused ones (\\(\\to\\) one function should do one thing).\nEliminating Duplication: Consolidating repeated code into reusable functions.\nSimplifying Logic: Reducing deeply nested code structures or introducing guard clauses for clarity.\nReorganizing Code: Grouping related functions or classes into appropriate files or modules.\n\n\nWhy refactor?\nRefactoring is typically done for two main reasons:\n\nAddressing Technical Debt:\nWhen code is written quickly‚Äîoften to meet deadlines‚Äîit may include shortcuts that make future changes harder. This accumulation of compromises is called ‚Äútechnical debt.‚Äù Refactoring cleans up this debt, improving code quality and making the code easier to understand.\n\nExample: Revisiting old code can be like tidying up a messy campsite. Just as a good scout leaves the campground cleaner than they found it, a responsible developer leaves the codebase better for the next person (or themselves in the future).\n\nMaking Change Easier:\nSometimes, implementing a new feature in your existing code feels like forcing a square peg into a round hole. Instead of struggling with awkward workarounds, you should first refactor your code to align with the new requirements. The goal of software design isn‚Äôt to predict every possible future change (which is impossible) but to adapt gracefully when those changes arise. This promotes an evolutionary architecture, where you solve problems once you understand them better [3].\n\nBefore adding a new feature, clean up your code so that the change feels natural and seamless. This not only simplifies the task at hand but also results in a more general, reusable functions and classes.\n\n\n\n\nRefactorings to simplify changes\n\nFor each desired change, make the change easy (warning: this may be hard), then make the easy change.\n‚Äì Kent Beck4\n\n\nReplace Magic Numbers with Constants: Magic numbers‚Äîvalues with unclear meaning‚Äîcan make code harder to understand and maintain. By replacing them with constants, you create a single source of truth that‚Äôs easy to modify.\n# Before:\nif status == 404:\n    ...\n\n# After:\nERROR_NOT_FOUND = 404\nif status == ERROR_NOT_FOUND:\n    ...\nDon‚Äôt Repeat Yourself (DRY): Copying and pasting code may seem like a quick fix, but it leads to problems later. If the logic changes, you‚Äôll need to update it everywhere it‚Äôs duplicated, which is error-prone. Instead, move the logic into a reusable function or method.\n# Before:\nif (model.a &gt; 5) and (model.b == 3) and (model.c &lt; 8):\n    ...\n\n# After:\nclass MyModel:\n    def is_ready(self):\n        return (self.a &gt; 5) and (self.b == 3) and (self.c &lt; 8)\n\nif model.is_ready():\n    ...\nImplement Wrappers: When working with external libraries or APIs, their provided interface might not align with your needs, and adapting to it directly can lead to awkward implementations in your code. A better solution is to create a wrapper that implements the interface you wish you had, translating the external API‚Äôs inputs and outputs into the format that best suits your implementation. This approach keeps your code clean, consistent, and easier to maintain, while confining the less-than-ideal API interactions to a single location. Plus, if the external API changes, you only need to update the wrapper instead of changing your code everywhere.\nUse Alternative Constructors: Similar to a wrapper, you can add a class method to create objects in a way that‚Äôs different from the regular constructor. This is useful when the input data doesn‚Äôt directly match what the constructor needs. For instance, imagine you have a configuration file that specifies settings for a simulation. If the names or structure of these settings don‚Äôt match the constructor‚Äôs parameters, you can create a from_config method to handle the translation and then call the constructor with the correct arguments. The advantage is that if the format of the configuration file changes in the future, you only need to update this one method, keeping the rest of your code the same.\nclass Date:\n  def __init__(self, year, month, day):\n      self.year = year\n      self.month = month\n      self.day = day\n\n  @classmethod\n  def from_str(cls, date_str):\n      # Parse the string and create a new instance\n      year, month, day = map(int, date_str.split('-'))\n      return cls(year, month, day)\n\n# Usage:\ndate1 = Date(2025, 01, 30)\ndate2 = Date.from_str(\"2025-01-30\")\nOrganize for Coherence: Keep code elements that need to change together in the same file or module. Conversely, separate unrelated parts of your code to prevent unnecessary entanglement. This way, changes are localized, which reduces cognitive load.\nIn larger codebases shared by multiple teams, this is even more critical. When changes require excessive communication and coordination, it signals a need to reorganize the code. Clear ownership and reduced dependencies help teams work independently while keeping the system coherent through agreed upon interfaces.\n\n\n\n\n\n\n\nAdditional tips\n\n\n\n\nTest as you refactor: Always run tests before and after refactoring to ensure no functionality is accidentally broken. Writing or expanding automated tests is often part of the process to safeguard against regressions.\nLeverage IDE support: Modern IDEs like PyCharm or Visual Studio Code provide tools for automated refactoring, such as renaming, extracting functions, or moving files. These can save time and reduce errors.\nAvoid over-refactoring: While cleaning up code is valuable, avoid making unnecessary changes that don‚Äôt improve functionality or clarity. Over-refactoring wastes time and can confuse collaborators.\n\n\n\nBy refactoring regularly and following these practices, you‚Äôll create a cleaner, more maintainable codebase that is adaptable to future needs and enjoyable to work with.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nHow to transform your ideas into code.\nSome best practices to write code that is easy to understand and maintain.\n\n\n\n\n\n\n\n[1] Beck K. Tidy First? O‚ÄôReilly Media, Inc. (2023).\n\n\n[2] Dean J, Ghemawat S. MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM 51(1), 107‚Äì113 (2008).\n\n\n[3] Ford N, Parsons R, Kua P, Sadalage P. Building Evolutionary Architectures. O‚ÄôReilly Media, Inc. (2022).\n\n\n[4] Fowler M. Refactoring: Improving the Design of Existing Code. Addison-Wesley Professional (2018).\n\n\n[5] Ousterhout JK. A Philosophy of Software Design. Yaknyam Press Palo Alto, CA, USA (2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "05_implementation.html#footnotes",
    "href": "05_implementation.html#footnotes",
    "title": "5¬† Implementation",
    "section": "",
    "text": "The __init__.py file is needed to turn a directory into a package from which other scripts can import functionality. Usually, the file is completely empty.‚Ü©Ô∏é\nhttps://martinfowler.com/bliki/TwoHardThings.html‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/704385198301904896‚Ü©Ô∏é\nhttps://x.com/KentBeck/status/250733358307500032‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "06_production.html",
    "href": "06_production.html",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Components of Software Products\nYour results look great, the paper is written, the conference talk is over‚Äînow you‚Äôre done, right?! Well, in academia, you might be. But let‚Äôs explore some concepts and tools that are common in industry and could take your code to the next level. Maybe they even inspire you to turn your project into a deployable app‚Äîan excellent reference when you apply for your next job!\nSo far, your code might consist of scripts or notebooks for analysis and a set of reusable helper functions in your personal library. The next step? Making your code accessible to others by turning it into standalone software with a graphical user interface (GUI). Furthermore, we‚Äôll explore how to expand beyond static data sources like CSV or Excel files.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_production.html#components-of-software-products",
    "href": "06_production.html#components-of-software-products",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Graphical User Interface (GUI)\nSoftware shines when users can interact with it easily. Instead of using a command-line interface (CLI), these days, users expect intuitive GUIs with buttons and visual elements.\nWe can broadly categorize software programs into:\n\nStand-alone desktop or mobile applications, which users download and install on their devices.\nWeb-based applications that run in a browser, like Google Docs. These are increasingly popular thanks to widespread internet access.\n\nFor web apps, the GUI users interact with is also referred to as the frontend, while the backend handles behind-the-scenes tasks like data storage and processing. Even seemingly standalone desktop clients often connect to a backend server for cloud storage or to enable collaboration on shared documents. We‚Äôll explore how this works in the section on APIs.\nIn research, the goal is often to make results more accessible, for example, by transforming a static report into an interactive dashboard where users can explore data. To do this, we recommend you start with a web-based app.\nMany books and tutorials were written on the topic of building user-friendly software applications and a lot of it is very specific to the programming language you‚Äôre using‚Äîplease consult your favorite search engine to discover more resources on this topic. üòâ\n\n\n\n\n\n\nWeb apps with Python‚Äôs streamlit framework\n\n\n\nIf you use Python, try the Streamlit framework to create web apps from your analysis scripts in minutes.\n\n\n\n\nDatabases\nSo far, we‚Äôve assumed that your data is stored in spreadsheets (like CSV or Excel files) on your computer. While this works for smaller datasets and simple workflows, it becomes less practical as your data grows or is generated dynamically, such as through user interactions, and needs to be accessed and updated by multiple people at the same time. This is where databases come in, offering a more efficient and scalable way to store, retrieve, and manage data [3].\nDatabases come in many forms, each suited to different types of data and use cases. Two key considerations when choosing a database are [5]:\n\nthe kind of data you need to store, and\nhow that data will be used.\n\n\nTypes of Data in Databases\nDifferent kinds of databases are ideal for different types of data (see also Chapter 2):\n\nStructured data: This resembles spreadsheet data, with rows for records and columns for attributes. Structured data is typically stored in relational (SQL) databases, where data is organized into multiple interrelated tables. Each table has a schema‚Äîa strict definition of the fields it contains and their types, such as text or numbers. If data doesn‚Äôt match the schema, it‚Äôs rejected.\n\n\n\n\n\n\nNormalization in relational databases\n\n\n\n\n\nA process called normalization reduces redundancy by splitting data into separate tables. For example, instead of storing material_type, material_supplier, and material_quality directly in a table of samples, you‚Äôd create a materials table with unique IDs for each material, then reference the material_id in the samples table. This avoids duplication and makes updates easier but requires more complex queries to combine tables and extract all the data needed for analysis.\n\n\n\nSemi-structured data: JSON or XML documents contain data in flexible key-value pairs and are often stored in NoSQL databases. Unlike SQL databases, these databases don‚Äôt enforce a strict schema, which makes them ideal for handling complex, nested data structures or dynamically changing datasets. For example, APIs often exchange data in JSON format, which can be stored as-is to avoid breaking it into tables and reconstructing it later.\nModern relational databases, such as Postgres, blur the line between structured and semi-structured data by supporting JSON columns alongside traditional tables.\nUnstructured data: Files like images, videos, and text documents are typically stored on disk. Data lakes (e.g., AWS S3) provide additional tools to manage these files, but fundamentally, this is similar to organizing files in folders on your computer. If your data is processed through a pipeline, it‚Äôs a good idea to save copies of the files at each stage (e.g., in raw and cleaned folders).\nStreaming data: High-volume, real-time data (e.g., IoT sensor logs) is best managed in specialized databases optimized for streaming, such as Apache Kafka.\n\n\n\nUse Cases for Databases\nWhen choosing a database, you‚Äôll also want to consider how the data will be used later:\n\nTransactional processing (OLTP): In this use case, individual records are frequently created and retrieved (e.g., financial transactions). These systems prioritize fast write speeds and maintaining an up-to-date view of the data.\nBatch analytics (OLAP): Data analysis is often performed in large batches to generate reports or insights, such as identifying which products users purchased. To avoid overloading the operational database with complex queries, data is typically copied from transactional systems to analytical systems (e.g., data warehouses) using an ETL process (Extract-Transform-Load).\nReal-time analytics: For applications requiring live data (e.g., interactive dashboards), databases and frameworks optimized for streaming or in-memory processing (e.g., Apache Flink) are ideal.\n\nScaling your database system is another critical factor. Consider how many users will access it simultaneously, how much data they‚Äôll retrieve, and how often it will be updated.\n\n\nCRUD Operations\nDatabases support four basic operations collectively called CRUD:\n\nCreate: Add new records.\nRead: Retrieve data.\nUpdate: Modify existing records.\nDelete: Remove records.\n\nThese operations are performed using queries, often written in SQL (Structured Query Language). For example, SELECT * FROM table; retrieves all records from a table. If you‚Äôre new to SQL, this tutorial is a great place to start.\n\n\nORMs and Database Migrations\nWriting raw database queries can be tedious, especially when working with complex schemas. Object-Relational Mappers (ORMs) simplify this by mapping database tables to objects in your code. With ORMs, you can interact with your database using familiar programming constructs and even define the schema directly in your code.\nWhen designing a database schema and implementing the corresponding ORM, it‚Äôs helpful to first sketch out the structure of the data (Figure¬†6.1). Start by identifying the key objects (which map to database tables), their fields, and their relationships.\n\n\n\n\n\n\nFigure¬†6.1: The classes User, Order, Product, and Category are mapped to the corresponding tables users, orders, products, and categories. Every record in a table is uniquely identified by a primary key, often named id. Fields in a table can either store data directly (e.g., text or boolean values) or reference records in another table, establishing relationships between tables. These relationships are defined using foreign keys, which store the primary key of a related record. For example, an Order references a single User ID (indicating the user who placed the order) and multiple Product IDs (the items included in the order).\nRelationships between tables can be bidirectional or unidirectional, depending on the use case. For instance, when querying a Product, we want to list all the categories it belongs to, and vice versa. In contrast, the relationship between Order and Product only goes one way: when retrieving an Order, we want to know which products are included, but querying a Product doesn‚Äôt usually require listing all the orders it appears in.\n\n\n\nDatabase migrations, or schema changes, often require careful coordination between code and database updates. For instance, renaming a field means you have to update your database and modify the code accessing it at the same time. Keeping migration scripts and application code (including ORMs) in the same repository helps ensure consistency during such updates.\n\n\n\n\n\n\nManaging databases in Python\n\n\n\nThe SQLModel library is highly recommended when working with relational databases in Python and also includes a great tutorial to learn more about ORMs and databases in general. For database migrations, check out Alembic.\n\n\n\n\n\nAPIs\nIn contrast to a user interface, through which a human interacts with a program, an Application Programming Interface (API) enables software components to communicate with one another. Think of it as a contract that defines how different systems interact. For example, an API might specify the classes and functions a library provides so developers can integrate it effectively.\nAPIs are often associated with Web APIs, which provide functionality over the internet. These can either be external services, like the Google Maps API for retrieving directions, or a custom-built backend that serves as an abstraction layer for a database. This abstraction is useful because it can combine data, enforce rules (e.g., verifying user permissions), and maintain a consistent interface even when the database structure changes.\n\nInteracting with APIs\nWeb APIs typically use four HTTP methods that correspond to the CRUD (Create, Read, Update, Delete) operations in databases:\n\nGET: Retrieves data, most commonly used when accessing websites. You can include additional parameters by appending a ? to the URL. For example, https://www.google.com/search?q=web+api searches for ‚Äúweb api‚Äù using the query parameter q. To pass multiple parameters, separate them with &.\nPOST: Sends data to create a new record, often as a JSON object, for example, when submitting a form.\nPUT: Updates an existing record.\nDELETE: Removes a record.\n\nYou typically interact with APIs through a website‚Äôs frontend, which triggers these API calls in the background. However, APIs can also be queried directly to access raw data, usually returned in JSON format.\n\n\n\n\n\n\nAPI Keys and Authentication\n\n\n\nMany APIs require an API key to access their functionality. This key serves as an identifier, allowing the API to authenticate users, track usage, and apply rate limits to prevent abuse. Always keep your API keys secure and avoid exposing them in public repositories or client-side code.\n\n\nThere are many free public APIs you can explore. As an example, we‚Äôll use The Cat API to demonstrate how to interact with an API.\nYou can perform a GET request directly in your web browser. For instance, by visiting https://api.thecatapi.com/v1/images/search?limit=10, you‚Äôll receive a JSON response containing a list of 10 random cat image URLs along with additional details like the image IDs.\nFor more advanced requests, such as POST, you‚Äôll need specialized tools. Popular GUI clients include Postman, Insomnia, and Bruno. If you prefer command-line tools, curl is a powerful option. Alternatively, you can interact with APIs programmatically using your preferred programming language and relevant libraries.\n\n\n\n\n\n\nInteracting with APIs programmatically\n\n\n\nIn the examples below, we use curl and Python to interact with The Cat API to retrieve the latest votes for cat images with a GET request and submit a new vote using a POST request.\nUsing curl\nEnsure curl is installed by running which curl in a terminal‚Äîthis should return a valid path to your installation.\n# GET request to view the last 10 votes for cat images\ncurl \"https://api.thecatapi.com/v1/votes?limit=10&order=DESC\" \\\n-H \"x-api-key: DEMO-API-KEY\"\n\n# POST request to submit a new vote\n# the payload after -d is the JSON object submitted to the API\ncurl -X POST \"https://api.thecatapi.com/v1/votes\" \\\n-H \"Content-Type: application/json\" \\\n-H \"x-api-key: DEMO-API-KEY\" \\\n-d '{\n  \"image_id\": \"HT902S6ra\",\n  \"sub_id\": \"my-user-1234\",\n  \"value\": 1\n}'\n# now run the GET request again to see your new vote\nUsing Python\nPython‚Äôs requests library is great for working with APIs.\nimport requests\n\nBASE_URL = \"https://api.thecatapi.com/v1/votes\"\nAPI_KEY = \"DEMO-API-KEY\"\n\n# GET request to fetch the last 10 votes\ndef get_last_votes():\n    response = requests.get(\n        BASE_URL,\n        headers={\"x-api-key\": API_KEY},\n        params={\"limit\": 10, \"order\": \"DESC\"}\n    )\n    if response.status_code == 200:\n        print(response.json())\n    else:\n        print(f\"Error: {response.status_code}\")\n\n# POST request to submit a new vote\ndef submit_vote(image_id, sub_id, value):\n    data = {\"image_id\": image_id, \"sub_id\": sub_id, \"value\": value}\n    response = requests.post(\n        BASE_URL,\n        headers={\"Content-Type\": \"application/json\", \"x-api-key\": API_KEY},\n        json=data\n    )\n    if response.status_code == 201:\n        print(\"Vote submitted!\")\n    else:\n        print(f\"Error: {response.status_code}\")\n\nif __name__ == '__main__':\n    get_last_votes()\n    submit_vote(\"HT902S6ra\", \"my-user-1234\", 1)\n\n\n\n\nImplementing APIs\nWhen designing an API, specifically a REST (REpresentational State Transfer) API, it‚Äôs important to understand the concept of an endpoint. An endpoint is a specific URL in your API where a resource can be accessed or modified. For example, if you‚Äôre building a photo-sharing app, an endpoint like /images might allow users to view or upload images. Endpoints should be named using descriptive, plural nouns (e.g., /users, /images) to clearly represent the resources being accessed. It‚Äôs also best practice to avoid including verbs in endpoint names (e.g., /get_users), since the HTTP method (like GET or POST) already specifies the action being taken, such as retrieving or creating data.\nAnother key design principle is statelessness. Similar to the concept of pure functions, this means that each API request should contain all the information needed to complete the action, like user authentication tokens. This way, the server doesn‚Äôt need to remember anything about previous requests, making the API easier to scale. This is especially important in cloud-based environments where multiple requests from the same user may be routed to different servers [1].\nData that needs to be persisted can be stored either in the frontend or the backend database, depending on its purpose. Temporary data, like a shopping cart, can be maintained on the user‚Äôs machine using cookies or local storage. Permanent data, such as a purchase order, is best stored in the backend database to ensure long-term accessibility. This approach supports stateless APIs, as the backend server doesn‚Äôt need to keep the session state in memory. Instead, all necessary data is either included in the request or can be fetched from the database, allowing each request to be processed independently.\n\n\n\n\n\n\nImplementing APIs in Python with FastAPI\n\n\n\nFastAPI is a Python framework that makes building APIs straightforward. With just a few lines of code, you can turn functions into endpoints that validate input and return JSON responses. It‚Äôs beginner-friendly and highly performant.\n\n\n\n\n\nAsynchronous Communication\nWhen your script calls a library function or API and waits for it to return before continuing with the rest of the code, this is an example of synchronous communication. It‚Äôs similar to a conversation where one person speaks and then waits and listens while the other person responds.\nIn contrast, asynchronous (async) communication allows the program to keep running while waiting for a response. Once the response arrives, it is processed and integrated into the workflow, but until then the code just continues without it. Just like when you send an email to someone asking for some data and they send you the results a few hours later.\nFor example, a website might fetch data from multiple APIs, showing placeholders until the responses arrive. This approach improves the user experience because it keeps the user interface (UI) responsive and enables faster loading by processing multiple tasks in parallel.\n\nEvent-Driven Architecture\nFor most applications, communicating directly with external services‚Äîwhether synchronously or async‚Äîis the right approach, because eventually the requested data is needed to finish the original task. But there are also use cases where it‚Äôs enough that your message was received and you don‚Äôt need to wait for a response. For example, when placing an order in an online shop, users only care that the order was submitted successfully. They don‚Äôt wait in front of the screen until it was packaged and shipped‚Äîwhich could take days. An email notification can inform them of progress later.\nSuch a scenario calls for an event-driven architecture, which takes async communication to the extreme. Here, multiple services can operate independently by exchanging information via events using a message queue (MQ), a system that temporarily stores event messages like JSON documents. These messages act as instructions, containing all relevant details about an event, such as a user‚Äôs order information. Publishers (senders) create events, and subscribers (receivers) process them based on their type, such as Order Submitted.\nAn event-driven architecture offers several advantages. By decoupling components, it allows publishers and subscribers to run independently, even in different programming languages or environments. This makes it easier to scale systems and assign teams to own specific components without needing to understand the full system. Additionally, one event can trigger multiple actions. For instance, when an order is packed, one system might update the user database while another generates a shipping label. This approach thereby simplifies the propagation of data to multiple services and enables replication of live data to testing and staging environments.\nHowever, this type of architecture also brings with it some challenges. Since no single component has a full view of the system, tracking the state of a specific task, such as whether an order is still waiting or in progress, can be difficult. Furthermore, while MQs often guarantee that each message is handled at least once, the system requires careful design in case a message is processed multiple times. For example, if a subscriber crashes after processing a message but before confirming its completion, the MQ might reassign the task to another instance, potentially leading to duplicate processing. For these reasons, event-driven architectures should only be used when direct communication between services is not an option [4].\n\n\n\nBatch Jobs\nUnlike continuously running services such as APIs, batch jobs are scripts or workflows used to process accumulated data in one go. They are particularly effective when tasks don‚Äôt require immediate processing or when grouping tasks can improve efficiency. To automate recurring tasks, batch workflows can be scheduled at specific intervals using tools like cron jobs.1\nExamples of scenarios where batch jobs are useful include:\n\nFetching new messages from a queue every 10 minutes to process them in bulk, reducing overhead.\nGenerating a sales report for the marketing department every Monday at midnight.\nRunning nightly data validation to check for data drift or anomalous patterns.\nRetraining a machine learning model every week using newly collected data to create updated recommendations, like Spotify‚Äôs ‚ÄúDiscover Weekly‚Äù playlist.\n\nFor large-scale jobs, distributed systems might be necessary to ensure they complete within an acceptable timeframe.\n\n\nSoftware Design Revisited\nAs your software evolves beyond a simple script, it becomes a system composed of multiple interconnected components. Each component can be viewed as a subsystem with its own defined boundaries and interface, responsible for specific functionalities while interacting with other parts of the system.\nTo manage this growing complexity, it‚Äôs best to think of these components‚Äîsuch as a GUI/frontend, API/backend, and database‚Äîas distinct layers. A clean design follows the principle of layered communication, where each layer interacts only with the layer directly below it. For example, the frontend communicates with the backend, and the backend interacts with the database, avoiding ‚Äúskip connections‚Äù where one layer bypasses another.\nThis design principle minimizes dependencies and makes the system easier to maintain: If the interface of one component changes, only the layer directly above it has to be adapted.\nWhen you design these more complicated systems, it‚Äôs even more important to sketch the overall architecture before you start with the implementation. Visualizing how the layers interact can reveal potential bottlenecks or unnecessary complexity and gives you and your collaborators clarity on the big picture.\nInstead of lumping everything into ‚Äúyour code‚Äù versus ‚Äúthe outside world‚Äù as we did in Figure¬†4.3, you can use swimlanes to separate the process steps performed by each component to distinguish between the different layers (Figure¬†6.2).\n\n\n\n\n\n\nFigure¬†6.2: A simplified flow showing what happens when you order something online: A user opens a product page, which triggers a request to the API to fetch the corresponding product details. The user then clicks the ‚Äúadd to cart‚Äù button, which places the product into the shopping cart (in local storage managed by the frontend). The user then views the shopping cart and clicks ‚Äúpurchase‚Äù, which triggers a POST request to the API, submitting the user‚Äôs cart contents. The API creates a new record in the orders table to store the purchase details and submits an Order event to the Message Queue, thereby alerting other services that a new order needs to be packed and shipped. The endpoint returns with the status code 201 (‚Äúsuccess‚Äù) and the frontend redirects the user to a page that tells them the purchase was successful, at which point the user closes the tab.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_production.html#delivery-deployment",
    "href": "06_production.html#delivery-deployment",
    "title": "6¬† From Research to Production",
    "section": "Delivery & Deployment",
    "text": "Delivery & Deployment\nModern software development requires reliable and efficient processes to build, test, and deploy applications [2]. Delivery and deployment strategies ensure that new features and updates are released quickly, safely, and at scale, minimizing disruptions to users while maintaining quality.\n\nCI/CD Pipelines: Automating Development Cycles\nContinuous Integration (CI) and Continuous Delivery/Deployment (CD) pipelines are the backbone of modern software practices. CI focuses on automating the process of integrating code changes into a shared repository. Every change triggers automated tests to ensure that the new code works harmoniously with the existing codebase. CD extends this by automating the preparation or deployment of changes into production, either ready for manual approval (Continuous Delivery) or fully automated (Continuous Deployment). This drastically reduces manual effort, minimizes human error, and enables faster iteration cycles.\nCI/CD pipelines are either included directly into version control platforms, such as GitHub Actions and GitLab CI/CD, or can be run using external tools like Jenkins or CircleCI.\n\nOptimizing CI/CD Pipelines\nTo enhance pipeline efficiency and reliability, consider the following practices:\n\nDependency Caching: Cache dependencies to reduce the time spent downloading and installing them for each build.\nSelective Testing: Run only the tests affected by recent changes to speed up feedback.\nReal-Time Notifications: Notify developers immediately when a pipeline fails, enabling faster issue resolution.\n\n\n\n\n\n\n\nSecurity in CI/CD Pipelines\n\n\n\nSecurity must be a priority in any CI/CD process. For example, it is best practice to include a dependency scanning step to detect vulnerabilities in third-party libraries. Furthermore, you should never include sensitive information‚Äîsuch as API tokens, database credentials, or private keys‚Äîdirectly in your code. However, because CI jobs often require access to this information, you can securely store secrets using dedicated CI/CD variables or external secret management tools like HashiCorp Vault or AWS Secrets Manager.\n\n\nA well-designed CI/CD pipeline not only saves time and resources but also ensures a consistent and high-quality delivery of software.\n\n\n\nContainers in the Cloud\nContainers, powered by tools like Docker, encapsulate applications with their dependencies, ensuring consistency across different environments. This portability simplifies deployment and reduces issues caused by environment differences.\nFor managing containerized applications at scale, Kubernetes (k8s) is the industry standard. Kubernetes automates the orchestration of containers, providing features like:\n\nAuto-scaling: Adjust resources dynamically based on workload.\nSelf-healing: Automatically restart failed containers.\nLoad Balancing: Distribute traffic efficiently across services.\n\n\nUsing Cloud Platforms\nCloud platforms like AWS, Google Cloud Platform (GCP), and Microsoft Azure offer robust infrastructures for deploying and scaling applications. For simpler workflows, managed services like Render or Heroku abstract away much of the operational complexity.\nManaging costs effectively is critical in cloud deployments. Key strategies include:\n\nResource Scaling: Reduce unused resources during off-peak hours.\nServerless Computing: Use serverless models, like AWS Lambda, for infrequent workloads to save costs.\nCost Monitoring Tools: Leverage AWS Cost Explorer or GCP Billing to track and optimize spending.\n\n\n\nInfrastructure as Code (IaC)\nInstead of configuring your cloud setup manually through the platform‚Äôs GUI, it is highly recommended to use Infrastructure as Code tools like Terraform and AWS CloudFormation to manage cloud infrastructure programmatically. The IaC configuration files can then be version-controlled, which ensures:\n\nReproducible setups for consistent environments.\nEasier onboarding for new team members.\nReduced risk of configuration drift.\n\n\n\nTesting and Staging Environments\nDeploying changes directly to production is risky. To ensure stability:\n\nUse staging environments that mimic production to validate changes before release.\nMaintain testing environments for early experimentation and debugging.\n\nTechniques like A/B testing and feature toggles allow gradual rollouts or controlled exposure of new features, minimizing user disruption. This can be achieved using deployment strategies like:\n\nBlue-Green Deployments: Maintain two environments (blue and green) and switch traffic between them for A/B tests or to reduce downtime during updates.\nCanary Releases: Gradually expose updates to a small group of users, monitoring for issues before full deployment.\n\n\n\n\nScaling Considerations\nAs applications grow, scaling requires thoughtful architectural design. You should consider:\n\nTask Separation: For example, train machine learning models periodically as batch jobs, while keeping prediction services running continuously. This is particularly important when services have vastly different user bases (e.g., hundreds of admins versus millions of regular users), as they require varying replication rates for horizontal scaling. Especially if services rely on distinct dependencies, combining them into a single Docker container can result in a large, inefficient image, which increases the services‚Äô startup time.\nTeam Autonomy: Design services so that teams can own and work on individual components independently, thereby reducing communication overhead and speeding up development cycles.\n\n\n\nMonitoring and Observability\nTo ensure smooth operation and detect issues proactively, monitoring and observability are essential. Focus on:\n\nSystem Performance: Monitor the ‚Äúgolden signals‚Äù‚Äîlatency, traffic, errors, and saturation. Tools like Prometheus and Grafana are commonly used.\nData Quality: Track changes in input data distributions and monitor metrics like model accuracy to detect data drift.\nSynthetic Monitoring: Simulate user behavior to identify bottlenecks and improve responsiveness. Complement this with chaos engineering tools like Chaos Monkey to test your system‚Äôs resilience by deliberately introducing failures, ensuring your infrastructure can handle unexpected disruptions effectively.\nDistributed Tracing: Debug across microservices using tools like Jaeger or OpenTelemetry.\n\nWhen issues arise, having a rollback strategy is crucial. Options include:\n\nReverting to a stable container image.\nRolling back database migrations.\nUsing feature toggles to disable problematic updates.\n\nBy combining robust delivery pipelines, thoughtful architecture, and effective monitoring, teams can ensure that their applications remain reliable, scalable, and adaptable to changing needs.\n\n\n\n\n\n\nBefore you continue\n\n\n\nAt this point, you should have a clear understanding of:\n\nWhich additional steps you could take to make your research project production-ready.\n\n\n\n\n\n\n\n[1] Davis C. Cloud Native Patterns: Designing Change-Tolerant Software. Simon; Schuster (2019).\n\n\n[2] Forsgren N, Humble J, Kim G. Accelerate: The Science of Lean Software and Devops: Building and Scaling High Performing Technology Organizations. IT Revolution (2018).\n\n\n[3] Kleppmann M. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O‚ÄôReilly Media, Inc. (2017).\n\n\n[4] Richards M, Ford N. Fundamentals of Software Architecture: An Engineering Approach. O‚ÄôReilly Media, Inc. (2020).\n\n\n[5] Serra J. Deciphering Data Architectures. O‚ÄôReilly Media, Inc. (2024).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "06_production.html#footnotes",
    "href": "06_production.html#footnotes",
    "title": "6¬† From Research to Production",
    "section": "",
    "text": "Tools like Crontab Guru can help configure these schedules.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>From Research to Production</span>"
    ]
  },
  {
    "objectID": "07_afterword.html",
    "href": "07_afterword.html",
    "title": "Afterword",
    "section": "",
    "text": "You‚Äôre still at the beginning of your journey towards professional software engineering. But I hope this book could give you a glimpse into what lies ahead.\nI‚Äôm always looking to improve the contents of this book (or any other resources you can find on my website). Therefore, I would be eternally grateful for your feedback‚Äîwhether you just found a typo, you think an explanation is unclear, or there are other topics that you think this book should cover‚Äîplease send me an email to hey@franziskahorn.de to let me know what you think!",
    "crumbs": [
      "Afterword"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] Beck K. Tidy First? O‚ÄôReilly Media,\nInc. (2023).\n\n\n[2] Callaway E. Chemistry Nobel Goes to Developers\nof AlphaFold AI That Predicts Protein Structures. Nature\n634(8034), 525‚Äì526 (2024).\n\n\n[3] Davis C. Cloud Native Patterns: Designing\nChange-Tolerant Software. Simon; Schuster (2019).\n\n\n[4] Dean J, Ghemawat S. MapReduce: Simplified Data\nProcessing on Large Clusters. Communications of the ACM 51(1),\n107‚Äì113 (2008).\n\n\n[5] Foote B, Yoder J. Big Ball of Mud. Pattern\nlanguages of program design 4, 654‚Äì692 (1997).\n\n\n[6] Ford N, Parsons R, Kua P, Sadalage P.\nBuilding Evolutionary Architectures. O‚ÄôReilly Media, Inc.\n(2022).\n\n\n[7] Forsgren N, Humble J, Kim G. Accelerate:\nThe Science of Lean Software and Devops: Building and Scaling High\nPerforming Technology Organizations. IT Revolution (2018).\n\n\n[8] Fowler M. Refactoring: Improving the Design\nof Existing Code. Addison-Wesley Professional (2018).\n\n\n[9] Freiesleben T, Molnar C. Supervised Machine Learning for\nScience: How to Stop Worrying and Love Your Black Box.\n(2024).\n\n\n[10] Horn F. A Practitioner‚Äôs Guide to\nMachine Learning. (2021).\n\n\n[11] Kleppmann M. Designing Data-Intensive\nApplications: The Big Ideas Behind Reliable, Scalable, and Maintainable\nSystems. O‚ÄôReilly Media, Inc. (2017).\n\n\n[12] Knaflic CN. Storytelling with Data: A Data\nVisualization Guide for Business Professionals. John Wiley &\nSons (2015).\n\n\n[13] McChrystal GS, Collins T, Silverman D, Fussell\nC. Team of Teams: New Rules of Engagement for a Complex World.\nPenguin (2015).\n\n\n[14] Normand E. Grokking Simplicity: Taming\nComplex Software with Functional Thinking. Simon; Schuster\n(2021).\n\n\n[15] Ousterhout JK. A Philosophy of Software\nDesign. Yaknyam Press Palo Alto, CA, USA (2018).\n\n\n[16] Richards M, Ford N. Fundamentals of\nSoftware Architecture: An Engineering Approach. O‚ÄôReilly Media,\nInc. (2020).\n\n\n[17] Serra J. Deciphering Data\nArchitectures. O‚ÄôReilly Media, Inc. (2024).",
    "crumbs": [
      "References"
    ]
  }
]